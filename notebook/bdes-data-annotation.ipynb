{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13871382,"sourceType":"datasetVersion","datasetId":8838063},{"sourceId":13890221,"sourceType":"datasetVersion","datasetId":8849278}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade pip\n!pip install opencv-python pillow \"numpy>=1.24.0,<2.0.0\" scipy>=1.10.0 matplotlib albumentations\n!pip install git+https://github.com/facebookresearch/segment-anything.git\n!pip install torch torchvision","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T07:48:20.144268Z","iopub.execute_input":"2025-11-27T07:48:20.144992Z","iopub.status.idle":"2025-11-27T07:50:10.732930Z","shell.execute_reply.started":"2025-11-27T07:48:20.144963Z","shell.execute_reply":"2025-11-27T07:50:10.732020Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\nCollecting pip\n  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\nDownloading pip-25.3-py3-none-any.whl (1.8 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: pip\n  Attempting uninstall: pip\n    Found existing installation: pip 24.1.2\n    Uninstalling pip-24.1.2:\n      Successfully uninstalled pip-24.1.2\nSuccessfully installed pip-25.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mCollecting git+https://github.com/facebookresearch/segment-anything.git\n  Cloning https://github.com/facebookresearch/segment-anything.git to /tmp/pip-req-build-7mccrcn1\n  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /tmp/pip-req-build-7mccrcn1\n  Resolved https://github.com/facebookresearch/segment-anything.git to commit dca509fe793f601edb92606367a655c15ac00fdf\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.20.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.15.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.10.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.3.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m119.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m156.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m  \u001b[33m0:00:07\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m  \u001b[33m0:00:06\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m99.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m117.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n\u001b[2K  Attempting uninstall: nvidia-nvjitlink-cu12\n\u001b[2K    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n\u001b[2K    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n\u001b[2K      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82/10\u001b[0m [nvidia-nvjitlink-cu12]\n\u001b[2K  Attempting uninstall: nvidia-curand-cu12‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 0/10\u001b[0m [nvidia-nvjitlink-cu12]\n\u001b[2K    Found existing installation: nvidia-curand-cu12 10.3.6.820m [nvidia-nvjitlink-cu12]\n\u001b[2K    Uninstalling nvidia-curand-cu12-10.3.6.82:0m \u001b[32m 0/10\u001b[0m [nvidia-nvjitlink-cu12]\n\u001b[2K      Successfully uninstalled nvidia-curand-cu12-10.3.6.82‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 1/10\u001b[0m [nvidia-curand-cu12]\n\u001b[2K  Attempting uninstall: nvidia-cufft-cu12‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 1/10\u001b[0m [nvidia-curand-cu12]\n\u001b[2K    Found existing installation: nvidia-cufft-cu12 11.2.3.61‚îÅ‚îÅ\u001b[0m \u001b[32m 1/10\u001b[0m [nvidia-curand-cu12]\n\u001b[2K    Uninstalling nvidia-cufft-cu12-11.2.3.61:‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 1/10\u001b[0m [nvidia-curand-cu12]\n\u001b[2K      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 2/10\u001b[0m [nvidia-cufft-cu12]\n\u001b[2K  Attempting uninstall: nvidia-cuda-runtime-cu12‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 2/10\u001b[0m [nvidia-cufft-cu12]\n\u001b[2K    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82m \u001b[32m 2/10\u001b[0m [nvidia-cufft-cu12]\n\u001b[2K    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 2/10\u001b[0m [nvidia-cufft-cu12]\n\u001b[2K      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 3/10\u001b[0m [nvidia-cuda-runtime-cu12]\n\u001b[2K  Attempting uninstall: nvidia-cuda-nvrtc-cu12‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 3/10\u001b[0m [nvidia-cuda-runtime-cu12]\n\u001b[2K    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82[0m \u001b[32m 3/10\u001b[0m [nvidia-cuda-runtime-cu12]\n\u001b[2K    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 3/10\u001b[0m [nvidia-cuda-runtime-cu12]\n\u001b[2K      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 4/10\u001b[0m [nvidia-cuda-nvrtc-cu12]\n\u001b[2K  Attempting uninstall: nvidia-cuda-cupti-cu12‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 4/10\u001b[0m [nvidia-cuda-nvrtc-cu12]\n\u001b[2K    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 5/10\u001b[0m [nvidia-cuda-cupti-cu12]\n\u001b[2K    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 5/10\u001b[0m [nvidia-cuda-cupti-cu12]\n\u001b[2K      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 5/10\u001b[0m [nvidia-cuda-cupti-cu12]\n\u001b[2K  Attempting uninstall: nvidia-cublas-cu12\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 5/10\u001b[0m [nvidia-cuda-cupti-cu12]\n\u001b[2K    Found existing installation: nvidia-cublas-cu12 12.5.3.2‚îÅ‚îÅ\u001b[0m \u001b[32m 5/10\u001b[0m [nvidia-cuda-cupti-cu12]\n\u001b[2K    Uninstalling nvidia-cublas-cu12-12.5.3.2:‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 5/10\u001b[0m [nvidia-cuda-cupti-cu12]\n\u001b[2K      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 6/10\u001b[0m [nvidia-cublas-cu12]\n\u001b[2K  Attempting uninstall: nvidia-cusparse-cu120m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 6/10\u001b[0m [nvidia-cublas-cu12]\n\u001b[2K    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\u001b[0m \u001b[32m 6/10\u001b[0m [nvidia-cublas-cu12]\n\u001b[2K    Uninstalling nvidia-cusparse-cu12-12.5.1.3:‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 6/10\u001b[0m [nvidia-cublas-cu12]\n\u001b[2K      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 7/10\u001b[0m [nvidia-cusparse-cu12]\n\u001b[2K  Attempting uninstall: nvidia-cudnn-cu12[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 7/10\u001b[0m [nvidia-cusparse-cu12]\n\u001b[2K    Found existing installation: nvidia-cudnn-cu12 9.3.0.75‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 7/10\u001b[0m [nvidia-cusparse-cu12]\n\u001b[2K    Uninstalling nvidia-cudnn-cu12-9.3.0.75:0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 7/10\u001b[0m [nvidia-cusparse-cu12]\n\u001b[2K      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.750m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 8/10\u001b[0m [nvidia-cudnn-cu12]\n\u001b[2K  Attempting uninstall: nvidia-cusolver-cu12\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 8/10\u001b[0m [nvidia-cudnn-cu12]\n\u001b[2K    Found existing installation: nvidia-cusolver-cu12 11.6.3.83[0m \u001b[32m 8/10\u001b[0m [nvidia-cudnn-cu12]\n\u001b[2K    Uninstalling nvidia-cusolver-cu12-11.6.3.83:0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 8/10\u001b[0m [nvidia-cudnn-cu12]\n\u001b[2K      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.830m‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 9/10\u001b[0m [nvidia-cusolver-cu12]\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10/10\u001b[0m [nvidia-cusolver-cu12]dia-cusolver-cu12]\n\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nimport shutil\nfrom PIL import Image\nimport albumentations as A\nimport torch\n\n# SAM imports\ntry:\n    from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n    SAM_AVAILABLE = True\n    print(\"‚úÖ SAM library loaded successfully!\")\nexcept ImportError:\n    SAM_AVAILABLE = False\n    print(\"‚ö†Ô∏è  SAM not available, will use improved color-based detection\")\n\nprint(\"‚úÖ All libraries imported successfully!\")\n\n# Download SAM model weights (vit_h is the largest, most accurate model)\n# Alternative: vit_l (large) or vit_b (base) - faster but less accurate\nSAM_MODEL_TYPE = \"vit_h\"  # Options: \"vit_h\", \"vit_l\", \"vit_b\"\nSAM_CHECKPOINT_PATH = \"/kaggle/working/sam_vit_h_4b8939.pth\"\n\n# Download model if not exists\nif not os.path.exists(SAM_CHECKPOINT_PATH):\n    print(\"üì• Downloading SAM model weights...\")\n    import urllib.request\n    urllib.request.urlretrieve(\n        \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\",\n        SAM_CHECKPOINT_PATH\n    )\n    print(\"‚úÖ Model downloaded!\")\n\n# Initialize SAM model\nsam_predictor = None\nif SAM_AVAILABLE:\n    try:\n        print(\"üîÑ Loading SAM model...\")\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        sam = sam_model_registry[SAM_MODEL_TYPE](checkpoint=SAM_CHECKPOINT_PATH)\n        sam.to(device=device)\n        sam_predictor = SamPredictor(sam)\n        print(f\"‚úÖ SAM model loaded on {device}!\")\n    except Exception as e:\n        print(f\"‚ö†Ô∏è  Error loading SAM: {e}\")\n        print(\"   Will use improved color-based detection instead\")\n        SAM_AVAILABLE = False","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-27T07:53:43.593929Z","iopub.execute_input":"2025-11-27T07:53:43.594632Z","iopub.status.idle":"2025-11-27T07:53:59.267396Z","shell.execute_reply.started":"2025-11-27T07:53:43.594610Z","shell.execute_reply":"2025-11-27T07:53:59.266457Z"}},"outputs":[{"name":"stdout","text":"‚úÖ SAM library loaded successfully!\n‚úÖ All libraries imported successfully!\nüì• Downloading SAM model weights...\n‚úÖ Model downloaded!\nüîÑ Loading SAM model...\n‚úÖ SAM model loaded on cuda!\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# ============================================================================\n# CELL 3: Setup Directories (KAGGLE VERSION)\n# ============================================================================\n# I-setup ang folder structure para sa Kaggle\n# ============================================================================\n\n# Kaggle Paths Configuration\nKAGGLE_INPUT_DIR = \"/kaggle/input\"\nKAGGLE_WORKING_DIR = \"/kaggle/working\"\nDATASET_NAME = \"early-stage\"\nSIGATOKA_FOLDER = \"Sigatoka pics\"\n\n# Input: Read-only from Kaggle input\nRAW_DATA_DIR = f\"{KAGGLE_INPUT_DIR}/{DATASET_NAME}/{SIGATOKA_FOLDER}\"\n\n# Output: Writeable sa Kaggle working directory\nBASE_DIR = f\"{KAGGLE_WORKING_DIR}/banana_sigatoka_dataset\"\nANNOTATED_DIR = f\"{BASE_DIR}/annotated\"\nAUGMENTED_DIR = f\"{BASE_DIR}/augmented\"\nVISUALIZATIONS_DIR = f\"{BASE_DIR}/visualizations\"\n\n# Create output directories\ndirectories = [\n    f\"{ANNOTATED_DIR}/images\",\n    f\"{ANNOTATED_DIR}/labels\",\n    f\"{AUGMENTED_DIR}/images\",\n    f\"{AUGMENTED_DIR}/labels\",\n    VISUALIZATIONS_DIR\n]\n\nfor directory in directories:\n    os.makedirs(directory, exist_ok=True)\n\nprint(\"‚úÖ Directories created successfully!\")\nprint(f\"\\nüìÅ Input: {RAW_DATA_DIR}\")\nprint(f\"üìÅ Output: {BASE_DIR}\")\n\n# Check if input directory exists and count images\nif os.path.exists(RAW_DATA_DIR):\n    print(f\"\\n‚úÖ Dataset found!\")\n    \n    # Count images in stage folders\n    stage_folders = ['Stage1', 'Stage2', 'Stage3']\n    stage_counts = {}\n    total_images = 0\n    \n    for stage in stage_folders:\n        stage_path = os.path.join(RAW_DATA_DIR, stage)\n        if os.path.exists(stage_path):\n            images = []\n            for ext in ['*.jpg', '*.jpeg', '*.png', '*.JPG', '*.JPEG', '*.PNG']:\n                images.extend(list(Path(stage_path).glob(ext)))\n            if len(images) > 0:\n                stage_counts[stage] = len(images)\n                total_images += len(images)\n    \n    # Display summary\n    print(f\"\\nüìä Images found:\")\n    for stage, count in stage_counts.items():\n        print(f\"   {stage}: {count} images\")\n    print(f\"\\nüìà Total: {total_images} images\")\n    \nelse:\n    print(f\"\\n‚ö†Ô∏è  Dataset not found: {RAW_DATA_DIR}\")\n    print(f\"üí° Make sure you've added the dataset '{DATASET_NAME}' to your Kaggle notebook\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T07:54:24.931128Z","iopub.execute_input":"2025-11-27T07:54:24.931969Z","iopub.status.idle":"2025-11-27T07:54:24.962846Z","shell.execute_reply.started":"2025-11-27T07:54:24.931943Z","shell.execute_reply":"2025-11-27T07:54:24.962144Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Directories created successfully!\n\nüìÅ Input: /kaggle/input/early-stage/Sigatoka pics\nüìÅ Output: /kaggle/working/banana_sigatoka_dataset\n\n‚úÖ Dataset found!\n\nüìä Images found:\n   Stage1: 13 images\n   Stage2: 13 images\n   Stage3: 13 images\n\nüìà Total: 39 images\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# ============================================================================\n# CELL 4: Auto-Annotation Function (with SAM for accurate detection)\n# ============================================================================\n# Automatic detection ng spots at whole leaf using SAM\n# ============================================================================\n\ndef detect_spots_with_sam(image, sam_predictor, min_spot_area, w, h):\n    \"\"\"\n    Use SAM to detect spots more accurately\n    \"\"\"\n    if sam_predictor is None:\n        return []\n    \n    # Convert BGR to RGB for SAM\n    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    \n    # Set image for SAM\n    sam_predictor.set_image(image_rgb)\n    \n    # First, use color-based detection to get candidate points\n    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n    l_channel, _, _ = cv2.split(lab)\n    \n    # Detect potential spot areas\n    _, dark_spots = cv2.threshold(l_channel, 80, 255, cv2.THRESH_BINARY_INV)\n    lower_brown = np.array([5, 50, 50])\n    upper_brown = np.array([30, 255, 220])\n    brown_spots = cv2.inRange(hsv, lower_brown, upper_brown)\n    spot_mask = cv2.bitwise_or(dark_spots, brown_spots)\n    \n    # Clean up\n    kernel = np.ones((5, 5), np.uint8)\n    spot_mask = cv2.morphologyEx(spot_mask, cv2.MORPH_OPEN, kernel)\n    spot_mask = cv2.morphologyEx(spot_mask, cv2.MORPH_CLOSE, kernel)\n    \n    # Find contours to get candidate points\n    contours, _ = cv2.findContours(spot_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    \n    spots = []\n    for contour in contours:\n        area = cv2.contourArea(contour)\n        if min_spot_area < area < (w * h * 0.15):\n            # Get center point of contour\n            M = cv2.moments(contour)\n            if M[\"m00\"] != 0:\n                cx = int(M[\"m10\"] / M[\"m00\"])\n                cy = int(M[\"m01\"] / M[\"m00\"])\n                \n                # Use SAM to segment at this point\n                try:\n                    masks, scores, logits = sam_predictor.predict(\n                        point_coords=np.array([[cx, cy]]),\n                        point_labels=np.array([1]),  # 1 = foreground\n                        multimask_output=False,\n                    )\n                    \n                    # Get the best mask\n                    if len(masks) > 0 and scores[0] > 0.5:  # Confidence threshold\n                        mask = masks[0]\n                        \n                        # Convert mask to bounding box\n                        y_indices, x_indices = np.where(mask)\n                        if len(x_indices) > 0 and len(y_indices) > 0:\n                            x_min, x_max = int(x_indices.min()), int(x_indices.max())\n                            y_min, y_max = int(y_indices.min()), int(y_indices.max())\n                            box_w = x_max - x_min\n                            box_h = y_max - y_min\n                            \n                            # Filter by size\n                            if min_spot_area < (box_w * box_h) < (w * h * 0.15):\n                                spots.append((x_min, y_min, box_w, box_h))\n                except:\n                    continue\n    \n    return spots\n\n\ndef detect_spots_improved(image, hsv, lab, min_spot_area, w, h):\n    \"\"\"\n    Improved color-based spot detection (fallback if SAM not available)\n    \"\"\"\n    l_channel, a_channel, b_channel = cv2.split(lab)\n    \n    # Method 1: Detect dark spots\n    _, dark_spots = cv2.threshold(l_channel, 80, 255, cv2.THRESH_BINARY_INV)\n    \n    # Method 2: Detect brown/yellow discoloration\n    lower_brown = np.array([5, 50, 50])\n    upper_brown = np.array([30, 255, 220])\n    brown_spots = cv2.inRange(hsv, lower_brown, upper_brown)\n    \n    # Method 3: Detect red/brown spots\n    lower_red = np.array([0, 50, 50])\n    upper_red = np.array([10, 255, 255])\n    red_spots = cv2.inRange(hsv, lower_red, upper_red)\n    \n    # Combine all methods\n    spot_mask = cv2.bitwise_or(dark_spots, brown_spots)\n    spot_mask = cv2.bitwise_or(spot_mask, red_spots)\n    \n    # Better noise removal\n    kernel_small = np.ones((5, 5), np.uint8)\n    spot_mask = cv2.morphologyEx(spot_mask, cv2.MORPH_OPEN, kernel_small)\n    spot_mask = cv2.morphologyEx(spot_mask, cv2.MORPH_CLOSE, kernel_small)\n    spot_mask = cv2.morphologyEx(spot_mask, cv2.MORPH_OPEN, np.ones((7, 7), np.uint8))\n    \n    # Find spot contours\n    spot_contours, _ = cv2.findContours(spot_mask, cv2.RETR_EXTERNAL, \n                                         cv2.CHAIN_APPROX_SIMPLE)\n    \n    spots = []\n    for contour in spot_contours:\n        area = cv2.contourArea(contour)\n        if min_spot_area < area < (w * h * 0.15):\n            x, y, box_w, box_h = cv2.boundingRect(contour)\n            aspect_ratio = box_w / box_h if box_h > 0 else 0\n            if 0.3 < aspect_ratio < 3.0:\n                spots.append((x, y, box_w, box_h))\n    \n    return spots\n\n\ndef auto_annotate_banana_leaf(image_path, output_image_dir, output_label_dir, \n                               visualize=True, min_spot_area=50, output_filename=None,\n                               stage_class_id=1, use_sam=True):\n    \"\"\"\n    Auto-detect banana leaf spots and create YOLO annotations using SAM\n    \n    Parameters:\n    - image_path: path ng image\n    - output_image_dir: saan isasave yung images\n    - output_label_dir: saan isasave yung annotations\n    - visualize: True kung gusto mo makita yung results\n    - min_spot_area: minimum size ng spot\n    - output_filename: custom filename\n    - stage_class_id: class ID para sa stage (1=stage1, 2=stage2, 3=stage3)\n    - use_sam: True kung gusto mo gamitin ang SAM\n    \n    Classes:\n    0 = whole_leaf (buong dahon)\n    1 = stage1\n    2 = stage2\n    3 = stage3\n    \"\"\"\n    \n    # Read image\n    image = cv2.imread(image_path)\n    if image is None:\n        print(f\"‚ùå Cannot read image: {image_path}\")\n        return None\n    \n    original = image.copy()\n    h, w = image.shape[:2]\n    \n    # Convert to different color spaces\n    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n    \n    annotations = []\n    \n    # ========================================\n    # STEP 1: Detect WHOLE LEAF (buong dahon)\n    # ========================================\n    \n    # Improved green detection\n    lower_green = np.array([30, 30, 30])\n    upper_green = np.array([90, 255, 255])\n    leaf_mask = cv2.inRange(hsv, lower_green, upper_green)\n    \n    # Better morphological operations\n    kernel_large = np.ones((20, 20), np.uint8)\n    leaf_mask = cv2.morphologyEx(leaf_mask, cv2.MORPH_CLOSE, kernel_large)\n    leaf_mask = cv2.morphologyEx(leaf_mask, cv2.MORPH_OPEN, kernel_large)\n    leaf_mask = cv2.morphologyEx(leaf_mask, cv2.MORPH_CLOSE, np.ones((30, 30), np.uint8))\n    \n    # Find contours for whole leaf\n    contours, _ = cv2.findContours(leaf_mask, cv2.RETR_EXTERNAL, \n                                     cv2.CHAIN_APPROX_SIMPLE)\n    \n    if contours:\n        largest_contour = max(contours, key=cv2.contourArea)\n        x, y, box_w, box_h = cv2.boundingRect(largest_contour)\n        \n        # Convert to YOLO format\n        x_center = (x + box_w / 2) / w\n        y_center = (y + box_h / 2) / h\n        norm_width = box_w / w\n        norm_height = box_h / h\n        \n        annotations.append(f\"0 {x_center:.6f} {y_center:.6f} {norm_width:.6f} {norm_height:.6f}\")\n        \n        if visualize:\n            cv2.rectangle(original, (x, y), (x + box_w, y + box_h), (0, 255, 0), 5)\n            cv2.putText(original, \"WHOLE LEAF\", (x, y - 10), \n                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 3)\n    \n    # ========================================\n    # STEP 2: Detect SPOTS using SAM or improved method\n    # ========================================\n    \n    if use_sam and SAM_AVAILABLE and sam_predictor is not None:\n        spots = detect_spots_with_sam(image, sam_predictor, min_spot_area, w, h)\n        method = \"SAM\"\n    else:\n        spots = detect_spots_improved(image, hsv, lab, min_spot_area, w, h)\n        method = \"Color-based\"\n    \n    # Filter and annotate spots\n    spot_count = 0\n    \n    for spot in spots:\n        x, y, box_w, box_h = spot\n        \n        # Convert to YOLO format\n        x_center = (x + box_w / 2) / w\n        y_center = (y + box_h / 2) / h\n        norm_width = box_w / w\n        norm_height = box_h / h\n        \n        # Add spot annotation with correct stage class ID\n        annotations.append(f\"{stage_class_id} {x_center:.6f} {y_center:.6f} {norm_width:.6f} {norm_height:.6f}\")\n        \n        if visualize:\n            stage_label = f\"STAGE{stage_class_id}\"\n            cv2.rectangle(original, (x, y), (x + box_w, y + box_h), (0, 0, 255), 4)\n            cv2.putText(original, stage_label, (x, y - 5), \n                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n        \n        spot_count += 1\n    \n    # Save files\n    if output_filename:\n        image_name = output_filename\n    else:\n        image_name = Path(image_path).stem\n    \n    # Save annotation file (.txt)\n    label_path = os.path.join(output_label_dir, f\"{image_name}.txt\")\n    with open(label_path, 'w') as f:\n        f.write('\\n'.join(annotations))\n    \n    # Copy original image to output\n    output_img_path = os.path.join(output_image_dir, f\"{image_name}.jpg\")\n    shutil.copy(image_path, output_img_path)\n    \n    # Save visualization\n    if visualize:\n        vis_path = os.path.join(VISUALIZATIONS_DIR, f\"{image_name}_annotated.jpg\")\n        cv2.imwrite(vis_path, original)\n    \n    stage_name = f\"stage{stage_class_id}\"\n    print(f\"‚úÖ {image_name}: {len(annotations)} total boxes (1 leaf + {spot_count} {stage_name} spots) [{method}]\")\n    \n    return annotations, original","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T07:54:29.214176Z","iopub.execute_input":"2025-11-27T07:54:29.214811Z","iopub.status.idle":"2025-11-27T07:54:29.237195Z","shell.execute_reply.started":"2025-11-27T07:54:29.214786Z","shell.execute_reply":"2025-11-27T07:54:29.236399Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# ============================================================================\n# CELL 5: Process All Images (Batch Auto-Annotation) - IMPROVED SAM\n# ============================================================================\n\ndef annotate_all_images(input_folder, output_image_dir, output_label_dir, \n                        min_spot_area=100, preserve_stage_info=True, use_sam=True):\n    \"\"\"\n    Process all images using improved SAM for accurate annotation\n    \"\"\"\n    \n    # Safely check if SAM is available\n    try:\n        sam_available = SAM_AVAILABLE if 'SAM_AVAILABLE' in globals() else False\n        sam_gen = sam_mask_generator if 'sam_mask_generator' in globals() and sam_mask_generator is not None else None\n    except:\n        sam_available = False\n        sam_gen = None\n    \n    supported_formats = ['.jpg', '.jpeg', '.png', '.JPG', '.JPEG', '.PNG']\n    image_files = []\n    \n    for ext in supported_formats:\n        image_files.extend(Path(input_folder).rglob(f\"*{ext}\"))\n    \n    if not image_files:\n        print(f\"‚ùå No images found in {input_folder}\")\n        return\n    \n    print(f\"üì∏ Found {len(image_files)} images\")\n    \n    # Determine detection method\n    if use_sam and sam_available and sam_gen is not None:\n        detection_method = \"SAM (Improved)\"\n    else:\n        detection_method = \"Color-based\"\n        if use_sam:\n            print(\"‚ö†Ô∏è  SAM requested but not available. Using color-based detection.\")\n    \n    print(f\"üîß Using {detection_method} detection\")\n    print(\"üöÄ Starting auto-annotation...\\n\")\n    \n    success_count = 0\n    stage_counts = {'Stage1': 0, 'Stage2': 0, 'Stage3': 0, 'other': 0}\n    \n    for idx, image_path in enumerate(image_files, 1):\n        # Extract stage info\n        path_parts = image_path.parts\n        stage_name = None\n        stage_class_id = 1\n        \n        for part in path_parts:\n            if 'stage1' in part.lower() or part == 'Stage1':\n                stage_name = 'Stage1'\n                stage_class_id = 1\n                break\n            elif 'stage2' in part.lower() or part == 'Stage2':\n                stage_name = 'Stage2'\n                stage_class_id = 2\n                break\n            elif 'stage3' in part.lower() or part == 'Stage3':\n                stage_name = 'Stage3'\n                stage_class_id = 3\n                break\n        \n        original_name = image_path.stem\n        if preserve_stage_info and stage_name:\n            new_filename = f\"{stage_name.lower()}_{original_name}\"\n            stage_counts[stage_name] = stage_counts.get(stage_name, 0) + 1\n        else:\n            new_filename = original_name\n            stage_counts['other'] += 1\n        \n        print(f\"[{idx}/{len(image_files)}] Processing: {image_path.name} ‚Üí {stage_name or 'unknown'}\")\n        \n        try:\n            result = auto_annotate_banana_leaf(\n                str(image_path), \n                output_image_dir, \n                output_label_dir,\n                visualize=True,\n                min_spot_area=min_spot_area,\n                output_filename=new_filename,\n                stage_class_id=stage_class_id,\n                use_sam=use_sam\n            )\n            if result:\n                success_count += 1\n        except Exception as e:\n            print(f\"‚ùå Error processing {image_path.name}: {e}\")\n    \n    print(f\"\\n{'='*70}\")\n    print(f\"‚úÖ Annotation complete!\")\n    print(f\"{'='*70}\")\n    print(f\"üìä Successfully annotated: {success_count}/{len(image_files)} images\")\n    print(f\"\\nüìà Images by stage:\")\n    for stage, count in stage_counts.items():\n        if count > 0:\n            print(f\"   {stage}: {count} images\")\n    print(f\"\\nüìÅ Images saved to: {output_image_dir}\")\n    print(f\"üìÅ Labels saved to: {output_label_dir}\")\n    print(f\"üìÅ Visualizations: {VISUALIZATIONS_DIR}\")\n\n\n# RUN ANNOTATION - Uncomment to execute\nannotate_all_images(\n    input_folder=RAW_DATA_DIR,\n    output_image_dir=f\"{ANNOTATED_DIR}/images\",\n    output_label_dir=f\"{ANNOTATED_DIR}/labels\",\n    min_spot_area=100,\n    preserve_stage_info=True,\n    use_sam=True  # Will auto-fallback to color-based if SAM not available\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T07:54:34.458326Z","iopub.execute_input":"2025-11-27T07:54:34.458904Z","iopub.status.idle":"2025-11-27T07:56:06.243355Z","shell.execute_reply.started":"2025-11-27T07:54:34.458884Z","shell.execute_reply":"2025-11-27T07:56:06.242657Z"}},"outputs":[{"name":"stdout","text":"üì∏ Found 39 images\n‚ö†Ô∏è  SAM requested but not available. Using color-based detection.\nüîß Using Color-based detection\nüöÄ Starting auto-annotation...\n\n[1/39] Processing: IMG_20251122_094534.jpg ‚Üí Stage1\n‚úÖ stage1_IMG_20251122_094534: 1 total boxes (1 leaf + 0 stage1 spots) [SAM]\n[2/39] Processing: IMG_20251122_093458.jpg ‚Üí Stage1\n‚úÖ stage1_IMG_20251122_093458: 1 total boxes (1 leaf + 0 stage1 spots) [SAM]\n[3/39] Processing: IMG_20251122_090007.jpg ‚Üí Stage1\n‚úÖ stage1_IMG_20251122_090007: 2 total boxes (1 leaf + 1 stage1 spots) [SAM]\n[4/39] Processing: IMG_20251122_093454.jpg ‚Üí Stage1\n‚úÖ stage1_IMG_20251122_093454: 1 total boxes (1 leaf + 0 stage1 spots) [SAM]\n[5/39] Processing: IMG_20251122_095349.jpg ‚Üí Stage1\n‚úÖ stage1_IMG_20251122_095349: 1 total boxes (1 leaf + 0 stage1 spots) [SAM]\n[6/39] Processing: IMG_20251122_094843.jpg ‚Üí Stage1\n‚úÖ stage1_IMG_20251122_094843: 1 total boxes (1 leaf + 0 stage1 spots) [SAM]\n[7/39] Processing: IMG_20251122_094954.jpg ‚Üí Stage1\n‚úÖ stage1_IMG_20251122_094954: 1 total boxes (1 leaf + 0 stage1 spots) [SAM]\n[8/39] Processing: IMG_20251122_090944.jpg ‚Üí Stage1\n‚úÖ stage1_IMG_20251122_090944: 9 total boxes (1 leaf + 8 stage1 spots) [SAM]\n[9/39] Processing: IMG_20251122_090650.jpg ‚Üí Stage1\n‚úÖ stage1_IMG_20251122_090650: 1 total boxes (1 leaf + 0 stage1 spots) [SAM]\n[10/39] Processing: IMG_20251122_094825.jpg ‚Üí Stage1\n‚úÖ stage1_IMG_20251122_094825: 2 total boxes (1 leaf + 1 stage1 spots) [SAM]\n[11/39] Processing: IMG_20251122_094743.jpg ‚Üí Stage1\n‚úÖ stage1_IMG_20251122_094743: 1 total boxes (1 leaf + 0 stage1 spots) [SAM]\n[12/39] Processing: IMG_20251122_094831.jpg ‚Üí Stage1\n‚úÖ stage1_IMG_20251122_094831: 1 total boxes (1 leaf + 0 stage1 spots) [SAM]\n[13/39] Processing: IMG_20251122_094849.jpg ‚Üí Stage1\n‚úÖ stage1_IMG_20251122_094849: 3 total boxes (1 leaf + 2 stage1 spots) [SAM]\n[14/39] Processing: IMG_20251122_091531.jpg ‚Üí Stage2\n‚úÖ stage2_IMG_20251122_091531: 1 total boxes (1 leaf + 0 stage2 spots) [SAM]\n[15/39] Processing: IMG_20251122_091044.jpg ‚Üí Stage2\n‚úÖ stage2_IMG_20251122_091044: 2 total boxes (1 leaf + 1 stage2 spots) [SAM]\n[16/39] Processing: IMG_20251122_093905.jpg ‚Üí Stage2\n‚úÖ stage2_IMG_20251122_093905: 4 total boxes (1 leaf + 3 stage2 spots) [SAM]\n[17/39] Processing: IMG_20251122_093425.jpg ‚Üí Stage2\n‚úÖ stage2_IMG_20251122_093425: 1 total boxes (1 leaf + 0 stage2 spots) [SAM]\n[18/39] Processing: IMG_20251122_090601.jpg ‚Üí Stage2\n‚úÖ stage2_IMG_20251122_090601: 2 total boxes (1 leaf + 1 stage2 spots) [SAM]\n[19/39] Processing: IMG_20251122_091202.jpg ‚Üí Stage2\n‚úÖ stage2_IMG_20251122_091202: 1 total boxes (1 leaf + 0 stage2 spots) [SAM]\n[20/39] Processing: IMG_20251122_095026.jpg ‚Üí Stage2\n‚úÖ stage2_IMG_20251122_095026: 2 total boxes (1 leaf + 1 stage2 spots) [SAM]\n[21/39] Processing: IMG_20251122_094939.jpg ‚Üí Stage2\n‚úÖ stage2_IMG_20251122_094939: 1 total boxes (1 leaf + 0 stage2 spots) [SAM]\n[22/39] Processing: IMG_20251122_093659.jpg ‚Üí Stage2\n‚úÖ stage2_IMG_20251122_093659: 1 total boxes (1 leaf + 0 stage2 spots) [SAM]\n[23/39] Processing: IMG_20251122_094923.jpg ‚Üí Stage2\n‚úÖ stage2_IMG_20251122_094923: 1 total boxes (1 leaf + 0 stage2 spots) [SAM]\n[24/39] Processing: IMG_20251122_095302.jpg ‚Üí Stage2\n‚úÖ stage2_IMG_20251122_095302: 1 total boxes (1 leaf + 0 stage2 spots) [SAM]\n[25/39] Processing: IMG_20251122_093423.jpg ‚Üí Stage2\n‚úÖ stage2_IMG_20251122_093423: 1 total boxes (1 leaf + 0 stage2 spots) [SAM]\n[26/39] Processing: IMG_20251122_095314.jpg ‚Üí Stage2\n‚úÖ stage2_IMG_20251122_095314: 1 total boxes (1 leaf + 0 stage2 spots) [SAM]\n[27/39] Processing: IMG_20251122_092111.jpg ‚Üí Stage3\n‚úÖ stage3_IMG_20251122_092111: 1 total boxes (1 leaf + 0 stage3 spots) [SAM]\n[28/39] Processing: IMG_20251122_094036.jpg ‚Üí Stage3\n‚úÖ stage3_IMG_20251122_094036: 11 total boxes (1 leaf + 10 stage3 spots) [SAM]\n[29/39] Processing: IMG_20251122_093251.jpg ‚Üí Stage3\n‚úÖ stage3_IMG_20251122_093251: 12 total boxes (1 leaf + 11 stage3 spots) [SAM]\n[30/39] Processing: IMG_20251122_094028.jpg ‚Üí Stage3\n‚úÖ stage3_IMG_20251122_094028: 3 total boxes (1 leaf + 2 stage3 spots) [SAM]\n[31/39] Processing: IMG_20251122_093655.jpg ‚Üí Stage3\n‚úÖ stage3_IMG_20251122_093655: 1 total boxes (1 leaf + 0 stage3 spots) [SAM]\n[32/39] Processing: IMG_20251122_091556.jpg ‚Üí Stage3\n‚úÖ stage3_IMG_20251122_091556: 4 total boxes (1 leaf + 3 stage3 spots) [SAM]\n[33/39] Processing: IMG_20251122_094045.jpg ‚Üí Stage3\n‚úÖ stage3_IMG_20251122_094045: 1 total boxes (1 leaf + 0 stage3 spots) [SAM]\n[34/39] Processing: IMG_20251122_095213.jpg ‚Üí Stage3\n‚úÖ stage3_IMG_20251122_095213: 1 total boxes (1 leaf + 0 stage3 spots) [SAM]\n[35/39] Processing: IMG_20251122_092113.jpg ‚Üí Stage3\n‚úÖ stage3_IMG_20251122_092113: 1 total boxes (1 leaf + 0 stage3 spots) [SAM]\n[36/39] Processing: IMG_20251122_095212.jpg ‚Üí Stage3\n‚úÖ stage3_IMG_20251122_095212: 1 total boxes (1 leaf + 0 stage3 spots) [SAM]\n[37/39] Processing: IMG_20251122_091156.jpg ‚Üí Stage3\n‚úÖ stage3_IMG_20251122_091156: 2 total boxes (1 leaf + 1 stage3 spots) [SAM]\n[38/39] Processing: IMG_20251122_095345.jpg ‚Üí Stage3\n‚úÖ stage3_IMG_20251122_095345: 1 total boxes (1 leaf + 0 stage3 spots) [SAM]\n[39/39] Processing: IMG_20251122_093428.jpg ‚Üí Stage3\n‚úÖ stage3_IMG_20251122_093428: 5 total boxes (1 leaf + 4 stage3 spots) [SAM]\n\n======================================================================\n‚úÖ Annotation complete!\n======================================================================\nüìä Successfully annotated: 39/39 images\n\nüìà Images by stage:\n   Stage1: 13 images\n   Stage2: 13 images\n   Stage3: 13 images\n\nüìÅ Images saved to: /kaggle/working/banana_sigatoka_dataset/annotated/images\nüìÅ Labels saved to: /kaggle/working/banana_sigatoka_dataset/annotated/labels\nüìÅ Visualizations: /kaggle/working/banana_sigatoka_dataset/visualizations\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# ============================================================================\n# CELL 5: Process All Images (Batch Auto-Annotation) - KAGGLE VERSION with SAM\n# ============================================================================\n\ndef annotate_all_images(input_folder, output_image_dir, output_label_dir, \n                        min_spot_area=50, preserve_stage_info=True, use_sam=True):\n    \"\"\"\n    Process all images using SAM for accurate annotation\n    \"\"\"\n    \n    supported_formats = ['.jpg', '.jpeg', '.png', '.JPG', '.JPEG', '.PNG']\n    image_files = []\n    \n    for ext in supported_formats:\n        image_files.extend(Path(input_folder).rglob(f\"*{ext}\"))\n    \n    if not image_files:\n        print(f\"‚ùå No images found in {input_folder}\")\n        return\n    \n    print(f\"üì∏ Found {len(image_files)} images\")\n    print(f\"üîß Using {'SAM' if (use_sam and SAM_AVAILABLE and sam_predictor is not None) else 'Color-based'} detection\")\n    print(\"üöÄ Starting auto-annotation...\\n\")\n    \n    success_count = 0\n    stage_counts = {'Stage1': 0, 'Stage2': 0, 'Stage3': 0, 'other': 0}\n    \n    for idx, image_path in enumerate(image_files, 1):\n        # Extract stage info from path\n        path_parts = image_path.parts\n        stage_name = None\n        stage_class_id = 1\n        \n        for part in path_parts:\n            if 'stage1' in part.lower() or part == 'Stage1':\n                stage_name = 'Stage1'\n                stage_class_id = 1\n                break\n            elif 'stage2' in part.lower() or part == 'Stage2':\n                stage_name = 'Stage2'\n                stage_class_id = 2\n                break\n            elif 'stage3' in part.lower() or part == 'Stage3':\n                stage_name = 'Stage3'\n                stage_class_id = 3\n                break\n        \n        # Create filename with stage info\n        original_name = image_path.stem\n        if preserve_stage_info and stage_name:\n            new_filename = f\"{stage_name.lower()}_{original_name}\"\n            stage_counts[stage_name] = stage_counts.get(stage_name, 0) + 1\n        else:\n            new_filename = original_name\n            stage_counts['other'] += 1\n        \n        print(f\"[{idx}/{len(image_files)}] Processing: {image_path.name} ‚Üí {stage_name or 'unknown'}\")\n        \n        try:\n            result = auto_annotate_banana_leaf(\n                str(image_path), \n                output_image_dir, \n                output_label_dir,\n                visualize=True,\n                min_spot_area=min_spot_area,\n                output_filename=new_filename,\n                stage_class_id=stage_class_id,\n                use_sam=use_sam\n            )\n            if result:\n                success_count += 1\n        except Exception as e:\n            print(f\"‚ùå Error processing {image_path.name}: {e}\")\n    \n    print(f\"\\n{'='*70}\")\n    print(f\"‚úÖ Annotation complete!\")\n    print(f\"{'='*70}\")\n    print(f\"üìä Successfully annotated: {success_count}/{len(image_files)} images\")\n    print(f\"\\nüìà Images by stage:\")\n    for stage, count in stage_counts.items():\n        if count > 0:\n            print(f\"   {stage}: {count} images\")\n    print(f\"\\nüìÅ Images saved to: {output_image_dir}\")\n    print(f\"üìÅ Labels saved to: {output_label_dir}\")\n    print(f\"üìÅ Visualizations: {VISUALIZATIONS_DIR}\")\n\n\n# RUN ANNOTATION - Uncomment to execute\nannotate_all_images(\n    input_folder=RAW_DATA_DIR,\n    output_image_dir=f\"{ANNOTATED_DIR}/images\",\n    output_label_dir=f\"{ANNOTATED_DIR}/labels\",\n    min_spot_area=50,\n    preserve_stage_info=True,\n    use_sam=True  # Set to False kung gusto mo color-based lang\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T07:51:39.311252Z","iopub.execute_input":"2025-11-27T07:51:39.311948Z","iopub.status.idle":"2025-11-27T07:51:39.329029Z","shell.execute_reply.started":"2025-11-27T07:51:39.311926Z","shell.execute_reply":"2025-11-27T07:51:39.328413Z"}},"outputs":[{"name":"stdout","text":"‚ùå No images found in /kaggle/working/banana_sigatoka_dataset/annotated/images\nüí° Run annotation first (Cell 5)\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_47/1461429032.py:32: UserWarning: Argument(s) 'var_limit' are not valid for transform GaussNoise\n  A.GaussNoise(                         # Add realistic noise\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# ============================================================================\n# CELL 7: Visualize Annotation Results\n# ============================================================================\n# Tignan kung tama yung annotations\n# ============================================================================\n\ndef visualize_annotation(image_path, label_path, figsize=(15, 10)):\n    \"\"\"\n    Display image with bounding boxes\n    \"\"\"\n    \n    image = cv2.imread(image_path)\n    if image is None:\n        print(f\"‚ùå Cannot read image: {image_path}\")\n        return\n    \n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    h, w = image.shape[:2]\n    \n    # Read annotations\n    if not os.path.exists(label_path):\n        print(f\"‚ö†Ô∏è  No label file: {label_path}\")\n        plt.figure(figsize=figsize)\n        plt.imshow(image)\n        plt.title(\"Image (No annotations)\")\n        plt.axis('off')\n        plt.show()\n        return\n    \n    with open(label_path, 'r') as f:\n        for line in f:\n            parts = line.strip().split()\n            if len(parts) < 5:\n                continue\n                \n            class_id = int(parts[0])\n            x_center, y_center, width, height = [float(x) for x in parts[1:5]]\n            \n            # Convert to pixel coordinates\n            x1 = int((x_center - width / 2) * w)\n            y1 = int((y_center - height / 2) * h)\n            x2 = int((x_center + width / 2) * w)\n            y2 = int((y_center + height / 2) * h)\n            \n            # Draw\n            color = (0, 255, 0) if class_id == 0 else (255, 0, 0)\n            label = \"WHOLE_LEAF\" if class_id == 0 else \"SPOT\"\n            thickness = 3 if class_id == 0 else 2\n            \n            cv2.rectangle(image, (x1, y1), (x2, y2), color, thickness)\n            cv2.putText(image, label, (x1, max(y1 - 10, 20)), \n                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n    \n    # Display\n    plt.figure(figsize=figsize)\n    plt.imshow(image)\n    plt.title(f'Annotated: {Path(image_path).name}')\n    plt.axis('off')\n    plt.tight_layout()\n    plt.show()\n\n\ndef show_random_samples(image_dir, label_dir, num_samples=3):\n    \"\"\"\n    Show random annotated images\n    \"\"\"\n    \n    image_files = list(Path(image_dir).glob(\"*.jpg\")) + list(Path(image_dir).glob(\"*.png\"))\n    \n    if not image_files:\n        print(f\"‚ùå No images in {image_dir}\")\n        return\n    \n    # Random sample\n    import random\n    samples = random.sample(image_files, min(num_samples, len(image_files)))\n    \n    print(f\"üì∏ Showing {len(samples)} random samples:\\n\")\n    \n    for img_path in samples:\n        label_path = Path(label_dir) / f\"{img_path.stem}.txt\"\n        visualize_annotation(str(img_path), str(label_path))\n\n\nVISUALIZE - Uncomment to see results\nshow_random_samples(\n    image_dir=f\"{ANNOTATED_DIR}/images\",\n    label_dir=f\"{ANNOTATED_DIR}/labels\",\n    num_samples=3\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CELL 8: Check Dataset Statistics\n# ============================================================================\n\ndef check_dataset_stats(image_dir, label_dir):\n    \"\"\"\n    Display dataset statistics\n    \"\"\"\n    \n    image_files = list(Path(image_dir).glob(\"*.jpg\")) + list(Path(image_dir).glob(\"*.png\"))\n    \n    if not image_files:\n        print(f\"‚ùå No images in {image_dir}\")\n        return\n    \n    total_images = len(image_files)\n    total_whole_leaf = 0\n    total_spots = 0\n    images_with_spots = 0\n    \n    for img_path in image_files:\n        label_path = Path(label_dir) / f\"{img_path.stem}.txt\"\n        \n        if label_path.exists():\n            with open(label_path, 'r') as f:\n                lines = f.readlines()\n                has_spot = False\n                for line in lines:\n                    parts = line.strip().split()\n                    if len(parts) >= 5:\n                        class_id = int(parts[0])\n                        if class_id == 0:\n                            total_whole_leaf += 1\n                        else:\n                            total_spots += 1\n                            has_spot = True\n                \n                if has_spot:\n                    images_with_spots += 1\n    \n    print(\"=\"*70)\n    print(\"üìä DATASET STATISTICS\")\n    print(\"=\"*70)\n    print(f\"üì∏ Total images: {total_images}\")\n    print(f\"üçÉ Whole leaf boxes: {total_whole_leaf}\")\n    print(f\"üî¥ Disease spots: {total_spots}\")\n    print(f\"üìà Images with spots: {images_with_spots}/{total_images} ({images_with_spots/total_images*100:.1f}%)\")\n    print(f\"üìä Average spots per image: {total_spots/total_images:.2f}\")\n    print(\"=\"*70)\n\n\nCHECK STATS - Uncomment to see\nprint(\"\\nüìä ANNOTATED DATASET:\")\ncheck_dataset_stats(f\"{ANNOTATED_DIR}/images\", f\"{ANNOTATED_DIR}/labels\")\n\nprint(\"\\nüìä AUGMENTED DATASET:\")\ncheck_dataset_stats(f\"{AUGMENTED_DIR}/images\", f\"{AUGMENTED_DIR}/labels\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def run_annotation_pipeline(raw_folder, augmentations_per_image=5, min_spot_area=50):\n    \"\"\"\n    Run complete annotation + augmentation pipeline\n    \n    Steps:\n    1. Auto-annotate all raw images\n    2. Augment annotated dataset\n    3. Show statistics\n    \"\"\"\n    \n    print(\"=\"*70)\n    print(\"üçå BANANA BLACK SIGATOKA - ANNOTATION & AUGMENTATION\")\n    print(\"=\"*70)\n    \n    # Step 1: Annotation\n    print(\"\\nüìç STEP 1: Auto-Annotation\")\n    print(\"-\"*70)\n    annotate_all_images(\n        input_folder=raw_folder,\n        output_image_dir=f\"{ANNOTATED_DIR}/images\",\n        output_label_dir=f\"{ANNOTATED_DIR}/labels\",\n        min_spot_area=min_spot_area\n    )\n    \n    # Step 2: Augmentation\n    print(\"\\nüìç STEP 2: Data Augmentation\")\n    print(\"-\"*70)\n    augment_dataset(\n        image_dir=f\"{ANNOTATED_DIR}/images\",\n        label_dir=f\"{ANNOTATED_DIR}/labels\",\n        output_image_dir=f\"{AUGMENTED_DIR}/images\",\n        output_label_dir=f\"{AUGMENTED_DIR}/labels\",\n        augmentations_per_image=augmentations_per_image\n    )\n    \n    # Step 3: Statistics\n    print(\"\\nüìç STEP 3: Dataset Statistics\")\n    print(\"-\"*70)\n    print(\"\\nüìä ANNOTATED DATASET:\")\n    check_dataset_stats(f\"{ANNOTATED_DIR}/images\", f\"{ANNOTATED_DIR}/labels\")\n    \n    print(\"\\nüìä AUGMENTED DATASET:\")\n    check_dataset_stats(f\"{AUGMENTED_DIR}/images\", f\"{AUGMENTED_DIR}/labels\")\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"‚úÖ PIPELINE COMPLETE!\")\n    print(\"=\"*70)\n    print(f\"\\nüìÅ Output folders:\")\n    print(f\"   ‚Ä¢ Annotated: {ANNOTATED_DIR}\")\n    print(f\"   ‚Ä¢ Augmented: {AUGMENTED_DIR}\")\n    print(f\"   ‚Ä¢ Visualizations: {VISUALIZATIONS_DIR}\")\n    print(f\"\\nüí° Next: Review visualizations, then ready for training!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}