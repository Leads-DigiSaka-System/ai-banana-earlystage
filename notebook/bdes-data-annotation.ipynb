{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13871382,"sourceType":"datasetVersion","datasetId":8838063},{"sourceId":13890221,"sourceType":"datasetVersion","datasetId":8849278}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U ultralytics","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nSAM 2 Auto-Annotation for Black Sigatoka Disease Detection\nStrategy: Color-based detection + SAM 2 refinement\nWorks specifically for banana leaf disease images\n\"\"\"\n\nimport os\nimport cv2\nimport numpy as np\nfrom pathlib import Path\nimport json\nimport shutil\nfrom ultralytics import SAM","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# \"\"\"\n# SAM 2 Auto-Annotation for Black Sigatoka Disease Detection\n# Strategy: Color-based detection + SAM 2 refinement\n# Works specifically for banana leaf disease images\n# \"\"\"\n\n# import os\n# import cv2\n# import numpy as np\n# from pathlib import Path\n# import json\n# import shutil\n# from ultralytics import SAM\n\nclass BlackSigatokaAnnotator:\n    def __init__(self, sam_model=\"sam2_b.pt\"):\n        \"\"\"Initialize SAM 2 model\"\"\"\n        print(f\"Loading SAM 2 model: {sam_model}\")\n        self.sam = SAM(sam_model)\n        print(\"‚úì Model loaded successfully!\")\n        \n        self.classes = {\n            0: \"whole_leaf\",\n            1: \"lesion_discoloration\"\n        }\n    \n    def detect_leaf_and_lesions(self, image):\n        \"\"\"\n        Detect leaf and lesion regions using color analysis\n        Returns bounding boxes for SAM prompts\n        \"\"\"\n        # Make sure image is valid\n        if image is None:\n            print(\"  ‚úó Invalid image\")\n            return []\n            \n        h, w = image.shape[:2]\n        \n        try:\n            # Convert to HSV color space\n            hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n            \n            # === STEP 1: Detect whole leaf (green regions) ===\n            # Fix: Ensure lowerb and upperb are proper numpy arrays\n            lower_green = np.array([25, 20, 20], dtype=np.uint8)\n            upper_green = np.array([95, 255, 255], dtype=np.uint8)\n            leaf_mask = cv2.inRange(hsv, lower_green, upper_green)\n            \n            # Clean up leaf mask\n            kernel_large = np.ones((15, 15), np.uint8)\n            leaf_mask = cv2.morphologyEx(leaf_mask, cv2.MORPH_CLOSE, kernel_large)\n            leaf_mask = cv2.morphologyEx(leaf_mask, cv2.MORPH_OPEN, kernel_large)\n            \n            # Find main leaf contour\n            contours, _ = cv2.findContours(leaf_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n            \n            detections = []\n            \n            if contours:\n                # Get largest contour (main leaf)\n                largest_contour = max(contours, key=cv2.contourArea)\n                area = cv2.contourArea(largest_contour)\n                \n                # Only process if leaf is significant portion of image\n                if area > (h * w * 0.1):  # At least 10% of image\n                    x, y, w_box, h_box = cv2.boundingRect(largest_contour)\n                    \n                    # Expand bbox slightly for better SAM results\n                    padding = 20\n                    x1 = max(0, x - padding)\n                    y1 = max(0, y - padding)\n                    x2 = min(w, x + w_box + padding)\n                    y2 = min(h, y + h_box + padding)\n                    \n                    detections.append({\n                        'bbox': [x1, y1, x2, y2],\n                        'class': 0,  # whole_leaf\n                        'type': 'leaf'\n                    })\n                    \n                    # === STEP 2: Detect lesions within leaf area ===\n                    # Create mask for leaf area only\n                    leaf_region_mask = np.zeros_like(leaf_mask)\n                    cv2.drawContours(leaf_region_mask, [largest_contour], -1, 255, -1)\n                    \n                    # Detect lesions: dark brown/reddish-brown spots (disease)\n                    # Method 1: Brown/reddish-brown in HSV (actual disease color)\n                    # More specific ranges for disease lesions\n                    lower_brown1 = np.array([0, 30, 0], dtype=np.uint8)      # Dark brown/black lesions\n                    upper_brown1 = np.array([25, 255, 80], dtype=np.uint8)  # Brown lesions (lower value = darker)\n                    \n                    # Reddish-brown lesions\n                    lower_brown2 = np.array([160, 30, 0], dtype=np.uint8)\n                    upper_brown2 = np.array([180, 255, 80], dtype=np.uint8)\n                    \n                    brown_mask1 = cv2.inRange(hsv, lower_brown1, upper_brown1)\n                    brown_mask2 = cv2.inRange(hsv, lower_brown2, upper_brown2)\n                    brown_mask = cv2.bitwise_or(brown_mask1, brown_mask2)\n                    brown_mask = cv2.bitwise_and(brown_mask, brown_mask, mask=leaf_region_mask)\n                    \n                    # Method 2: Dark areas in LAB (significantly darker than healthy leaf)\n                    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n                    l_channel = lab[:, :, 0]\n                    masked_l = cv2.bitwise_and(l_channel, l_channel, mask=leaf_region_mask)\n                    \n                    # Get mean and std brightness of healthy leaf\n                    mean_brightness = cv2.mean(l_channel, mask=leaf_region_mask)[0]\n                    std_brightness = np.std(masked_l[masked_l > 0])\n                    \n                    # Only detect areas SIGNIFICANTLY darker (disease lesions are much darker)\n                    dark_threshold = mean_brightness - max(20, std_brightness * 1.5)\n                    dark_mask = np.where((masked_l < dark_threshold) & (masked_l > 0), 255, 0).astype(np.uint8)\n                    dark_mask = cv2.bitwise_and(dark_mask, dark_mask, mask=leaf_region_mask)\n                    \n                    # Method 3: Yellow/light areas (for early stage lesions only)\n                    lower_yellow = np.array([15, 40, 40], dtype=np.uint8)  # More saturated yellow\n                    upper_yellow = np.array([35, 255, 255], dtype=np.uint8)\n                    yellow_mask = cv2.inRange(hsv, lower_yellow, upper_yellow)\n                    yellow_mask = cv2.bitwise_and(yellow_mask, yellow_mask, mask=leaf_region_mask)\n                    \n                    # Combine all methods\n                    lesion_mask = cv2.bitwise_or(brown_mask, dark_mask)\n                    lesion_mask = cv2.bitwise_or(lesion_mask, yellow_mask)\n                    \n                    # Clean up lesion mask (remove noise)\n                    kernel_small = np.ones((7, 7), np.uint8)  # Larger kernel to remove small noise\n                    lesion_mask = cv2.morphologyEx(lesion_mask, cv2.MORPH_OPEN, kernel_small)\n                    lesion_mask = cv2.morphologyEx(lesion_mask, cv2.MORPH_CLOSE, kernel_small)\n                    \n                    # Find lesion contours\n                    # Find lesion contours\n                    lesion_contours, _ = cv2.findContours(lesion_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n                    \n                    for contour in lesion_contours:\n                        area = cv2.contourArea(contour)\n                        \n                        # Filter 1: Minimum size (small lesions allowed)\n                        min_area_pixels = 200\n                        min_area_ratio = 0.0002\n                        min_area = max(min_area_pixels, h * w * min_area_ratio)\n                        \n                        if area < min_area:\n                            continue\n                        \n                        # Get bounding box for shape analysis\n                        x, y, w_box, h_box = cv2.boundingRect(contour)\n                        \n                        # Filter 2: Filter only extremely elongated shapes (veins)\n                        aspect_ratio = max(w_box, h_box) / max(min(w_box, h_box), 1)\n                        if aspect_ratio > 12:\n                            continue\n                        \n                        # Filter 3: Basic circularity (very lenient)\n                        perimeter = cv2.arcLength(contour, True)\n                        if perimeter > 0:\n                            circularity = 4 * np.pi * area / (perimeter * perimeter)\n                            if circularity < 0.12:  # Very lenient\n                                continue\n                        \n                        # Filter 4: Color verification (optional - can be removed if too strict)\n                        try:\n                            mask_roi = np.zeros((h, w), dtype=np.uint8)\n                            cv2.drawContours(mask_roi, [contour], -1, 255, -1)\n                            \n                            mean_color = cv2.mean(image, mask=mask_roi)[:3]\n                            mean_hsv = cv2.cvtColor(np.uint8([[mean_color]]), cv2.COLOR_BGR2HSV)[0][0]\n                            \n                            hue = mean_hsv[0]\n                            saturation = mean_hsv[1]\n                            value = mean_hsv[2]\n                            \n                            # More lenient color check\n                            is_brown = (hue < 30 or hue > 160) and saturation > 20 and value < 120\n                            is_dark = value < (mean_brightness * 0.7)\n                            is_yellowish = (hue >= 15 and hue <= 40) and value > 50\n                            is_green = (hue >= 35 and hue <= 85) and saturation > 40\n                            \n                            # Skip only if clearly green (not brown/dark/yellow)\n                            if is_green and not (is_brown or is_dark or is_yellowish):\n                                continue\n                        except:\n                            # If color check fails, continue anyway (don't filter out)\n                            pass\n                        \n                        # Passed all filters - this is likely a lesion\n                        padding = 10\n                        x1 = max(0, x - padding)\n                        y1 = max(0, y - padding)\n                        x2 = min(w, x + w_box + padding)\n                        y2 = min(h, y + h_box + padding)\n                        \n                        detections.append({\n                            'bbox': [x1, y1, x2, y2],\n                            'class': 1,  # lesion\n                            'type': 'lesion'\n                        })\n            \n            return detections\n            \n        except Exception as e:\n            print(f\"  ‚úó Error in color detection: {e}\")\n            return []\n    \n    def segment_with_sam(self, image_path, detections):\n        \"\"\"Use SAM to create precise masks from bboxes\"\"\"\n        annotations = []\n        \n        img = cv2.imread(str(image_path))\n        if img is None:\n            print(f\"  ‚úó Cannot read image for SAM: {image_path}\")\n            return annotations\n            \n        h, w = img.shape[:2]\n        \n        for det in detections:\n            bbox = det['bbox']\n            class_id = det['class']\n            \n            try:\n                # Run SAM with bbox prompt\n                results = self.sam(str(image_path), bboxes=[bbox], verbose=False)\n                \n                if results and len(results) > 0 and results[0].masks is not None:\n                    masks_data = results[0].masks.data\n                    \n                    if len(masks_data) > 0:\n                        mask = masks_data[0].cpu().numpy()\n                        \n                        # Convert to polygon\n                        mask_uint8 = (mask * 255).astype(np.uint8)\n                        contours, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n                        \n                        if contours:\n                            largest = max(contours, key=cv2.contourArea)\n                            \n                            # Simplify polygon\n                            epsilon = 0.002 * cv2.arcLength(largest, True)\n                            polygon = cv2.approxPolyDP(largest, epsilon, True).reshape(-1, 2)\n                            \n                            if len(polygon) >= 3:\n                                annotations.append({\n                                    'class': class_id,\n                                    'polygon': polygon,\n                                    'mask': mask,\n                                    'type': det['type']\n                                })\n                                continue\n                \n                # Fallback: use bbox as polygon\n                print(f\"  ‚ö† SAM failed for {det['type']}, using bbox\")\n                x1, y1, x2, y2 = bbox\n                polygon = np.array([[x1, y1], [x2, y1], [x2, y2], [x1, y2]])\n                mask = np.zeros((h, w), dtype=np.float32)\n                cv2.rectangle(mask, (x1, y1), (x2, y2), 1.0, -1)\n                \n                annotations.append({\n                    'class': class_id,\n                    'polygon': polygon,\n                    'mask': mask,\n                    'type': det['type']\n                })\n                \n            except Exception as e:\n                print(f\"  ‚úó Error on {det['type']}: {e}\")\n        \n        return annotations\n    \n    def polygon_to_yolo(self, polygon, img_width, img_height):\n        \"\"\"Convert polygon to YOLO format\"\"\"\n        normalized = []\n        for point in polygon:\n            x_norm = max(0, min(1, point[0] / img_width))\n            y_norm = max(0, min(1, point[1] / img_height))\n            normalized.extend([x_norm, y_norm])\n        return ' '.join(f\"{x:.6f}\" for x in normalized)\n    \n    def annotate_image(self, image_path, output_dir):\n        \"\"\"Process single image\"\"\"\n        # Read image\n        img = cv2.imread(str(image_path))\n        if img is None:\n            print(f\"‚úó Cannot read image: {image_path}\")\n            return None\n        \n        h, w = img.shape[:2]\n        \n        # Step 1: Color-based detection\n        detections = self.detect_leaf_and_lesions(img)\n        \n        if not detections:\n            print(f\"  ‚ö† No leaf detected\")\n            return None\n        \n        # Step 2: SAM refinement\n        annotations = self.segment_with_sam(image_path, detections)\n        \n        if not annotations:\n            print(f\"  ‚ö† No valid annotations\")\n            return None\n        \n        # Step 3: Save YOLO format\n        labels_dir = Path(output_dir) / 'labels'\n        labels_dir.mkdir(parents=True, exist_ok=True)\n        \n        label_file = labels_dir / f\"{Path(image_path).stem}.txt\"\n        \n        with open(label_file, 'w') as f:\n            for ann in annotations:\n                yolo_str = self.polygon_to_yolo(ann['polygon'], w, h)\n                f.write(f\"{ann['class']} {yolo_str}\\n\")\n        \n        # Step 4: Visualization\n        vis_dir = Path(output_dir) / 'visualizations'\n        vis_dir.mkdir(parents=True, exist_ok=True)\n        \n        vis_img = img.copy()\n        colors = {0: (0, 255, 0), 1: (0, 255, 255)}\n        \n        for ann in annotations:\n            color = colors[ann['class']]\n            mask_uint8 = (ann['mask'] * 255).astype(np.uint8)\n            contours, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n            \n            # Draw with transparency\n            overlay = vis_img.copy()\n            cv2.drawContours(overlay, contours, -1, color, -1)\n            vis_img = cv2.addWeighted(vis_img, 0.7, overlay, 0.3, 0)\n            cv2.drawContours(vis_img, contours, -1, color, 2)\n            \n            # Add label\n            if contours:\n                M = cv2.moments(contours[0])\n                if M[\"m00\"] != 0:\n                    cx = int(M[\"m10\"] / M[\"m00\"])\n                    cy = int(M[\"m01\"] / M[\"m00\"])\n                    label = self.classes[ann['class']]\n                    cv2.putText(vis_img, label, (cx-40, cy), \n                              cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n        \n        vis_file = vis_dir / f\"{Path(image_path).stem}_annotated.jpg\"\n        cv2.imwrite(str(vis_file), vis_img)\n        \n        return annotations\n    \n    def process_dataset(self, image_dir, output_dir):\n        \"\"\"Process entire dataset\"\"\"\n        image_dir = Path(image_dir)\n        output_dir = Path(output_dir)\n        \n        # Create output structure\n        images_dir = output_dir / 'images'\n        images_dir.mkdir(parents=True, exist_ok=True)\n        \n        # Get all images\n        image_files = list(image_dir.glob('*.jpg')) + \\\n                     list(image_dir.glob('*.jpeg')) + \\\n                     list(image_dir.glob('*.png')) + \\\n                     list(image_dir.glob('*.JPG')) + \\\n                     list(image_dir.glob('*.JPEG')) + \\\n                     list(image_dir.glob('*.PNG'))\n        \n        if not image_files:\n            print(f\"‚úó No images found in {image_dir}\")\n            return []\n        \n        print(f\"\\n{'='*60}\")\n        print(f\"Processing {len(image_files)} images\")\n        print(f\"{'='*60}\\n\")\n        \n        results = []\n        \n        for i, img_path in enumerate(image_files, 1):\n            print(f\"[{i}/{len(image_files)}] {img_path.name}...\")\n            \n            # Copy image\n            try:\n                shutil.copy(img_path, images_dir / img_path.name)\n            except Exception as e:\n                print(f\"  ‚úó Error copying image: {e}\")\n                continue\n            \n            # Annotate\n            try:\n                annotations = self.annotate_image(img_path, output_dir)\n                \n                if annotations:\n                    leaf_count = sum(1 for a in annotations if a['class'] == 0)\n                    lesion_count = sum(1 for a in annotations if a['class'] == 1)\n                    print(f\"  ‚úì Leaf: {leaf_count}, Lesions: {lesion_count}\")\n                    \n                    results.append({\n                        'image': img_path.name,\n                        'status': 'success',\n                        'leaf_count': leaf_count,\n                        'lesion_count': lesion_count\n                    })\n                else:\n                    print(f\"  ‚ö† No annotations generated\")\n                    results.append({\n                        'image': img_path.name,\n                        'status': 'no_detection'\n                    })\n            except Exception as e:\n                print(f\"  ‚úó Error: {e}\")\n                results.append({\n                    'image': img_path.name,\n                    'status': 'error',\n                    'error': str(e)\n                })\n        \n        # Summary\n        print(f\"\\n{'='*60}\")\n        print(\"SUMMARY\")\n        print(f\"{'='*60}\")\n        success = sum(1 for r in results if r['status'] == 'success')\n        print(f\"‚úì Successfully annotated: {success}/{len(image_files)}\")\n        print(f\"‚úó Failed: {len(image_files) - success}\")\n        \n        total_lesions = sum(r.get('lesion_count', 0) for r in results)\n        print(f\"\\nüìä Total lesions detected: {total_lesions}\")\n        \n        # Save summary\n        with open(output_dir / 'summary.json', 'w') as f:\n            json.dump(results, f, indent=2)\n        \n        # Create dataset YAML\n        yaml_content = f\"\"\"# Black Sigatoka Dataset\npath: {output_dir}\ntrain: images\nval: images\n\nnames:\n  0: whole_leaf\n  1: lesion_discoloration\n\nnc: 2\n\"\"\"\n        with open(output_dir / 'dataset.yaml', 'w') as f:\n            f.write(yaml_content)\n        \n        print(f\"\\n‚úì Dataset config: {output_dir / 'dataset.yaml'}\")\n        \n        return results\n\n\n# ============================================================================\n# MAIN EXECUTION\n# ============================================================================\n\nif __name__ == \"__main__\":\n    # Configuration\n    IMAGE_DIR = \"/kaggle/input/early-stage/Sigatoka pics/Stage1\"  # CHANGE THIS!\n    OUTPUT_DIR = \"/kaggle/working/black_sigatoka_dataset\"\n    \n    print(\"=\"*60)\n    print(\"BLACK SIGATOKA AUTO-ANNOTATION\")\n    print(\"=\"*60)\n    print(\"Strategy: Color Detection + SAM 2 Refinement\")\n    print(\"=\"*60)\n    \n    # Initialize\n    annotator = BlackSigatokaAnnotator(sam_model=\"sam2_b.pt\")\n    \n    # Process\n    results = annotator.process_dataset(IMAGE_DIR, OUTPUT_DIR)\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"‚úì COMPLETE!\")\n    print(\"=\"*60)\n    print(f\"üìÅ Images:         {OUTPUT_DIR}/images/\")\n    print(f\"üè∑Ô∏è  Labels:         {OUTPUT_DIR}/labels/\")\n    print(f\"üé® Visualizations: {OUTPUT_DIR}/visualizations/\")\n    print(f\"üìù Config:         {OUTPUT_DIR}/dataset.yaml\")\n    print(\"=\"*60)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Display annotated images (No matplotlib - uses IPython only)\nfrom IPython.display import display, Image, HTML\nfrom pathlib import Path\n\noutput_dir = Path(\"/kaggle/working/black_sigatoka_dataset\")\nvis_dir = output_dir / 'visualizations'\n\nif vis_dir.exists():\n    vis_files = sorted(list(vis_dir.glob('*_annotated.jpg')))\n    \n    if vis_files:\n        print(f\"\\n{'='*60}\")\n        print(f\"üñºÔ∏è  ANNOTATED IMAGES ({len(vis_files)}):\")\n        print(f\"{'='*60}\\n\")\n        \n        # Display images in grid using HTML\n        images_html = []\n        for vis_file in vis_files:\n            # Read and encode image\n            import base64\n            with open(vis_file, 'rb') as f:\n                img_data = base64.b64encode(f.read()).decode()\n            \n            images_html.append(f\"\"\"\n            <div style='display: inline-block; margin: 10px; text-align: center; vertical-align: top;'>\n                <p style='font-size: 11px; margin: 5px 0; font-weight: bold;'>{vis_file.stem.replace('_annotated', '')}</p>\n                <img src='data:image/jpeg;base64,{img_data}' \n                     style='max-width: 400px; max-height: 400px; border: 2px solid #4CAF50; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.2);'/>\n            </div>\n            \"\"\")\n        \n        html_content = f\"\"\"\n        <div style='display: flex; flex-wrap: wrap; justify-content: center; gap: 15px; padding: 20px;'>\n            {''.join(images_html)}\n        </div>\n        \"\"\"\n        \n        display(HTML(html_content))\n        \n        # Print list\n        print(f\"\\nüìã Image List ({len(vis_files)}):\")\n        for i, vf in enumerate(vis_files, 1):\n            print(f\"  {i:2d}. {vf.name}\")\n    else:\n        print(\"‚ö† No visualization files found\")\nelse:\n    print(\"‚ö† Visualizations directory not found\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}