{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 1: Clean Installation\n",
    "!pip uninstall -y seaborn scipy -q\n",
    "!pip install numpy==1.24.3 scipy==1.10.1 ultralytics>=8.3.0 roboflow opencv-python pillow matplotlib pyyaml -q\n",
    "!pip install seaborn\n",
    "print(\"‚úÖ Packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 2: GPU Verification (Multi-GPU Support)\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    print(f\"‚úÖ Found {gpu_count} GPU(s):\")\n",
    "    for i in range(gpu_count):\n",
    "        print(f\"   GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"   Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    \n",
    "    # Set device for multi-GPU training\n",
    "    if gpu_count > 1:\n",
    "        DEVICE = list(range(gpu_count))  # Use all GPUs: [0, 1, ...]\n",
    "        print(f\"\\nüöÄ Multi-GPU training enabled: Using GPUs {DEVICE}\")\n",
    "    else:\n",
    "        DEVICE = 0  # Single GPU\n",
    "        print(f\"\\nüìå Single GPU training: Using GPU {DEVICE}\")\n",
    "else:\n",
    "    print(\"‚ùå No GPU detected! Training will be very slow.\")\n",
    "    print(\"Please check: Runtime > Change runtime type > Hardware accelerator > GPU\")\n",
    "    DEVICE = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 3: Fixed Imports (NO SEABORN - causing errors)\n",
    "from ultralytics import YOLO\n",
    "from roboflow import Roboflow\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "# Visualization - using matplotlib only\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset_path = '/kaggle/input/banana-datasets-early-v2/kaggle/working/combined_yolo_dataset'\n",
    "print(\"=\"*60)\n",
    "print(\"üìä ANALYZING: YOLO CLASSIFICATION DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create a working copy in writable directory\n",
    "working_dir = Path('/kaggle/working')\n",
    "dataset_copy_dir = working_dir / 'yolo_classification_dataset'\n",
    "\n",
    "# Copy the dataset to working directory if not already copied\n",
    "if not dataset_copy_dir.exists():\n",
    "    print(f\"üìÅ Copying dataset to {dataset_copy_dir}...\")\n",
    "    shutil.copytree(dataset_path, dataset_copy_dir)\n",
    "    print(\"‚úÖ Dataset copied to working directory\")\n",
    "else:\n",
    "    print(\"‚úÖ Using existing copy in working directory\")\n",
    "\n",
    "# Now use the copy\n",
    "data_yaml_path = dataset_copy_dir / 'data.yaml'\n",
    "\n",
    "# Load data.yaml\n",
    "with open(data_yaml_path, 'r') as file:\n",
    "    data_config = yaml.safe_load(file)\n",
    "\n",
    "print(f\"\\nNumber of classes: {data_config['nc']}\")\n",
    "print(f\"Class names: {data_config['names']}\")\n",
    "\n",
    "# Update paths in the working copy\n",
    "if 'train' in data_config:\n",
    "    # Fix path relative to current location\n",
    "    train_relative = data_config['train'].replace('../', '')\n",
    "    train_path = dataset_copy_dir / train_relative\n",
    "    data_config['train'] = str(train_path)\n",
    "    \n",
    "if 'val' in data_config:\n",
    "    val_relative = data_config['val'].replace('../', '')\n",
    "    val_path = dataset_copy_dir / val_relative\n",
    "    data_config['val'] = str(val_path)\n",
    "    \n",
    "if 'test' in data_config:\n",
    "    test_relative = data_config['test'].replace('../', '')\n",
    "    test_path = dataset_copy_dir / test_relative\n",
    "    data_config['test'] = str(test_path)\n",
    "\n",
    "# Count images\n",
    "train_img_path = Path(data_config['train'])\n",
    "val_img_path = Path(data_config['val'])\n",
    "\n",
    "train_images = len(list(train_img_path.glob('*.jpg'))) + len(list(train_img_path.glob('*.png')))\n",
    "val_images = len(list(val_img_path.glob('*.jpg'))) + len(list(val_img_path.glob('*.png')))\n",
    "\n",
    "if 'test' in data_config:\n",
    "    test_img_path = Path(data_config['test'])\n",
    "    test_images = len(list(test_img_path.glob('*.jpg'))) + len(list(test_img_path.glob('*.png')))\n",
    "else:\n",
    "    test_images = 0\n",
    "\n",
    "print(f\"\\nüìà Dataset Statistics:\")\n",
    "print(f\"Training images: {train_images}\")\n",
    "print(f\"Validation images: {val_images}\")\n",
    "print(f\"Test images: {test_images}\")\n",
    "\n",
    "# Update and save yaml - Now it's writable!\n",
    "data_config['path'] = str(dataset_copy_dir)\n",
    "with open(data_yaml_path, 'w') as f:\n",
    "    yaml.dump(data_config, f, default_flow_style=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset configuration saved to: {data_yaml_path}\")\n",
    "print(f\"Working directory: {dataset_copy_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 6: Visualize sample images with annotations (FIXED - Correct colors and class names)\n",
    "\n",
    "# Class colors matching data-labeling-classification.ipynb\n",
    "# Colors in RGB format (for display after BGR2RGB conversion)\n",
    "CLASS_COLORS_RGB = {\n",
    "    0: (0, 100, 0),        # Healthy: Dark Green (#006400)\n",
    "    1: (0, 255, 0),        # Stage1: Green (#00FF00)\n",
    "    2: (144, 238, 144),    # Stage2: Light Green (#90EE90)\n",
    "    3: (173, 255, 47),    # Stage3: Yellow Green (#ADFF2F)\n",
    "    4: (255, 255, 0),      # Stage4: Yellow (#FFFF00)\n",
    "    5: (255, 165, 0),      # Stage5: Orange (#FFA500)\n",
    "    6: (255, 0, 0)         # Stage6: Red (#FF0000)\n",
    "}\n",
    "\n",
    "def visualize_yolo_annotations(image_path, label_path, class_names_dict):\n",
    "    \"\"\"\n",
    "    Visualization function para sa YOLO format annotations\n",
    "    Uses correct class colors from data-labeling-classification.ipynb\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read image\n",
    "        img = cv2.imread(str(image_path))\n",
    "        if img is None:\n",
    "            print(f\"‚ö†Ô∏è Cannot read image: {image_path}\")\n",
    "            return None\n",
    "            \n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        h, w = img.shape[:2]\n",
    "        \n",
    "        # Check if label file exists\n",
    "        if not label_path.exists():\n",
    "            print(f\"‚ö†Ô∏è No label file for: {image_path.name}\")\n",
    "            return img\n",
    "        \n",
    "        # Read annotations\n",
    "        with open(label_path, 'r') as f:\n",
    "            annotations = f.readlines()\n",
    "        \n",
    "        # Draw bounding boxes\n",
    "        for ann in annotations:\n",
    "            parts = ann.strip().split()\n",
    "            if len(parts) != 5:\n",
    "                continue\n",
    "                \n",
    "            class_id, x_center, y_center, width, height = map(float, parts)\n",
    "            class_id = int(class_id)\n",
    "            \n",
    "            # Convert YOLO format to pixel coordinates\n",
    "            x1 = int((x_center - width/2) * w)\n",
    "            y1 = int((y_center - height/2) * h)\n",
    "            x2 = int((x_center + width/2) * w)\n",
    "            y2 = int((y_center + height/2) * h)\n",
    "            \n",
    "            # Get color based on class (using correct color scheme)\n",
    "            color = CLASS_COLORS_RGB.get(class_id, (128, 128, 128))  # Default to gray if unknown\n",
    "            \n",
    "            # Draw rectangle and label\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), color, 3)\n",
    "            \n",
    "            # Get class name from the provided dictionary\n",
    "            class_name = class_names_dict.get(class_id, f\"Class_{class_id}\")\n",
    "            \n",
    "            # Add label with background\n",
    "            label = f\"{class_name}\"\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            font_scale = 0.7\n",
    "            thickness = 2\n",
    "            \n",
    "            (text_width, text_height), _ = cv2.getTextSize(label, font, font_scale, thickness)\n",
    "            cv2.rectangle(img, (x1, y1-text_height-10), (x1+text_width+10, y1), color, -1)\n",
    "            cv2.putText(img, label, (x1+5, y1-5), font, font_scale, (0, 0, 0), thickness)\n",
    "        \n",
    "        return img\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Get the correct paths - FIX: Resolve relative paths properly\n",
    "print(\"üîç Looking for images...\\n\")\n",
    "\n",
    "# Resolve the train path relative to dataset_path\n",
    "dataset_path =\"/kaggle/working/yolo_classification_dataset\"\n",
    "base_path = Path(dataset_path)\n",
    "\n",
    "# Get base path from the actual train path in config\n",
    "train_path = Path(data_config['train'])  # This is already updated to yolo_classification_dataset\n",
    "base_path = train_path.parent.parent  # Go up from train/images to yolo_classification_dataset\n",
    "train_img_dir = train_path\n",
    "train_label_dir = base_path / 'train' / 'labels'\n",
    "\n",
    "print(f\"Base dataset path: {base_path}\")\n",
    "print(f\"Image directory: {train_img_dir}\")\n",
    "print(f\"Label directory: {train_label_dir}\")\n",
    "print(f\"Image directory exists: {train_img_dir.exists()}\")\n",
    "print(f\"Label directory exists: {train_label_dir.exists()}\")\n",
    "\n",
    "# Get ALL image files (jpg and png)\n",
    "all_images = list(train_img_dir.glob('*.jpg')) + list(train_img_dir.glob('*.png'))\n",
    "print(f\"Total images found: {len(all_images)}\")\n",
    "\n",
    "if len(all_images) == 0:\n",
    "    print(\"‚ùå No images found! Check the paths.\")\n",
    "    print(f\"Trying alternative path resolution...\")\n",
    "    # Try alternative path resolution\n",
    "    if 'train' in data_config['train']:\n",
    "        train_img_dir = base_path / 'train' / 'images'\n",
    "        train_label_dir = base_path / 'train' / 'labels'\n",
    "        all_images = list(train_img_dir.glob('*.jpg')) + list(train_img_dir.glob('*.png'))\n",
    "        print(f\"Alternative path - Images found: {len(all_images)}\")\n",
    "        print(f\"Alternative image dir: {train_img_dir}\")\n",
    "        print(f\"Alternative label dir: {train_label_dir}\")\n",
    "\n",
    "if len(all_images) > 0:\n",
    "    import random\n",
    "    \n",
    "    # Use actual class names from data_config (7 classes: Healthy, Stage1-Stage6)\n",
    "    class_names_map = data_config['names']  # This is already a dict: {0: 'Healthy', 1: 'Stage1', ...}\n",
    "    \n",
    "    # Group images by the stages they contain\n",
    "    # This ensures we get samples from different stages\n",
    "    print(\"üìã Grouping images by stage...\")\n",
    "    images_by_stage = {i: [] for i in range(7)}  # One list per stage (0-6)\n",
    "    \n",
    "    for img_path in all_images:\n",
    "        label_path = train_label_dir / f\"{img_path.stem}.txt\"\n",
    "        if label_path.exists():\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) >= 1:\n",
    "                        class_id = int(parts[0])\n",
    "                        if 0 <= class_id <= 6:\n",
    "                            # Add image to this stage's list (avoid duplicates)\n",
    "                            if img_path not in images_by_stage[class_id]:\n",
    "                                images_by_stage[class_id].append(img_path)\n",
    "    \n",
    "    # Print stage availability\n",
    "    print(\"\\nüìä Available images per stage:\")\n",
    "    for stage_id in range(7):\n",
    "        stage_name = class_names_map.get(stage_id, f\"Stage{stage_id}\")\n",
    "        count = len(images_by_stage[stage_id])\n",
    "        print(f\"  {stage_name}: {count} images\")\n",
    "    \n",
    "    # Select one image from each available stage (prioritize diversity)\n",
    "    sample_images = []\n",
    "    selected_stages = []\n",
    "    selected_image_paths = set()  # Track selected images to avoid duplicates\n",
    "    \n",
    "    # Try to get one sample from each stage (0-6)\n",
    "    for stage_id in range(7):\n",
    "        if len(images_by_stage[stage_id]) > 0:\n",
    "            # Get available images for this stage that we haven't selected yet\n",
    "            available_for_stage = [img for img in images_by_stage[stage_id] if img not in selected_image_paths]\n",
    "            \n",
    "            if len(available_for_stage) > 0:\n",
    "                # Randomly select one image from this stage\n",
    "                selected_img = random.choice(available_for_stage)\n",
    "                sample_images.append(selected_img)\n",
    "                selected_image_paths.add(selected_img)\n",
    "                selected_stages.append(stage_id)\n",
    "                stage_name = class_names_map.get(stage_id, f\"Stage{stage_id}\")\n",
    "                print(f\"‚úì Selected {stage_name} sample: {selected_img.name}\")\n",
    "    \n",
    "    # If we have less than 4 samples, fill with random samples from any stage\n",
    "    if len(sample_images) < 4:\n",
    "        remaining_needed = 4 - len(sample_images)\n",
    "        # Get images we haven't selected yet\n",
    "        remaining_images = [img for img in all_images if img not in selected_image_paths]\n",
    "        if len(remaining_images) > 0:\n",
    "            additional_samples = random.sample(remaining_images, min(remaining_needed, len(remaining_images)))\n",
    "            sample_images.extend(additional_samples)\n",
    "            selected_image_paths.update(additional_samples)\n",
    "            print(f\"‚úì Added {len(additional_samples)} additional random samples\")\n",
    "    \n",
    "    # Limit to 4 samples for 2x2 grid (should already be unique, but ensure it)\n",
    "    sample_images = sample_images[:4]\n",
    "    \n",
    "    print(f\"\\n‚úÖ Selected {len(sample_images)} diverse samples from different stages\")\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    images_processed = 0\n",
    "    \n",
    "    for idx, img_path in enumerate(sample_images):\n",
    "        # Corresponding label file\n",
    "        label_path = train_label_dir / f\"{img_path.stem}.txt\"\n",
    "        \n",
    "        # Get the primary stage for this image (for title)\n",
    "        primary_stage = \"Unknown\"\n",
    "        stage_classes = set()\n",
    "        if label_path.exists():\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) >= 1:\n",
    "                        class_id = int(parts[0])\n",
    "                        if 0 <= class_id <= 6:\n",
    "                            stage_classes.add(class_id)\n",
    "                            primary_stage = class_names_map.get(class_id, f\"Stage{class_id}\")\n",
    "        \n",
    "        # Visualize with correct class names\n",
    "        img = visualize_yolo_annotations(img_path, label_path, class_names_map)\n",
    "        \n",
    "        if img is not None:\n",
    "            axes[idx].imshow(img)\n",
    "            # Show stage in title\n",
    "            stages_str = \", \".join([class_names_map.get(sid, f\"Stage{sid}\") for sid in sorted(stage_classes)])\n",
    "            axes[idx].set_title(f'Sample {idx+1}: {primary_stage}\\n{img_path.name}', fontsize=10, fontweight='bold')\n",
    "            axes[idx].axis('off')\n",
    "            images_processed += 1\n",
    "            \n",
    "            # Print annotation info\n",
    "            if label_path.exists():\n",
    "                with open(label_path, 'r') as f:\n",
    "                    num_objects = len(f.readlines())\n",
    "                print(f\"‚úì {img_path.name}: {num_objects} object(s) - Stages: {stages_str}\")\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for idx in range(images_processed, 4):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('sample_annotations.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Processed {images_processed} images\")\n",
    "    print(\"‚úÖ Visualization saved as 'sample_annotations.png'\")\n",
    "    \n",
    "    # Show class distribution for ALL 7 classes\n",
    "    print(\"\\nüìä Quick Class Distribution Check (first 100 labels):\")\n",
    "    class_counts = {i: 0 for i in range(7)}  # Initialize all 7 classes\n",
    "    \n",
    "    for label_file in list(train_label_dir.glob('*.txt'))[:100]:\n",
    "        with open(label_file, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 1:\n",
    "                    class_id = int(parts[0])\n",
    "                    if 0 <= class_id <= 6:\n",
    "                        class_counts[class_id] = class_counts.get(class_id, 0) + 1\n",
    "    \n",
    "    for class_id in range(7):\n",
    "        count = class_counts.get(class_id, 0)\n",
    "        class_name = class_names_map.get(class_id, f\"Class_{class_id}\")\n",
    "        print(f\"  {class_name}: {count} instances\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Could not find images. Please verify the dataset path structure.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 7: Setup YOLO Model\n",
    "\n",
    "# Load YOLO model (using YOLO11 for compatibility)\n",
    "model = YOLO('yolo12n.pt')  # You can change to yolo11s.pt, yolo11m.pt, yolo11l.pt, yolo11x.pt for larger models\n",
    "\n",
    "print(\"‚úÖ YOLO model loaded successfully!\")\n",
    "print(f\"Model type: {type(model)}\")\n",
    "\n",
    "print(\"‚úÖ Enhanced evaluation tools loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 8: Train YOLO Model with Early Detection Optimization\n",
    "print(\"üöÄ TRAINING YOLO MODEL FOR EARLY DISEASE DETECTION\")\n",
    "print(\"=\" * 60)\n",
    "data_yaml_path = '/kaggle/working/yolo_classification_dataset/data.yaml'\n",
    "\n",
    "# ‚úÖ FIXED: Removed duplicate cos_lr, deprecated parameters, and corrected all settings\n",
    "training_config = {\n",
    "    # Basic settings\n",
    "    'data': data_yaml_path,\n",
    "    'epochs': 150,\n",
    "    'imgsz': 736,\n",
    "    'batch': 40,\n",
    "    'patience': 15,\n",
    "    'save': True,\n",
    "    'device': 0 if torch.cuda.is_available() else 'cpu',\n",
    "    \n",
    "    # Optimizer - AdamW is good for small objects (pests/disease spots)\n",
    "    'optimizer': 'AdamW',\n",
    "    'lr0': 0.001,              # Lower LR for fine-tuning\n",
    "    'lrf': 0.01,               # Final learning rate = lr0 * lrf\n",
    "    'momentum': 0.937,         # SGD momentum\n",
    "    'weight_decay': 0.0005,\n",
    "    'warmup_epochs': 5,        # Increased warmup for stability\n",
    "    'warmup_momentum': 0.8,\n",
    "    'warmup_bias_lr': 0.1,\n",
    "    \n",
    "    # Loss weights - IMPORTANT for pest/disease detection\n",
    "    'box': 7.5,                # Higher weight for accurate bounding boxes\n",
    "    'cls': 0.5,                # Balanced classification (important for disease stages)\n",
    "    'dfl': 1.5,                # Distribution focal loss for precise boxes\n",
    "    \n",
    "    # Data augmentation - CRITICAL for pest/disease detection\n",
    "    'hsv_h': 0.015,            # Hue variation (different lighting conditions)\n",
    "    'hsv_s': 0.7,              # Saturation (different camera settings)\n",
    "    'hsv_v': 0.4,              # Brightness (outdoor/indoor variations)\n",
    "    'degrees': 10.0,           # Rotation (leaves at different angles)\n",
    "    'translate': 0.1,          # Translation (pest/disease at different positions)\n",
    "    'fliplr': 0.5,             # Horizontal flip (symmetric disease patterns)\n",
    "    'scale': 0.7,              # Increased scale variation (0.3-1.7x)\n",
    "    'shear': 2.0,              # Small shear transformation\n",
    "    'perspective': 0.0001,     # Slight perspective change\n",
    "    'flipud': 0.0,             # No vertical flip (leaves don't grow upside down)\n",
    "    \n",
    "    'mosaic': 1.0,             # Mosaic augmentation (helps with small objects)\n",
    "    'mixup': 0.1,              # Mixup (helps distinguish similar symptoms)\n",
    "    'copy_paste': 0.1,         # Copy-paste augmentation (for rare disease cases)\n",
    "    'auto_augment': 'randaugment',  # Additional augmentation\n",
    "    'erasing': 0.4,            # Random erasing augmentation\n",
    "    \n",
    "    # Advanced settings for small object detection\n",
    "    'multi_scale': False,      # Keep single scale for consistency\n",
    "    'conf': 0.25,              # Lower confidence threshold (catch early symptoms)\n",
    "    'iou': 0.7,                # IoU threshold for NMS\n",
    "    'close_mosaic': 15,        # Disable mosaic last 15 epochs\n",
    "    \n",
    "    # ‚úÖ FIXED: Removed duplicate cos_lr (was defined twice)\n",
    "    'cos_lr': True,            # Cosine learning rate scheduler (smoother convergence)\n",
    "    \n",
    "    # ‚úÖ FIXED: Removed deprecated parameters (label_smoothing, save_hybrid)\n",
    "    'nbs': 64,                 # Nominal batch size for scaling\n",
    "    'overlap_mask': True,      # Better for overlapping objects\n",
    "    'mask_ratio': 4,           # Mask downsampling ratio\n",
    "    'dropout': 0.0,            # No dropout (YOLO handles this internally)\n",
    "    \n",
    "    'val': True,               # Validate during training\n",
    "    'plots': True,             # Generate training plots\n",
    "    'save_json': True,         # Save results in JSON\n",
    "    'verbose': True,           # Verbose output\n",
    "    'deterministic': False,    # Faster training (set True for reproducibility)\n",
    "    \n",
    "    # Project settings\n",
    "    'name': 'banana_pest_disease_yolo11',  # Updated name to reflect YOLO11\n",
    "    'project': 'runs/detect',\n",
    "    'exist_ok': True,\n",
    "    'workers': 8,              # Faster data loading\n",
    "}\n",
    "\n",
    "print(\"üìã TRAINING CONFIGURATION:\")\n",
    "for key, value in training_config.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "# Start training\n",
    "print(f\"\\n‚è≥ Starting training with {data_config['nc']} classes:\")\n",
    "\n",
    "# ‚úÖ FIXED: Proper iteration over dictionary\n",
    "for class_id, class_name in sorted(data_config['names'].items()):\n",
    "    print(f\"   Class {class_id}: {class_name}\")\n",
    "\n",
    "try:\n",
    "    # Train the model\n",
    "    results = model.train(**training_config)\n",
    "    print(\"‚úÖ Training completed successfully!\")\n",
    "    \n",
    "    # Display results\n",
    "    if hasattr(results, 'save_dir'):\n",
    "        print(f\"\\nüìÅ Model saved to: {results.save_dir}\")\n",
    "        print(f\"üìÅ Best model: {results.save_dir / 'weights' / 'best.pt'}\")\n",
    "        print(f\"üìÅ Last model: {results.save_dir / 'weights' / 'last.pt'}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Training error: {e}\")\n",
    "    print(\"üîÑ Trying with simplified configuration...\")\n",
    "    \n",
    "    # Fallback configuration\n",
    "    simple_config = {\n",
    "        'data': data_yaml_path,\n",
    "        'epochs': 10,\n",
    "        'imgsz': 640,\n",
    "        'batch': 8,\n",
    "        'patience': 10,\n",
    "        'save': True,\n",
    "        'device': 0 if torch.cuda.is_available() else 'cpu',\n",
    "        'name': 'banana_pest_disease_yolo11_simple',\n",
    "        'project': 'runs/detect',\n",
    "        'exist_ok': True,\n",
    "    }\n",
    "    \n",
    "    results = model.train(**simple_config)\n",
    "    print(\"‚úÖ Training completed with simplified config!\")\n",
    "\n",
    "# Final summary\n",
    "if 'results' in locals():\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéâ TRAINING SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    if hasattr(results, 'save_dir'):\n",
    "        print(f\"üìÅ Output directory: {results.save_dir}\")\n",
    "        print(f\"üìä View training plots: {results.save_dir / 'results.png'}\")\n",
    "        print(f\"üìä View confusion matrix: {results.save_dir / 'confusion_matrix.png'}\")\n",
    "    else:\n",
    "        print(f\"üìÅ Output directory: runs/detect/banana_pest_disease_yolo12\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 9: COMPREHENSIVE PERFORMANCE EVALUATION\n",
    "# ============================================\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from ultralytics import YOLO\n",
    "\n",
    "print(\"üìä COMPREHENSIVE PERFORMANCE EVALUATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================\n",
    "# VERIFY DEPENDENCIES FROM PREVIOUS CELLS\n",
    "# ============================================\n",
    "# Ensure data_config and data_yaml_path are available\n",
    "if 'data_config' not in globals():\n",
    "    print(\"‚ö†Ô∏è data_config not found. Loading from data.yaml...\")\n",
    "    data_yaml_path = Path('/kaggle/working/yolo_classification_dataset/data.yaml')\n",
    "    import yaml\n",
    "    with open(data_yaml_path, 'r') as f:\n",
    "        data_config = yaml.safe_load(f)\n",
    "else:\n",
    "    # Convert Path to string if needed\n",
    "    if isinstance(data_yaml_path, Path):\n",
    "        data_yaml_path = str(data_yaml_path)\n",
    "    else:\n",
    "        data_yaml_path = str(data_yaml_path)\n",
    "\n",
    "# Get class names from data_config\n",
    "class_names_map = data_config['names']  # {0: 'Healthy', 1: 'Stage1', ...}\n",
    "num_classes = data_config['nc']\n",
    "\n",
    "# Class colors matching Cell 4\n",
    "CLASS_COLORS_RGB = {\n",
    "    0: (0, 100, 0),        # Healthy: Dark Green\n",
    "    1: (0, 255, 0),        # Stage1: Green\n",
    "    2: (144, 238, 144),    # Stage2: Light Green\n",
    "    3: (173, 255, 47),     # Stage3: Yellow Green\n",
    "    4: (255, 255, 0),      # Stage4: Yellow\n",
    "    5: (255, 165, 0),      # Stage5: Orange\n",
    "    6: (255, 0, 0)         # Stage6: Red\n",
    "}\n",
    "\n",
    "# Convert RGB tuples to hex for matplotlib\n",
    "def rgb_to_hex(rgb):\n",
    "    return '#%02x%02x%02x' % tuple(rgb)\n",
    "\n",
    "class_colors = [rgb_to_hex(CLASS_COLORS_RGB[i]) for i in range(num_classes)]\n",
    "\n",
    "# ============================================\n",
    "# LOAD TRAINED MODEL\n",
    "# ============================================\n",
    "print(\"\\n1Ô∏è‚É£ Loading trained model...\")\n",
    "\n",
    "# Try multiple possible model paths\n",
    "possible_paths = [\n",
    "    'runs/detect/banana_pest_disease_yolo11/weights/best.pt',\n",
    "    'runs/detect/banana_pest_disease_yolo12/weights/best.pt',\n",
    "    'runs/detect/banana_pest_disease_yolo11/weights/last.pt',\n",
    "    'runs/detect/banana_pest_disease_yolo12/weights/last.pt',\n",
    "]\n",
    "\n",
    "model_path = None\n",
    "for path in possible_paths:\n",
    "    if Path(path).exists():\n",
    "        model_path = path\n",
    "        break\n",
    "\n",
    "if model_path is None:\n",
    "    print(\"‚ùå No trained model found! Train the model first (Cell 8)\")\n",
    "    raise FileNotFoundError(\"Model not found. Please run Cell 8 to train the model first.\")\n",
    "\n",
    "print(f\"‚úÖ Model loaded from: {model_path}\")\n",
    "trained_model = YOLO(model_path)\n",
    "\n",
    "# ============================================\n",
    "# RUN VALIDATION ON TEST SET\n",
    "# ============================================\n",
    "print(\"\\n2Ô∏è‚É£ Running validation on test set...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ‚úÖ CORRECT METHOD: Use model.val() with data path\n",
    "# Note: Ultralytics automatically uses the 'test' split if defined in data.yaml\n",
    "try:\n",
    "    test_results = trained_model.val(\n",
    "        data=data_yaml_path,\n",
    "        batch=16,\n",
    "        imgsz=640,\n",
    "        conf=0.25,\n",
    "        iou=0.7,\n",
    "        verbose=True\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error during validation: {e}\")\n",
    "    print(\"üîÑ Trying with default settings...\")\n",
    "    test_results = trained_model.val(data=data_yaml_path, verbose=True)\n",
    "\n",
    "# ============================================\n",
    "# EXTRACT OVERALL METRICS\n",
    "# ============================================\n",
    "print(\"\\nüìà TEST SET PERFORMANCE METRICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£ OVERALL METRICS:\")\n",
    "\n",
    "# Safely extract metrics with error handling\n",
    "try:\n",
    "    map50 = float(test_results.box.map50)\n",
    "    map50_95 = float(test_results.box.map)\n",
    "    precision = float(test_results.box.mp)\n",
    "    recall = float(test_results.box.mr)\n",
    "except AttributeError as e:\n",
    "    print(f\"‚ö†Ô∏è Error extracting metrics: {e}\")\n",
    "    print(\"   Using alternative metric extraction...\")\n",
    "    # Fallback: try to get from results dict\n",
    "    if hasattr(test_results, 'results_dict'):\n",
    "        map50 = test_results.results_dict.get('metrics/mAP50(B)', 0.0)\n",
    "        map50_95 = test_results.results_dict.get('metrics/mAP50-95(B)', 0.0)\n",
    "        precision = test_results.results_dict.get('metrics/precision(B)', 0.0)\n",
    "        recall = test_results.results_dict.get('metrics/recall(B)', 0.0)\n",
    "    else:\n",
    "        raise ValueError(\"Cannot extract metrics from validation results\")\n",
    "\n",
    "print(f\"   ‚Ä¢ mAP50:     {map50:.3f} ({map50*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ mAP50-95:  {map50_95:.3f} ({map50_95*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Precision: {precision:.3f} ({precision*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Recall:    {recall:.3f} ({recall*100:.1f}%)\")\n",
    "\n",
    "# Calculate F1 score with safe division\n",
    "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "print(f\"   ‚Ä¢ F1 Score:  {f1_score:.3f} ({f1_score*100:.1f}%)\")\n",
    "\n",
    "# ============================================\n",
    "# PER-CLASS METRICS\n",
    "# ============================================\n",
    "print(\"\\n4Ô∏è‚É£ PER-CLASS PERFORMANCE:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ‚úÖ CORRECT METHOD: Access per-class AP\n",
    "try:\n",
    "    ap50_per_class = test_results.box.ap50  # AP at IoU=0.50\n",
    "    ap_per_class = test_results.box.ap      # AP at IoU=0.50:0.95\n",
    "    \n",
    "    # Handle different array shapes\n",
    "    if hasattr(ap50_per_class, 'shape'):\n",
    "        if len(ap50_per_class.shape) > 1:\n",
    "            # If 2D array (class, iou_threshold), take mean\n",
    "            ap50_per_class = ap50_per_class.mean(axis=-1) if ap50_per_class.ndim > 1 else ap50_per_class\n",
    "            ap_per_class = ap_per_class.mean(axis=-1) if ap_per_class.ndim > 1 else ap_per_class\n",
    "except AttributeError:\n",
    "    print(\"‚ö†Ô∏è Per-class metrics not available in this format\")\n",
    "    ap50_per_class = np.zeros(num_classes)\n",
    "    ap_per_class = np.zeros(num_classes)\n",
    "\n",
    "# Table header\n",
    "print(f\"\\n{'Class':<15} | {'mAP50':<10} | {'mAP50-95':<10}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "class_metrics = {}\n",
    "\n",
    "for class_id in range(num_classes):\n",
    "    class_name = class_names_map.get(class_id, f\"Class_{class_id}\")\n",
    "    \n",
    "    try:\n",
    "        # Get AP for this class\n",
    "        if isinstance(ap50_per_class, (list, np.ndarray)):\n",
    "            ap50 = float(ap50_per_class[class_id]) if class_id < len(ap50_per_class) else 0.0\n",
    "        else:\n",
    "            ap50 = 0.0\n",
    "            \n",
    "        if isinstance(ap_per_class, (list, np.ndarray)):\n",
    "            ap = float(ap_per_class[class_id]) if class_id < len(ap_per_class) else 0.0\n",
    "        else:\n",
    "            ap = 0.0\n",
    "    except (IndexError, TypeError, AttributeError) as e:\n",
    "        print(f\"   ‚ö†Ô∏è Warning: Could not get metrics for {class_name}: {e}\")\n",
    "        ap50 = 0.0\n",
    "        ap = 0.0\n",
    "    \n",
    "    # Store metrics\n",
    "    class_metrics[class_name] = {\n",
    "        'ap50': ap50,\n",
    "        'ap': ap\n",
    "    }\n",
    "    \n",
    "    # Print row\n",
    "    print(f\"{class_name:<15} | {ap50:<10.3f} | {ap:<10.3f}\")\n",
    "\n",
    "# ============================================\n",
    "# VISUALIZATION: PER-CLASS PERFORMANCE\n",
    "# ============================================\n",
    "print(\"\\n5Ô∏è‚É£ Generating per-class performance visualization...\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "fig.suptitle('üéØ Per-Class Performance Analysis (Test Set)', \n",
    "             fontsize=16, fontweight='bold')\n",
    "\n",
    "classes = [class_names_map[i] for i in range(num_classes)]\n",
    "\n",
    "# Extract metrics for plotting\n",
    "ap50s = [class_metrics[c]['ap50'] for c in classes]\n",
    "aps = [class_metrics[c]['ap'] for c in classes]\n",
    "\n",
    "# Plot 1: mAP50 per class\n",
    "ax1 = axes[0]\n",
    "bars1 = ax1.bar(classes, ap50s, color=class_colors[:len(classes)])\n",
    "ax1.set_ylabel('mAP50', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('mAP50 per Disease Class', fontsize=13, fontweight='bold')\n",
    "ax1.set_ylim([0, 1.05])\n",
    "ax1.grid(True, alpha=0.3, axis='y', linestyle='--')\n",
    "ax1.set_xticklabels(classes, rotation=15, ha='right', fontsize=10)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    if height > 0:\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{height:.3f}', ha='center', va='bottom', \n",
    "                fontsize=9, fontweight='bold')\n",
    "\n",
    "# Plot 2: mAP50-95 per class\n",
    "ax2 = axes[1]\n",
    "bars2 = ax2.bar(classes, aps, color=class_colors[:len(classes)])\n",
    "ax2.set_ylabel('mAP50-95', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('mAP50-95 per Disease Class', fontsize=13, fontweight='bold')\n",
    "ax2.set_ylim([0, 1.05])\n",
    "ax2.grid(True, alpha=0.3, axis='y', linestyle='--')\n",
    "ax2.set_xticklabels(classes, rotation=15, ha='right', fontsize=10)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    if height > 0:\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{height:.3f}', ha='center', va='bottom', \n",
    "                fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('per_class_performance.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úÖ Visualization saved as 'per_class_performance.png'\")\n",
    "plt.show()\n",
    "\n",
    "# ============================================\n",
    "# DETAILED COMPARISON TABLE\n",
    "# ============================================\n",
    "print(\"\\n6Ô∏è‚É£ DETAILED METRICS COMPARISON:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create comparison with overall metrics\n",
    "print(f\"\\n{'Metric':<20} | {'Overall':<12} | {'Best Class':<20} | {'Worst Class':<20}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "# Overall vs per-class comparison\n",
    "if class_metrics:\n",
    "    best_ap50_class = max(class_metrics.items(), key=lambda x: x[1]['ap50'])\n",
    "    worst_ap50_class = min(class_metrics.items(), key=lambda x: x[1]['ap50'])\n",
    "    \n",
    "    print(f\"{'mAP50':<20} | {map50:<12.3f} | {best_ap50_class[0]:<20} ({best_ap50_class[1]['ap50']:.3f}) | {worst_ap50_class[0]:<20} ({worst_ap50_class[1]['ap50']:.3f})\")\n",
    "    \n",
    "    best_ap_class = max(class_metrics.items(), key=lambda x: x[1]['ap'])\n",
    "    worst_ap_class = min(class_metrics.items(), key=lambda x: x[1]['ap'])\n",
    "    \n",
    "    print(f\"{'mAP50-95':<20} | {map50_95:<12.3f} | {best_ap_class[0]:<20} ({best_ap_class[1]['ap']:.3f}) | {worst_ap_class[0]:<20} ({worst_ap_class[1]['ap']:.3f})\")\n",
    "\n",
    "# ============================================\n",
    "# PERFORMANCE INTERPRETATION\n",
    "# ============================================\n",
    "print(\"\\n7Ô∏è‚É£ PERFORMANCE INTERPRETATION:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if map50 > 0.8:\n",
    "    status = \"üéâ EXCELLENT\"\n",
    "    interpretation = \"Model performing very well on test set! Ready for deployment.\"\n",
    "elif map50 > 0.7:\n",
    "    status = \"‚úÖ GOOD\"\n",
    "    interpretation = \"Model shows strong performance. Consider fine-tuning for production.\"\n",
    "elif map50 > 0.6:\n",
    "    status = \"üìä ACCEPTABLE\"\n",
    "    interpretation = \"Model is functional but has room for improvement.\"\n",
    "else:\n",
    "    status = \"‚ö†Ô∏è NEEDS IMPROVEMENT\"\n",
    "    interpretation = \"Consider more training, data augmentation, or larger model.\"\n",
    "\n",
    "print(f\"\\n   Status: {status}\")\n",
    "print(f\"   {interpretation}\")\n",
    "\n",
    "# Precision/Recall balance\n",
    "print(f\"\\n   Balance Analysis:\")\n",
    "if precision > 0.7 and recall > 0.7:\n",
    "    print(f\"   ‚úÖ Well-balanced precision ({precision:.3f}) and recall ({recall:.3f})\")\n",
    "elif precision < 0.6:\n",
    "    print(f\"   ‚ö†Ô∏è Low precision ({precision:.3f}) - Too many false positives\")\n",
    "    print(f\"   üí° Recommendation: Increase confidence threshold (e.g., conf=0.35)\")\n",
    "elif recall < 0.6:\n",
    "    print(f\"   ‚ö†Ô∏è Low recall ({recall:.3f}) - Missing many true positives\")\n",
    "    print(f\"   üí° Recommendation: Decrease confidence threshold (e.g., conf=0.20)\")\n",
    "\n",
    "# Class-specific insights\n",
    "if class_metrics:\n",
    "    print(f\"\\n   Class-Specific Insights:\")\n",
    "    print(f\"   ‚Ä¢ Best performing: {best_ap50_class[0]} (mAP50: {best_ap50_class[1]['ap50']:.3f})\")\n",
    "    print(f\"   ‚Ä¢ Needs attention: {worst_ap50_class[0]} (mAP50: {worst_ap50_class[1]['ap50']:.3f})\")\n",
    "    \n",
    "    # Additional insight: classes below threshold\n",
    "    threshold = 0.5\n",
    "    low_performers = [name for name, metrics in class_metrics.items() \n",
    "                      if metrics['ap50'] < threshold]\n",
    "    if low_performers:\n",
    "        print(f\"   ‚Ä¢ Classes below {threshold} mAP50: {', '.join(low_performers)}\")\n",
    "        print(f\"   üí° Consider: More training data or class-specific augmentation for these classes\")\n",
    "\n",
    "# ============================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìã EVALUATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\"\"\n",
    "‚úÖ TEST SET RESULTS:\n",
    "\n",
    "   ‚Ä¢ Dataset: {data_yaml_path}\n",
    "   ‚Ä¢ Model: {model_path}\n",
    "   ‚Ä¢ Classes: {num_classes} ({', '.join(classes)})\n",
    "\n",
    "üìä OVERALL PERFORMANCE:\n",
    "\n",
    "   ‚Ä¢ mAP50:     {map50:.3f} ({map50*100:.1f}%)\n",
    "   ‚Ä¢ mAP50-95:  {map50_95:.3f} ({map50_95*100:.1f}%)\n",
    "   ‚Ä¢ Precision: {precision:.3f} ({precision*100:.1f}%)\n",
    "   ‚Ä¢ Recall:    {recall:.3f} ({recall*100:.1f}%)\n",
    "   ‚Ä¢ F1 Score:  {f1_score:.3f} ({f1_score*100:.1f}%)\n",
    "\n",
    "üìÅ OUTPUTS SAVED:\n",
    "\n",
    "   ‚Ä¢ per_class_performance.png - Per-class visualization\n",
    "   ‚Ä¢ Confusion matrix: runs/detect/val/confusion_matrix.png\n",
    "   ‚Ä¢ Other metrics: runs/detect/val/\n",
    "\n",
    "\"\"\")\n",
    "print(\"=\" * 80)\n",
    "print(\"‚úÖ Evaluation completed successfully!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "# ============================================\n",
    "# OVERFITTING & UNDERFITTING DETECTION (OPTIMIZED)\n",
    "# ============================================\n",
    "print(\"üîç DETECTING OVERFITTING & UNDERFITTING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================\n",
    "# SMART PATH FINDING\n",
    "# ============================================\n",
    "def find_results_csv():\n",
    "    \"\"\"Find results.csv automatically - tries multiple paths\"\"\"\n",
    "    possible_paths = [\n",
    "        'runs/detect/banana_pest_disease_yolo11/results.csv',\n",
    "        'runs/detect/banana_pest_disease_yolo12/results.csv',\n",
    "        'runs/detect/banana_pest_disease_yolo11_simple/results.csv',\n",
    "    ]\n",
    "    \n",
    "    # Also search for any results.csv in runs/detect\n",
    "    search_patterns = [\n",
    "        'runs/detect/*/results.csv',\n",
    "        'runs/detect/*/*/results.csv',\n",
    "    ]\n",
    "    \n",
    "    # Try hardcoded paths first\n",
    "    for path in possible_paths:\n",
    "        if Path(path).exists():\n",
    "            return Path(path)\n",
    "    \n",
    "    # Search dynamically\n",
    "    for pattern in search_patterns:\n",
    "        matches = glob.glob(pattern)\n",
    "        if matches:\n",
    "            # Get the most recent one\n",
    "            return Path(max(matches, key=lambda p: Path(p).stat().st_mtime))\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Find results file\n",
    "results_csv = find_results_csv()\n",
    "\n",
    "if not results_csv or not results_csv.exists():\n",
    "    print(\"‚ùå Results CSV not found!\")\n",
    "    print(\"   Searched in:\")\n",
    "    print(\"   ‚Ä¢ runs/detect/banana_pest_disease_yolo11/\")\n",
    "    print(\"   ‚Ä¢ runs/detect/banana_pest_disease_yolo12/\")\n",
    "    print(\"   ‚Ä¢ runs/detect/*/\")\n",
    "    print(\"\\nüí° Make sure training has completed and results.csv exists.\")\n",
    "    raise FileNotFoundError(\"results.csv not found. Please run training first.\")\n",
    "\n",
    "results_dir = results_csv.parent\n",
    "print(f\"‚úÖ Found results: {results_csv}\")\n",
    "print(f\"üìÅ Results directory: {results_dir}\\n\")\n",
    "\n",
    "# Load and clean data\n",
    "try:\n",
    "    df = pd.read_csv(results_csv)\n",
    "    df.columns = df.columns.str.strip()\n",
    "    \n",
    "    # Reset index to start from 0\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    # Validate dataframe\n",
    "    if len(df) == 0:\n",
    "        raise ValueError(\"Results CSV is empty!\")\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(df)} epochs\")\n",
    "    print(f\"üìä Columns found: {len(df.columns)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading results: {e}\")\n",
    "    raise\n",
    "\n",
    "# ============================================\n",
    "# SMART COLUMN DETECTION\n",
    "# ============================================\n",
    "def find_column(df, possible_names):\n",
    "    \"\"\"Find column by trying multiple possible names\"\"\"\n",
    "    for name in possible_names:\n",
    "        if name in df.columns:\n",
    "            return df[name]\n",
    "    return None\n",
    "\n",
    "# Get columns with fallbacks\n",
    "train_box = find_column(df, ['train/box_loss', 'train_box_loss', 'box_loss'])\n",
    "train_cls = find_column(df, ['train/cls_loss', 'train_cls_loss', 'cls_loss'])\n",
    "val_box = find_column(df, ['val/box_loss', 'val_box_loss'])\n",
    "val_cls = find_column(df, ['val/cls_loss', 'val_cls_loss'])\n",
    "map50 = find_column(df, ['metrics/mAP50(B)', 'metrics/mAP50', 'mAP50', 'map50'])\n",
    "map50_95 = find_column(df, ['metrics/mAP50-95(B)', 'metrics/mAP50-95', 'mAP50-95', 'map50_95'])\n",
    "precision = find_column(df, ['metrics/precision(B)', 'metrics/precision', 'precision'])\n",
    "recall = find_column(df, ['metrics/recall(B)', 'metrics/recall', 'recall'])\n",
    "\n",
    "# Validate required columns\n",
    "if train_box is None:\n",
    "    raise ValueError(\"Required column 'train/box_loss' not found!\")\n",
    "\n",
    "if map50 is None:\n",
    "    raise ValueError(\"Required column 'metrics/mAP50(B)' not found!\")\n",
    "\n",
    "print(\"‚úÖ Required columns found\\n\")\n",
    "\n",
    "# ============================================\n",
    "# FUNCTION 1: TRAIN vs VAL LOSS COMPARISON (OPTIMIZED)\n",
    "# ============================================\n",
    "def plot_overfitting_detection(df, train_box, val_box, train_cls, val_cls, \n",
    "                                map50, map50_95, save_path=None):\n",
    "    \"\"\"Detect overfitting with improved error handling\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    fig.suptitle('üî¨ OVERFITTING & UNDERFITTING ANALYSIS', \n",
    "                 fontsize=16, fontweight='bold', y=0.995)\n",
    "    \n",
    "    epochs = df.index + 1  # Start from epoch 1\n",
    "    \n",
    "    # ==========================================\n",
    "    # PLOT 1: Box Loss (Train vs Val)\n",
    "    # ==========================================\n",
    "    ax1 = axes[0, 0]\n",
    "    \n",
    "    ax1.plot(epochs, train_box, label='Train Loss', linewidth=2.5, color='blue', marker='o', markersize=2)\n",
    "    \n",
    "    if val_box is not None:\n",
    "        ax1.plot(epochs, val_box, label='Val Loss', linewidth=2.5, color='red', marker='s', markersize=2)\n",
    "        \n",
    "        # Calculate gap\n",
    "        gap = val_box.iloc[-1] - train_box.iloc[-1]\n",
    "        \n",
    "        # Fill between with better visualization\n",
    "        ax1.fill_between(epochs, train_box, val_box, \n",
    "                        where=(val_box >= train_box), \n",
    "                        alpha=0.2, color='red', label='Overfitting Gap')\n",
    "        ax1.fill_between(epochs, train_box, val_box, \n",
    "                        where=(val_box < train_box), \n",
    "                        alpha=0.2, color='green', label='Good Generalization')\n",
    "        \n",
    "        # Add gap annotation\n",
    "        final_gap = gap\n",
    "        ax1.annotate(f'Final Gap: {final_gap:.4f}',\n",
    "                    xy=(len(epochs), (train_box.iloc[-1] + val_box.iloc[-1])/2),\n",
    "                    xytext=(len(epochs)*0.7, max(train_box.max(), val_box.max())*0.8),\n",
    "                    bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7),\n",
    "                    fontsize=10, fontweight='bold',\n",
    "                    arrowprops=dict(arrowstyle='->', color='black', lw=1.5))\n",
    "    else:\n",
    "        ax1.plot(epochs, train_box, linewidth=2.5, color='blue')\n",
    "        ax1.text(0.5, 0.95, '‚ö†Ô∏è Validation loss not available',\n",
    "                transform=ax1.transAxes, ha='center', va='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7),\n",
    "                fontsize=10)\n",
    "    \n",
    "    ax1.set_xlabel('Epoch', fontsize=11, fontweight='bold')\n",
    "    ax1.set_ylabel('Box Loss', fontsize=11, fontweight='bold')\n",
    "    ax1.set_title('üì¶ Box Loss: Train vs Validation', fontsize=12, fontweight='bold')\n",
    "    ax1.legend(fontsize=10, loc='best')\n",
    "    ax1.grid(True, alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # ==========================================\n",
    "    # PLOT 2: Classification Loss (Train vs Val)\n",
    "    # ==========================================\n",
    "    ax2 = axes[0, 1]\n",
    "    \n",
    "    if train_cls is not None:\n",
    "        ax2.plot(epochs, train_cls, label='Train Loss', linewidth=2.5, color='blue', marker='o', markersize=2)\n",
    "        \n",
    "        if val_cls is not None:\n",
    "            ax2.plot(epochs, val_cls, label='Val Loss', linewidth=2.5, color='red', marker='s', markersize=2)\n",
    "            gap_cls = val_cls.iloc[-1] - train_cls.iloc[-1]\n",
    "            ax2.fill_between(epochs, train_cls, val_cls, alpha=0.2, color='orange')\n",
    "            ax2.text(len(epochs)*0.7, max(train_cls.max(), val_cls.max())*0.8,\n",
    "                    f'Gap: {gap_cls:.4f}',\n",
    "                    bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7),\n",
    "                    fontsize=10, fontweight='bold')\n",
    "    else:\n",
    "        ax2.text(0.5, 0.5, 'Classification loss not available',\n",
    "                transform=ax2.transAxes, ha='center', va='center',\n",
    "                fontsize=12, style='italic')\n",
    "    \n",
    "    ax2.set_xlabel('Epoch', fontsize=11, fontweight='bold')\n",
    "    ax2.set_ylabel('Classification Loss', fontsize=11, fontweight='bold')\n",
    "    ax2.set_title('üéØ Classification Loss: Train vs Validation', fontsize=12, fontweight='bold')\n",
    "    if train_cls is not None:\n",
    "        ax2.legend(fontsize=10, loc='best')\n",
    "    ax2.grid(True, alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # ==========================================\n",
    "    # PLOT 3: mAP Performance Over Time\n",
    "    # ==========================================\n",
    "    ax3 = axes[1, 0]\n",
    "    \n",
    "    ax3.plot(epochs, map50, label='mAP50', linewidth=2.5, color='blue', \n",
    "            marker='o', markersize=3, alpha=0.8)\n",
    "    \n",
    "    if map50_95 is not None:\n",
    "        ax3.plot(epochs, map50_95, label='mAP50-95', linewidth=2.5, color='orange', \n",
    "                marker='s', markersize=3, alpha=0.8)\n",
    "    \n",
    "    # Check if mAP is plateauing\n",
    "    if len(map50) > 10:\n",
    "        last_10_change = map50.iloc[-1] - map50.iloc[-10]\n",
    "        if abs(last_10_change) < 0.01:\n",
    "            ax3.axhline(y=map50.iloc[-1], color='orange', linestyle='--', \n",
    "                       linewidth=2, alpha=0.7, label=f'Plateaued at {map50.iloc[-1]:.3f}')\n",
    "    \n",
    "    # Add best epoch marker\n",
    "    best_epoch = map50.idxmax() + 1\n",
    "    best_map = map50.max()\n",
    "    ax3.plot(best_epoch, best_map, 'g*', markersize=15, \n",
    "            label=f'Best: Epoch {best_epoch} ({best_map:.3f})')\n",
    "    \n",
    "    ax3.set_xlabel('Epoch', fontsize=11, fontweight='bold')\n",
    "    ax3.set_ylabel('mAP Score', fontsize=11, fontweight='bold')\n",
    "    ax3.set_title('üìà Validation Accuracy (mAP) Progression', fontsize=12, fontweight='bold')\n",
    "    ax3.legend(fontsize=10, loc='best')\n",
    "    ax3.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax3.set_ylim([0, max(1.0, map50.max() * 1.1)])\n",
    "    \n",
    "    # ==========================================\n",
    "    # PLOT 4: DIAGNOSTIC SUMMARY (IMPROVED)\n",
    "    # ==========================================\n",
    "    ax4 = axes[1, 1]\n",
    "    ax4.axis('off')\n",
    "    \n",
    "    # Analyze overfitting/underfitting\n",
    "    final_train_loss = train_box.iloc[-1]\n",
    "    final_map = map50.iloc[-1]\n",
    "    \n",
    "    # Calculate loss trend\n",
    "    recent_epochs = min(20, len(train_box))\n",
    "    train_trend = train_box.iloc[-recent_epochs:].values\n",
    "    train_decreasing = train_trend[-1] < train_trend[0]\n",
    "    \n",
    "    # Determine status\n",
    "    status = []\n",
    "    color = 'green'\n",
    "    \n",
    "    # Check for underfitting\n",
    "    if final_map < 0.5 and final_train_loss > 0.5:\n",
    "        status.append(\"‚ö†Ô∏è UNDERFITTING DETECTED\")\n",
    "        status.append(\"\")\n",
    "        status.append(\"Symptoms:\")\n",
    "        status.append(f\"‚Ä¢ Low accuracy: mAP={final_map:.3f} < 0.5\")\n",
    "        status.append(f\"‚Ä¢ High train loss: {final_train_loss:.4f}\")\n",
    "        status.append(\"\")\n",
    "        status.append(\"Solutions:\")\n",
    "        status.append(\"1. Train longer (more epochs)\")\n",
    "        status.append(\"2. Use larger model\")\n",
    "        status.append(\"3. Reduce augmentation\")\n",
    "        status.append(\"4. Check data quality\")\n",
    "        color = 'red'\n",
    "    \n",
    "    # Check for overfitting\n",
    "    elif val_box is not None:\n",
    "        gap = val_box.iloc[-1] - train_box.iloc[-1]\n",
    "        \n",
    "        if gap > 0.15:\n",
    "            status.append(\"‚ö†Ô∏è OVERFITTING DETECTED\")\n",
    "            status.append(\"\")\n",
    "            status.append(\"Symptoms:\")\n",
    "            status.append(f\"‚Ä¢ Large gap: {gap:.4f} > 0.15\")\n",
    "            status.append(\"‚Ä¢ Val loss > Train loss\")\n",
    "            status.append(\"\")\n",
    "            status.append(\"Solutions:\")\n",
    "            status.append(\"1. Increase augmentation\")\n",
    "            status.append(\"2. Add more data\")\n",
    "            status.append(\"3. Use regularization\")\n",
    "            status.append(\"4. Early stopping\")\n",
    "            status.append(\"5. Reduce model size\")\n",
    "            color = 'orange'\n",
    "        \n",
    "        elif gap < 0.05 and final_map > 0.7:\n",
    "            status.append(\"‚úÖ GOOD FIT\")\n",
    "            status.append(\"\")\n",
    "            status.append(\"Indicators:\")\n",
    "            status.append(f\"‚Ä¢ Small gap: {gap:.4f}\")\n",
    "            status.append(f\"‚Ä¢ Good mAP: {final_map:.3f}\")\n",
    "            status.append(\"‚Ä¢ Healthy model\")\n",
    "            status.append(\"\")\n",
    "            status.append(\"Recommendation:\")\n",
    "            status.append(\"‚Ä¢ Continue training\")\n",
    "            status.append(\"‚Ä¢ Monitor closely\")\n",
    "            color = 'lightgreen'\n",
    "        \n",
    "        else:\n",
    "            status.append(\"üìä ACCEPTABLE\")\n",
    "            status.append(\"\")\n",
    "            status.append(\"Metrics:\")\n",
    "            status.append(f\"‚Ä¢ Gap: {gap:.4f}\")\n",
    "            status.append(f\"‚Ä¢ mAP: {final_map:.3f}\")\n",
    "            status.append(\"\")\n",
    "            status.append(\"Actions:\")\n",
    "            status.append(\"‚Ä¢ Continue monitoring\")\n",
    "            status.append(\"‚Ä¢ Check test set\")\n",
    "            color = 'lightyellow'\n",
    "    \n",
    "    else:\n",
    "        status.append(\"‚ÑπÔ∏è LIMITED ANALYSIS\")\n",
    "        status.append(\"\")\n",
    "        status.append(\"No validation loss\")\n",
    "        status.append(\"available.\")\n",
    "        status.append(\"\")\n",
    "        if final_map > 0.7:\n",
    "            status.append(f\"‚úÖ Good mAP: {final_map:.3f}\")\n",
    "        else:\n",
    "            status.append(f\"‚ö†Ô∏è Low mAP: {final_map:.3f}\")\n",
    "        color = 'lightblue'\n",
    "    \n",
    "    # Add metrics summary\n",
    "    status.append(\"\")\n",
    "    status.append(\"‚îÄ\" * 35)\n",
    "    status.append(\"METRICS:\")\n",
    "    status.append(f\"Epochs: {len(df)}\")\n",
    "    status.append(f\"Train Loss: {final_train_loss:.4f}\")\n",
    "    if val_box is not None:\n",
    "        status.append(f\"Val Loss: {val_box.iloc[-1]:.4f}\")\n",
    "    status.append(f\"mAP50: {final_map:.3f}\")\n",
    "    if map50_95 is not None:\n",
    "        status.append(f\"mAP50-95: {map50_95.iloc[-1]:.3f}\")\n",
    "    \n",
    "    status_text = '\\n'.join(status)\n",
    "    \n",
    "    ax4.text(0.05, 0.5, status_text, \n",
    "             fontsize=10, \n",
    "             family='monospace',\n",
    "             verticalalignment='center',\n",
    "             bbox=dict(boxstyle='round', facecolor=color, alpha=0.6, edgecolor='black', linewidth=1.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"üíæ Saved: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return status\n",
    "\n",
    "# ============================================\n",
    "# FUNCTION 2: LEARNING CURVE ANALYSIS (OPTIMIZED)\n",
    "# ============================================\n",
    "def plot_learning_curves(df, train_box, save_path=None):\n",
    "    \"\"\"Learning curve analysis with better error handling\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    fig.suptitle('üìö LEARNING CURVE ANALYSIS', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    epochs = df.index + 1\n",
    "    \n",
    "    # Plot 1: Loss curves with moving average\n",
    "    ax1 = axes[0]\n",
    "    window = min(5, len(train_box) // 4)  # Adaptive window\n",
    "    if window < 1:\n",
    "        window = 1\n",
    "    \n",
    "    train_box_ma = train_box.rolling(window=window, min_periods=1).mean()\n",
    "    \n",
    "    ax1.plot(epochs, train_box, alpha=0.3, color='blue', label='Raw Train Loss', linewidth=1)\n",
    "    ax1.plot(epochs, train_box_ma, linewidth=2.5, color='blue', label=f'Train Loss (MA-{window})')\n",
    "    \n",
    "    ax1.set_xlabel('Epoch', fontsize=11, fontweight='bold')\n",
    "    ax1.set_ylabel('Box Loss', fontsize=11, fontweight='bold')\n",
    "    ax1.set_title('Loss Smoothing (Moving Average)', fontsize=12, fontweight='bold')\n",
    "    ax1.legend(fontsize=10)\n",
    "    ax1.grid(True, alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # Plot 2: Loss derivative\n",
    "    ax2 = axes[1]\n",
    "    if len(train_box) > 1:\n",
    "        loss_change = train_box.diff()\n",
    "        \n",
    "        ax2.plot(epochs[1:], loss_change[1:], color='purple', linewidth=2)\n",
    "        ax2.axhline(y=0, color='red', linestyle='--', linewidth=1.5)\n",
    "        ax2.fill_between(epochs[1:], 0, loss_change[1:], \n",
    "                         where=(loss_change[1:] < 0), alpha=0.3, color='green', label='Improving')\n",
    "        ax2.fill_between(epochs[1:], 0, loss_change[1:], \n",
    "                         where=(loss_change[1:] > 0), alpha=0.3, color='red', label='Worsening')\n",
    "        \n",
    "        ax2.set_xlabel('Epoch', fontsize=11, fontweight='bold')\n",
    "        ax2.set_ylabel('Loss Change', fontsize=11, fontweight='bold')\n",
    "        ax2.set_title('Loss Improvement Rate', fontsize=12, fontweight='bold')\n",
    "        ax2.legend(fontsize=10)\n",
    "        ax2.grid(True, alpha=0.3, linestyle='--')\n",
    "    else:\n",
    "        ax2.text(0.5, 0.5, 'Not enough data for derivative',\n",
    "                transform=ax2.transAxes, ha='center', va='center')\n",
    "    \n",
    "    # Plot 3: Convergence indicator\n",
    "    ax3 = axes[2]\n",
    "    \n",
    "    window_size = min(10, len(train_box) // 2)\n",
    "    if window_size >= 2 and len(train_box) > window_size:\n",
    "        rolling_std = train_box.rolling(window=window_size, min_periods=1).std()\n",
    "        ax3.plot(epochs, rolling_std, color='orange', linewidth=2.5)\n",
    "        ax3.set_xlabel('Epoch', fontsize=11, fontweight='bold')\n",
    "        ax3.set_ylabel(f'Loss Std Dev (window={window_size})', fontsize=11, fontweight='bold')\n",
    "        ax3.set_title('Training Stability', fontsize=12, fontweight='bold')\n",
    "        ax3.grid(True, alpha=0.3, linestyle='--')\n",
    "        \n",
    "        convergence_threshold = 0.01\n",
    "        ax3.axhline(y=convergence_threshold, color='green', linestyle='--', \n",
    "                   label=f'Converged < {convergence_threshold}', linewidth=2)\n",
    "        ax3.legend(fontsize=10)\n",
    "    else:\n",
    "        ax3.text(0.5, 0.5, 'Not enough data for stability analysis',\n",
    "                transform=ax3.transAxes, ha='center', va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"üíæ Saved: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# ============================================\n",
    "# RUN ANALYSIS\n",
    "# ============================================\n",
    "\n",
    "# Main overfitting detection\n",
    "print(\"üìä Generating overfitting analysis...\")\n",
    "status = plot_overfitting_detection(df, train_box, val_box, train_cls, val_cls,\n",
    "                                    map50, map50_95, \n",
    "                                    save_path=results_dir / 'overfitting_analysis.png')\n",
    "\n",
    "# Learning curves\n",
    "print(\"\\nüìä Generating learning curves...\")\n",
    "plot_learning_curves(df, train_box, save_path=results_dir / 'learning_curves.png')\n",
    "\n",
    "# ============================================\n",
    "# DETAILED DIAGNOSIS REPORT (IMPROVED)\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìã DETAILED DIAGNOSIS REPORT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get key metrics safely\n",
    "final_train_loss = train_box.iloc[-1]\n",
    "final_map50 = map50.iloc[-1]\n",
    "final_precision = precision.iloc[-1] if precision is not None else None\n",
    "final_recall = recall.iloc[-1] if recall is not None else None\n",
    "\n",
    "print(f\"\\n1Ô∏è‚É£ TRAINING METRICS:\")\n",
    "print(f\"   ‚Ä¢ Final Train Loss: {final_train_loss:.4f}\")\n",
    "print(f\"   ‚Ä¢ Final mAP50: {final_map50:.3f}\")\n",
    "if final_precision is not None:\n",
    "    print(f\"   ‚Ä¢ Final Precision: {final_precision:.3f}\")\n",
    "if final_recall is not None:\n",
    "    print(f\"   ‚Ä¢ Final Recall: {final_recall:.3f}\")\n",
    "\n",
    "# Check validation loss if available\n",
    "if val_box is not None:\n",
    "    final_val_loss = val_box.iloc[-1]\n",
    "    gap = final_val_loss - final_train_loss\n",
    "    print(f\"   ‚Ä¢ Final Val Loss: {final_val_loss:.4f}\")\n",
    "    print(f\"   ‚Ä¢ Train-Val Gap: {gap:.4f}\")\n",
    "\n",
    "# Trend analysis\n",
    "print(f\"\\n2Ô∏è‚É£ TREND ANALYSIS:\")\n",
    "recent_epochs = min(20, len(df))\n",
    "if len(df) >= recent_epochs:\n",
    "    map_start = map50.iloc[-recent_epochs]\n",
    "    map_end = map50.iloc[-1]\n",
    "    map_improvement = map_end - map_start\n",
    "    \n",
    "    if map_improvement > 0.05:\n",
    "        print(f\"   ‚úÖ Still improving (+{map_improvement:.3f} in last {recent_epochs} epochs)\")\n",
    "        print(f\"   üí° Continue training for better results\")\n",
    "    elif map_improvement > 0:\n",
    "        print(f\"   üìä Slow improvement (+{map_improvement:.3f} in last {recent_epochs} epochs)\")\n",
    "        print(f\"   üí° May be close to optimal\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è Not improving ({map_improvement:.3f} in last {recent_epochs} epochs)\")\n",
    "        print(f\"   üí° Consider stopping or adjusting\")\n",
    "\n",
    "# Final recommendations\n",
    "print(f\"\\n3Ô∏è‚É£ ACTIONABLE RECOMMENDATIONS:\")\n",
    "\n",
    "recommendations = []\n",
    "\n",
    "# Based on mAP\n",
    "if final_map50 < 0.5:\n",
    "    recommendations.append(\"‚ùå Low accuracy - Need improvements:\")\n",
    "    recommendations.append(\"   ‚Ä¢ Add more diverse training data\")\n",
    "    recommendations.append(\"   ‚Ä¢ Use larger model (yolov8m/l)\")\n",
    "    recommendations.append(\"   ‚Ä¢ Check data quality and labels\")\n",
    "elif final_map50 < 0.7:\n",
    "    recommendations.append(\"‚ö†Ô∏è Moderate accuracy - Room for improvement:\")\n",
    "    recommendations.append(\"   ‚Ä¢ Increase epochs\")\n",
    "    recommendations.append(\"   ‚Ä¢ Fine-tune augmentation\")\n",
    "    recommendations.append(\"   ‚Ä¢ Verify label quality\")\n",
    "else:\n",
    "    recommendations.append(\"‚úÖ Good accuracy - Model performing well!\")\n",
    "    recommendations.append(\"   ‚Ä¢ Ready for testing\")\n",
    "    recommendations.append(\"   ‚Ä¢ Fine-tune confidence threshold\")\n",
    "\n",
    "# Based on precision-recall balance\n",
    "if final_precision is not None and final_precision < 0.6:\n",
    "    recommendations.append(\"\\n‚ö†Ô∏è Low precision - Too many false positives:\")\n",
    "    recommendations.append(\"   ‚Ä¢ Increase confidence threshold (conf=0.3-0.4)\")\n",
    "\n",
    "if final_recall is not None and final_recall < 0.6:\n",
    "    recommendations.append(\"\\n‚ö†Ô∏è Low recall - Missing detections:\")\n",
    "    recommendations.append(\"   ‚Ä¢ Decrease confidence threshold (conf=0.15-0.25)\")\n",
    "\n",
    "for rec in recommendations:\n",
    "    print(f\"   {rec}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ Analysis complete! Check generated images:\")\n",
    "print(f\"   üìÅ {results_dir}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Cell 10: Enhanced Prediction Visualization (OPTIMIZED)\n",
    "# ============================================\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from pathlib import Path\n",
    "import random\n",
    "import glob\n",
    "from ultralytics import YOLO\n",
    "\n",
    "print(\"üîç ENHANCED PREDICTION VISUALIZATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================\n",
    "# SMART MODEL PATH FINDING\n",
    "# ============================================\n",
    "def find_trained_model():\n",
    "    \"\"\"Auto-detect trained model - tries multiple paths\"\"\"\n",
    "    possible_paths = [\n",
    "        'runs/detect/banana_pest_disease_yolo11/weights/best.pt',\n",
    "        'runs/detect/banana_pest_disease_yolo12/weights/best.pt',\n",
    "        'runs/detect/banana_pest_disease_yolo11_simple/weights/best.pt',\n",
    "    ]\n",
    "    \n",
    "    # Search dynamically\n",
    "    search_patterns = [\n",
    "        'runs/detect/*/weights/best.pt',\n",
    "        'runs/detect/*/*/weights/best.pt',\n",
    "    ]\n",
    "    \n",
    "    # Try hardcoded paths first\n",
    "    for path in possible_paths:\n",
    "        if Path(path).exists():\n",
    "            return Path(path)\n",
    "    \n",
    "    # Search dynamically\n",
    "    for pattern in search_patterns:\n",
    "        matches = glob.glob(pattern)\n",
    "        if matches:\n",
    "            # Get most recent\n",
    "            return Path(max(matches, key=lambda p: Path(p).stat().st_mtime))\n",
    "    \n",
    "    # Try last.pt as fallback\n",
    "    for path in possible_paths:\n",
    "        last_path = path.replace('best.pt', 'last.pt')\n",
    "        if Path(last_path).exists():\n",
    "            print(f\"‚ö†Ô∏è Using last.pt instead of best.pt\")\n",
    "            return Path(last_path)\n",
    "    \n",
    "    return None\n",
    "\n",
    "# ============================================\n",
    "# SMART TEST IMAGE PATH FINDING\n",
    "# ============================================\n",
    "def find_test_images_dir():\n",
    "    \"\"\"Auto-detect test images directory\"\"\"\n",
    "    # Try from data_config if available\n",
    "    if 'data_config' in globals():\n",
    "        test_path = data_config.get('test', '')\n",
    "        if test_path:\n",
    "            test_dir = Path(test_path)\n",
    "            # Try different structures\n",
    "            possible_dirs = [\n",
    "                test_dir,  # Direct path\n",
    "                test_dir.parent / 'images',  # Parent/images\n",
    "                test_dir / 'images',  # test/images\n",
    "            ]\n",
    "            \n",
    "            for dir_path in possible_dirs:\n",
    "                if dir_path.exists() and dir_path.is_dir():\n",
    "                    # Check if has images\n",
    "                    images = list(dir_path.glob('*.jpg')) + list(dir_path.glob('*.png'))\n",
    "                    if len(images) > 0:\n",
    "                        return dir_path\n",
    "    \n",
    "    # Fallback: search common locations\n",
    "    search_paths = [\n",
    "        Path('kaggle/working/yolo_classification_dataset/test/images'),\n",
    "        Path('yolo_classification_dataset/test/images'),\n",
    "        Path('dataset/test/images'),\n",
    "        Path('test/images'),\n",
    "    ]\n",
    "    \n",
    "    for path in search_paths:\n",
    "        if path.exists():\n",
    "            return path\n",
    "    \n",
    "    return None\n",
    "\n",
    "# ============================================\n",
    "# ENHANCED VISUALIZATION FUNCTION\n",
    "# ============================================\n",
    "def visualize_enhanced_predictions(model, test_images_dir, class_names_map, \n",
    "                                   num_samples=50, conf_threshold=0.2):\n",
    "    \"\"\"\n",
    "    Visualize model predictions with confidence-based coloring\n",
    "    \n",
    "    Color Coding:\n",
    "    - GREEN (>70%): High confidence - Clear disease detection\n",
    "    - YELLOW (50-70%): Medium confidence - Likely disease\n",
    "    - ORANGE (30-50%): Low confidence - Possible disease\n",
    "    - RED (<30%): Very low confidence - Uncertain\n",
    "    \"\"\"\n",
    "    \n",
    "    test_images_dir = Path(test_images_dir)\n",
    "    \n",
    "    if not test_images_dir.exists():\n",
    "        print(f\"‚ùå Directory not found: {test_images_dir}\")\n",
    "        print(f\"   Absolute path: {test_images_dir.absolute()}\")\n",
    "        return None\n",
    "    \n",
    "    # Find all test images\n",
    "    image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.JPG', '*.JPEG', '*.PNG']\n",
    "    test_images = []\n",
    "    for ext in image_extensions:\n",
    "        test_images.extend(list(test_images_dir.glob(ext)))\n",
    "    \n",
    "    if len(test_images) == 0:\n",
    "        print(f\"‚ùå No images found in {test_images_dir}\")\n",
    "        print(\"üí° Check if path is correct\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"üì∏ Found {len(test_images)} test images\")\n",
    "    \n",
    "    # Sample images\n",
    "    num_samples = min(num_samples, len(test_images))\n",
    "    sample_images = random.sample(test_images, num_samples)\n",
    "    print(f\"üé≤ Randomly selected {num_samples} images for visualization\\n\")\n",
    "    \n",
    "    # ============================================\n",
    "    # AUTO-GRID LAYOUT\n",
    "    # ============================================\n",
    "    cols = 4  # Images per row\n",
    "    rows = int(np.ceil(num_samples / cols))\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(5 * cols, 5 * rows))\n",
    "    \n",
    "    # Handle different axis shapes\n",
    "    if rows == 1 and cols == 1:\n",
    "        axes = np.array([axes])\n",
    "    elif rows == 1:\n",
    "        axes = axes.reshape(-1)\n",
    "    elif cols == 1:\n",
    "        axes = axes.reshape(-1)\n",
    "    else:\n",
    "        axes = axes.ravel()\n",
    "    \n",
    "    # ============================================\n",
    "    # CONFIDENCE COLOR MAPPING (RGB for matplotlib)\n",
    "    # ============================================\n",
    "    confidence_colors = {\n",
    "        'high': (0, 1, 0),        # Green: >70%\n",
    "        'medium': (1, 1, 0),      # Yellow: 50-70%\n",
    "        'low': (1, 0.65, 0),      # Orange: 30-50%\n",
    "        'very_low': (1, 0, 0)     # Red: <30%\n",
    "    }\n",
    "    \n",
    "    # For OpenCV (BGR)\n",
    "    confidence_colors_cv = {\n",
    "        'high': (0, 255, 0),\n",
    "        'medium': (0, 255, 255),    # Yellow in BGR\n",
    "        'low': (0, 165, 255),       # Orange in BGR\n",
    "        'very_low': (0, 0, 255)\n",
    "    }\n",
    "    \n",
    "    # ============================================\n",
    "    # PROCESS EACH IMAGE\n",
    "    # ============================================\n",
    "    all_detections = []\n",
    "    processed_count = 0\n",
    "    \n",
    "    print(\"üîÑ Processing images...\")\n",
    "    for idx, img_path in enumerate(sample_images):\n",
    "        try:\n",
    "            # Get model predictions\n",
    "            results = model.predict(str(img_path), conf=conf_threshold, verbose=False)\n",
    "            \n",
    "            # Read and prepare image\n",
    "            img = cv2.imread(str(img_path))\n",
    "            if img is None:\n",
    "                print(f\"   ‚ö†Ô∏è Could not read: {img_path.name}\")\n",
    "                axes[idx].text(0.5, 0.5, 'Image Error', ha='center', va='center',\n",
    "                             fontsize=10, color='red')\n",
    "                axes[idx].axis('off')\n",
    "                all_detections.append({'image': img_path.name, 'count': 0, 'details': []})\n",
    "                continue\n",
    "            \n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            original_img = img.copy()\n",
    "            \n",
    "            detections_count = 0\n",
    "            detection_details = []\n",
    "            \n",
    "            # Process detections\n",
    "            for result in results:\n",
    "                boxes = result.boxes\n",
    "                if boxes is not None and len(boxes) > 0:\n",
    "                    for box in boxes:\n",
    "                        confidence = float(box.conf.item())\n",
    "                        class_id = int(box.cls.item())\n",
    "                        class_name = class_names_map.get(class_id, f'Class_{class_id}')\n",
    "                        \n",
    "                        # Determine color based on confidence\n",
    "                        if confidence > 0.7:\n",
    "                            color_cv = confidence_colors_cv['high']\n",
    "                            conf_label = 'HIGH'\n",
    "                        elif confidence > 0.5:\n",
    "                            color_cv = confidence_colors_cv['medium']\n",
    "                            conf_label = 'MEDIUM'\n",
    "                        elif confidence > 0.3:\n",
    "                            color_cv = confidence_colors_cv['low']\n",
    "                            conf_label = 'LOW'\n",
    "                        else:\n",
    "                            color_cv = confidence_colors_cv['very_low']\n",
    "                            conf_label = 'VERY LOW'\n",
    "                        \n",
    "                        # Draw bounding box\n",
    "                        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                        cv2.rectangle(img, (x1, y1), (x2, y2), color_cv, 3)\n",
    "                        \n",
    "                        # Prepare label\n",
    "                        label = f\"{class_name}: {confidence:.2f}\"\n",
    "                        \n",
    "                        # Draw label background\n",
    "                        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                        font_scale = 0.6\n",
    "                        thickness = 2\n",
    "                        (text_width, text_height), baseline = cv2.getTextSize(\n",
    "                            label, font, font_scale, thickness\n",
    "                        )\n",
    "                        \n",
    "                        # Background rectangle\n",
    "                        cv2.rectangle(\n",
    "                            img, \n",
    "                            (x1, y1 - text_height - 10), \n",
    "                            (x1 + text_width + 10, y1), \n",
    "                            color_cv, \n",
    "                            -1\n",
    "                        )\n",
    "                        \n",
    "                        # Label text\n",
    "                        cv2.putText(\n",
    "                            img, \n",
    "                            label, \n",
    "                            (x1 + 5, y1 - 5), \n",
    "                            font, \n",
    "                            font_scale, \n",
    "                            (0, 0, 0),  # Black text\n",
    "                            thickness\n",
    "                        )\n",
    "                        \n",
    "                        detections_count += 1\n",
    "                        detection_details.append({\n",
    "                            'class': class_name,\n",
    "                            'confidence': confidence,\n",
    "                            'conf_level': conf_label,\n",
    "                            'bbox': [x1, y1, x2, y2]\n",
    "                        })\n",
    "            \n",
    "            # Display image\n",
    "            axes[idx].imshow(img)\n",
    "            \n",
    "            # Create title with detection info\n",
    "            title_max_len = 35\n",
    "            img_name = img_path.name[:title_max_len] + ('...' if len(img_path.name) > title_max_len else '')\n",
    "            \n",
    "            if detections_count > 0:\n",
    "                title = f'{img_name}\\n‚úÖ {detections_count} detection(s)'\n",
    "                axes[idx].set_title(title, fontsize=9, color='green', fontweight='bold')\n",
    "            else:\n",
    "                title = f'{img_name}\\n‚ùå No detections'\n",
    "                axes[idx].set_title(title, fontsize=9, color='red')\n",
    "            \n",
    "            axes[idx].axis('off')\n",
    "            \n",
    "            # Store detection info\n",
    "            all_detections.append({\n",
    "                'image': img_path.name,\n",
    "                'count': detections_count,\n",
    "                'details': detection_details\n",
    "            })\n",
    "            \n",
    "            processed_count += 1\n",
    "            if (idx + 1) % 10 == 0:\n",
    "                print(f\"   Processed {idx + 1}/{num_samples} images...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error processing {img_path.name}: {str(e)[:50]}\")\n",
    "            axes[idx].text(0.5, 0.5, f'Error', \n",
    "                          ha='center', va='center', fontsize=8, color='red')\n",
    "            axes[idx].axis('off')\n",
    "            all_detections.append({'image': img_path.name, 'count': 0, 'details': []})\n",
    "    \n",
    "    print(f\"‚úÖ Processed {processed_count}/{num_samples} images\\n\")\n",
    "    \n",
    "    # ============================================\n",
    "    # HIDE UNUSED SUBPLOTS\n",
    "    # ============================================\n",
    "    for idx in range(num_samples, len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    # ============================================\n",
    "    # ADD COLOR LEGEND\n",
    "    # ============================================\n",
    "    legend_elements = [\n",
    "        mpatches.Patch(facecolor=confidence_colors['high'], \n",
    "                      edgecolor='black', linewidth=1,\n",
    "                      label='High Confidence (>70%)'),\n",
    "        mpatches.Patch(facecolor=confidence_colors['medium'], \n",
    "                      edgecolor='black', linewidth=1,\n",
    "                      label='Medium Confidence (50-70%)'),\n",
    "        mpatches.Patch(facecolor=confidence_colors['low'], \n",
    "                      edgecolor='black', linewidth=1,\n",
    "                      label='Low Confidence (30-50%)'),\n",
    "        mpatches.Patch(facecolor=confidence_colors['very_low'], \n",
    "                      edgecolor='black', linewidth=1,\n",
    "                      label='Very Low Confidence (<30%)')\n",
    "    ]\n",
    "    \n",
    "    fig.legend(\n",
    "        handles=legend_elements,\n",
    "        loc='upper center',\n",
    "        bbox_to_anchor=(0.5, 0.98),\n",
    "        ncol=4,\n",
    "        fontsize=11,\n",
    "        frameon=True,\n",
    "        fancybox=True,\n",
    "        shadow=True,\n",
    "        framealpha=0.9\n",
    "    )\n",
    "    \n",
    "    plt.suptitle(\n",
    "        'üåø Banana Disease Detection - Enhanced Predictions with Confidence Levels',\n",
    "        fontsize=16,\n",
    "        fontweight='bold',\n",
    "        y=0.995\n",
    "    )\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    \n",
    "    # Save figure\n",
    "    output_path = 'enhanced_predictions.png'\n",
    "    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"üíæ Predictions saved as '{output_path}'\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # ============================================\n",
    "    # DETECTION SUMMARY (ENHANCED)\n",
    "    # ============================================\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üìä DETECTION SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    total_detections = sum(d['count'] for d in all_detections)\n",
    "    images_with_detections = sum(1 for d in all_detections if d['count'] > 0)\n",
    "    images_without_detections = len(all_detections) - images_with_detections\n",
    "    \n",
    "    print(f\"\\nüìà OVERALL STATISTICS:\")\n",
    "    print(f\"   ‚Ä¢ Total images analyzed: {len(all_detections)}\")\n",
    "    print(f\"   ‚Ä¢ Images with detections: {images_with_detections} ({images_with_detections/len(all_detections)*100:.1f}%)\")\n",
    "    print(f\"   ‚Ä¢ Images without detections: {images_without_detections} ({images_without_detections/len(all_detections)*100:.1f}%)\")\n",
    "    print(f\"   ‚Ä¢ Total detections: {total_detections}\")\n",
    "    if len(all_detections) > 0:\n",
    "        print(f\"   ‚Ä¢ Average detections per image: {total_detections/len(all_detections):.2f}\")\n",
    "    \n",
    "    # Count confidence levels\n",
    "    confidence_counts = {'HIGH': 0, 'MEDIUM': 0, 'LOW': 0, 'VERY LOW': 0}\n",
    "    class_counts = {}\n",
    "    confidence_values = []\n",
    "    \n",
    "    for detection in all_detections:\n",
    "        for detail in detection['details']:\n",
    "            confidence_counts[detail['conf_level']] += 1\n",
    "            class_name = detail['class']\n",
    "            class_counts[class_name] = class_counts.get(class_name, 0) + 1\n",
    "            confidence_values.append(detail['confidence'])\n",
    "    \n",
    "    if total_detections > 0:\n",
    "        print(f\"\\nüéØ CONFIDENCE DISTRIBUTION:\")\n",
    "        for level, count in confidence_counts.items():\n",
    "            percentage = (count / total_detections) * 100\n",
    "            bar = '‚ñà' * int(percentage / 2)  # Visual bar\n",
    "            print(f\"   ‚Ä¢ {level:12s}: {count:3d} ({percentage:5.1f}%) {bar}\")\n",
    "        \n",
    "        if confidence_values:\n",
    "            avg_conf = np.mean(confidence_values)\n",
    "            print(f\"\\n   ‚Ä¢ Average Confidence: {avg_conf:.3f}\")\n",
    "        \n",
    "        print(f\"\\nüè∑Ô∏è CLASS DISTRIBUTION:\")\n",
    "        sorted_classes = sorted(class_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        for class_name, count in sorted_classes:\n",
    "            percentage = (count / total_detections) * 100\n",
    "            bar = '‚ñà' * int(percentage / 2)\n",
    "            print(f\"   ‚Ä¢ {class_name:20s}: {count:3d} ({percentage:5.1f}%) {bar}\")\n",
    "    \n",
    "    # Top detections\n",
    "    if all_detections:\n",
    "        top_detections = sorted(all_detections, key=lambda x: x['count'], reverse=True)[:5]\n",
    "        print(f\"\\nüîù TOP 5 IMAGES WITH MOST DETECTIONS:\")\n",
    "        for i, det in enumerate(top_detections, 1):\n",
    "            print(f\"   {i}. {det['image'][:40]:40s} - {det['count']} detection(s)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    \n",
    "    return all_detections\n",
    "\n",
    "# ============================================\n",
    "# RUN VISUALIZATION\n",
    "# ============================================\n",
    "print(\"\\nüì¶ Loading trained model...\")\n",
    "\n",
    "try:\n",
    "    # Find model\n",
    "    model_path = find_trained_model()\n",
    "    \n",
    "    if model_path is None:\n",
    "        print(\"‚ùå No trained model found!\")\n",
    "        print(\"üí° Train the model first using Cell 8\")\n",
    "        print(\"\\nüí° Searched in:\")\n",
    "        print(\"   ‚Ä¢ runs/detect/banana_pest_disease_yolo11/weights/\")\n",
    "        print(\"   ‚Ä¢ runs/detect/banana_pest_disease_yolo12/weights/\")\n",
    "        print(\"   ‚Ä¢ runs/detect/*/weights/\")\n",
    "    else:\n",
    "        model = YOLO(str(model_path))\n",
    "        print(f\"‚úÖ Model loaded from: {model_path}\")\n",
    "        \n",
    "        # Get class names\n",
    "        if 'data_config' in globals():\n",
    "            class_names_map = data_config.get('names', {})\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è data_config not found, using default class names\")\n",
    "            class_names_map = {i: f'Class_{i}' for i in range(7)}\n",
    "        \n",
    "        print(f\"üìã Classes: {len(class_names_map)}\")\n",
    "        for class_id, class_name in sorted(class_names_map.items()):\n",
    "            print(f\"   ‚Ä¢ {class_id}: {class_name}\")\n",
    "        \n",
    "        # Find test images directory\n",
    "        test_images_dir = find_test_images_dir()\n",
    "        \n",
    "        if test_images_dir is None:\n",
    "            print(\"\\n‚ùå Test images directory not found!\")\n",
    "            print(\"\\nüí° Possible fixes:\")\n",
    "            print(\"   1. Check if test images are in the correct location\")\n",
    "            print(\"   2. Verify your dataset structure\")\n",
    "            print(\"   3. Update data_config['test'] path\")\n",
    "            \n",
    "            # Try manual path\n",
    "            manual_path = input(\"\\nüí° Enter test images path (or press Enter to skip): \").strip()\n",
    "            if manual_path:\n",
    "                test_images_dir = Path(manual_path)\n",
    "            else:\n",
    "                raise FileNotFoundError(\"Test images directory not found\")\n",
    "        else:\n",
    "            print(f\"üìÅ Test images directory: {test_images_dir}\")\n",
    "        \n",
    "        # Run visualization\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        detection_results = visualize_enhanced_predictions(\n",
    "            model=model,\n",
    "            test_images_dir=test_images_dir,\n",
    "            class_names_map=class_names_map,\n",
    "            num_samples=50,  # Change this to show more/less images\n",
    "            conf_threshold=0.2  # Minimum confidence threshold\n",
    "        )\n",
    "        \n",
    "        if detection_results:\n",
    "            print(\"\\n‚úÖ Enhanced visualization completed!\")\n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è Visualization completed with errors\")\n",
    "            \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\n‚ùå Error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Unexpected error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8956079,
     "sourceId": 14070175,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
