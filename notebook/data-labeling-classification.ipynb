{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"‚úÖ Google Drive mounted!\")\n",
        "print(\"üìÅ Your Drive files are now in: /content/drive/MyDrive/\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "from pathlib import Path\n",
        "from typing import Dict, Tuple, Any, Optional, List\n",
        "import json\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import logging\n",
        "from tqdm import tqdm \n",
        "from datetime import datetime\n",
        "import shutil\n",
        "import csv\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "# Setup logging (Senior-level: Proper logging instead of print)\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('/content/data_labeling.log'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "# OPTION 1: Data in Google Drive (most common)\n",
        "USE_DRIVE = True  # Set to False if data is uploaded to Colab\n",
        "\n",
        "if USE_DRIVE:\n",
        "    # Paths for Google Drive\n",
        "    DRIVE_BASE = Path(\"/content/drive/MyDrive\")\n",
        "    # Update this path to match your Drive folder structure\n",
        "    # Example: DRIVE_BASE / \"ai-banana-earlystage\" / \"Data\" / \"Sigatoka pics\"\n",
        "    DATA_DIR = DRIVE_BASE / \"Machine_Learning\" / \"Sigatoka pics\"\n",
        "    OUTPUT_DIR = DRIVE_BASE / \"Machine_Learning\" / \"Data Labeling\" \n",
        "    \n",
        "    print(\"üìÅ Using Google Drive paths\")\n",
        "    print(f\"   Data: {DATA_DIR}\")\n",
        "    print(f\"   Output: {OUTPUT_DIR}\")\n",
        "else:\n",
        "    # OPTION 2: Data uploaded to Colab (use /content/)\n",
        "    DATA_DIR = Path(\"/content/Data/Sigatoka pics\")\n",
        "    OUTPUT_DIR = Path(\"/content/Data Labeling\")\n",
        "    \n",
        "    print(\"üìÅ Using Colab workspace paths\")\n",
        "    print(f\"   Data: {DATA_DIR}\")\n",
        "    print(f\"   Output: {OUTPUT_DIR}\")\n",
        "\n",
        "# Validate paths exist\n",
        "if not DATA_DIR.exists():\n",
        "    logger.warning(f\"‚ö†Ô∏è  Data directory not found: {DATA_DIR}\")\n",
        "    print(f\"\\n‚ö†Ô∏è  WARNING: Data directory not found!\")\n",
        "    print(f\"   Please check the path: {DATA_DIR}\")\n",
        "    print(f\"   Update DATA_DIR in this cell to match your folder structure\")\n",
        "else:\n",
        "    print(f\"‚úÖ Data directory found: {DATA_DIR}\")\n",
        "    \n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "logger.info(f\"Output directory: {OUTPUT_DIR}\")\n",
        "print(f\"‚úÖ Output directory ready: {OUTPUT_DIR}\")\n",
        "\n",
        "# Class labels mapping\n",
        "CLASS_LABELS = {\n",
        "    \"Stage1\": \"Stage1\",\n",
        "    \"Stage2\": \"Stage2\", \n",
        "    \"Stage3\": \"Stage3\",\n",
        "    \"Healthy\": \"Healthy\"  # If you have healthy samples\n",
        "}\n",
        "\n",
        "# Image quality thresholds\n",
        "QUALITY_THRESHOLDS = {\n",
        "    \"min_resolution\": (256, 256),  # Minimum width, height\n",
        "    \"max_blur_threshold\": 100.0,  # Laplacian variance threshold (lower = blurrier)\n",
        "    \"min_brightness\": 20,  # Minimum average brightness (0-255)\n",
        "    \"max_brightness\": 240,  # Maximum average brightness (0-255)\n",
        "    \"min_file_size_kb\": 50,  # Minimum file size in KB\n",
        "    \"max_file_size_mb\": 10,  # Maximum file size in MB\n",
        "}\n",
        "\n",
        "# Supported image formats\n",
        "SUPPORTED_FORMATS = [\".jpg\", \".jpeg\", \".png\", \".JPG\", \".JPEG\", \".PNG\"]\n",
        "\n",
        "# Metadata fields to extract\n",
        "METADATA_FIELDS = [\n",
        "    \"image_path\",\n",
        "    \"class_label\",\n",
        "    \"width\",\n",
        "    \"height\",\n",
        "    \"file_size_kb\",\n",
        "    \"blur_score\",\n",
        "    \"brightness_score\",\n",
        "    \"quality_status\",\n",
        "    \"timestamp\",\n",
        "    \"source_folder\"\n",
        "]\n",
        "\n",
        "print(\"‚úì Configuration loaded\")\n",
        "print(f\"Data directory: {DATA_DIR}\")\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "class ImageQualityAssessor:\n",
        "\n",
        "    \n",
        "    def __init__(self, \n",
        "                 min_resolution: Tuple[int, int] = (256, 256),\n",
        "                 max_blur_threshold: float = 100.0,\n",
        "                 min_brightness: int = 20,\n",
        "                 max_brightness: int = 240):\n",
        "        \"\"\"\n",
        "        Initialize quality assessor with thresholds.\n",
        "        \n",
        "        Args:\n",
        "            min_resolution: Minimum (width, height) in pixels\n",
        "            max_blur_threshold: Laplacian variance threshold (lower = blurrier)\n",
        "            min_brightness: Minimum average brightness (0-255)\n",
        "            max_brightness: Maximum average brightness (0-255)\n",
        "            \n",
        "        Raises:\n",
        "            ValueError: If thresholds are invalid\n",
        "        \"\"\"\n",
        "        # Input validation (Senior-level)\n",
        "        if min_resolution[0] < 0 or min_resolution[1] < 0:\n",
        "            raise ValueError(\"Resolution must be positive\")\n",
        "        if max_blur_threshold < 0:\n",
        "            raise ValueError(\"Blur threshold must be positive\")\n",
        "        if not (0 <= min_brightness < max_brightness <= 255):\n",
        "            raise ValueError(\"Brightness thresholds must be in range [0, 255]\")\n",
        "        \n",
        "        self.min_resolution = min_resolution\n",
        "        self.max_blur_threshold = max_blur_threshold\n",
        "        self.min_brightness = min_brightness\n",
        "        self.max_brightness = max_brightness\n",
        "        \n",
        "        logger.info(f\"ImageQualityAssessor initialized with thresholds: {self.__dict__}\")\n",
        "    \n",
        "    def check_blur(self, image: np.ndarray) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Assess image blur using Laplacian variance.\n",
        "        Args:\n",
        "            image: Input image as numpy array\n",
        "            \n",
        "        Returns:\n",
        "            Dict with 'score' (higher = sharper) and 'status' ('good'/'blurry'/'invalid')\n",
        "        \"\"\"\n",
        "        if image is None or not isinstance(image, np.ndarray) or image.size == 0:\n",
        "            logger.warning(\"Invalid image provided for blur check\")\n",
        "            return {\"score\": 0.0, \"status\": \"invalid\"}\n",
        "        \n",
        "        try:\n",
        "            # Convert to grayscale if needed (Senior-level: Handle different formats)\n",
        "            if len(image.shape) == 3:\n",
        "                gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "            elif len(image.shape) == 2:\n",
        "                gray = image\n",
        "            else:\n",
        "                logger.warning(f\"Unexpected image shape: {image.shape}\")\n",
        "                return {\"score\": 0.0, \"status\": \"invalid\"}\n",
        "            \n",
        "            # Calculate Laplacian variance (Senior-level: Efficient computation)\n",
        "            laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
        "            status = \"good\" if laplacian_var >= self.max_blur_threshold else \"blurry\"\n",
        "            \n",
        "            return {\"score\": float(laplacian_var), \"status\": status}\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in blur check: {e}\", exc_info=True)\n",
        "            return {\"score\": 0.0, \"status\": \"error\", \"error\": str(e)}\n",
        "    \n",
        "    def check_brightness(self, image: np.ndarray) -> Dict[str, float]:\n",
        "        \"\"\"Assess image brightness\"\"\"\n",
        "        if image is None or image.size == 0:\n",
        "            return {\"score\": 0.0, \"status\": \"invalid\"}\n",
        "        \n",
        "        if len(image.shape) == 3:\n",
        "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        else:\n",
        "            gray = image\n",
        "        \n",
        "        avg_brightness = np.mean(gray)\n",
        "        \n",
        "        if avg_brightness < self.min_brightness:\n",
        "            status = \"too_dark\"\n",
        "        elif avg_brightness > self.max_brightness:\n",
        "            status = \"too_bright\"\n",
        "        else:\n",
        "            status = \"good\"\n",
        "        \n",
        "        return {\"score\": float(avg_brightness), \"status\": status}\n",
        "    \n",
        "    def check_resolution(self, image: np.ndarray) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Check image resolution.\n",
        "        Args:\n",
        "            image: Input image as numpy array    \n",
        "        Returns:\n",
        "            Dict with width, height, and status\n",
        "        \"\"\"\n",
        "        if image is None or not isinstance(image, np.ndarray) or image.size == 0:\n",
        "            return {\"width\": 0, \"height\": 0, \"status\": \"invalid\"}\n",
        "        \n",
        "        try:\n",
        "            height, width = image.shape[:2]\n",
        "            min_w, min_h = self.min_resolution\n",
        "            status = \"good\" if width >= min_w and height >= min_h else \"too_small\"\n",
        "            \n",
        "            return {\"width\": int(width), \"height\": int(height), \"status\": status}\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in resolution check: {e}\", exc_info=True)\n",
        "            return {\"width\": 0, \"height\": 0, \"status\": \"error\", \"error\": str(e)}\n",
        "    \n",
        "    def check_file_size(self, file_path: Path) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Check file size with proper error handling.\n",
        "        Args:\n",
        "            file_path: Path to file \n",
        "        Returns:\n",
        "            Dict with file size information\n",
        "        \"\"\"\n",
        "        if not isinstance(file_path, Path):\n",
        "            file_path = Path(file_path)\n",
        "        \n",
        "        try:\n",
        "            if not file_path.exists():\n",
        "                logger.warning(f\"File not found: {file_path}\")\n",
        "                return {\"size_bytes\": 0, \"size_kb\": 0.0, \"size_mb\": 0.0, \"status\": \"file_not_found\"}\n",
        "            \n",
        "            size_bytes = os.path.getsize(file_path)\n",
        "            size_kb = size_bytes / 1024\n",
        "            size_mb = size_kb / 1024\n",
        "            \n",
        "            if size_kb < 50:\n",
        "                status = \"too_small\"\n",
        "            elif size_mb > 10:\n",
        "                status = \"too_large\"\n",
        "            else:\n",
        "                status = \"good\"\n",
        "            \n",
        "            return {\n",
        "                \"size_bytes\": int(size_bytes),\n",
        "                \"size_kb\": round(size_kb, 2),\n",
        "                \"size_mb\": round(size_mb, 2),\n",
        "                \"status\": status\n",
        "            }\n",
        "        except OSError as e:\n",
        "            logger.error(f\"OS error checking file size for {file_path}: {e}\")\n",
        "            return {\"size_bytes\": 0, \"size_kb\": 0.0, \"size_mb\": 0.0, \"status\": f\"error: {str(e)}\"}\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Unexpected error checking file size: {e}\", exc_info=True)\n",
        "            return {\"size_bytes\": 0, \"size_kb\": 0.0, \"size_mb\": 0.0, \"status\": f\"error: {str(e)}\"}\n",
        "    \n",
        "    def assess_image(self, image_path: Path) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            image_path: Path to image file\n",
        "            \n",
        "        Returns:\n",
        "            Dict with comprehensive quality metrics\n",
        "        \"\"\"\n",
        "        if not isinstance(image_path, Path):\n",
        "            image_path = Path(image_path)\n",
        "        \n",
        "        results = {\n",
        "            \"image_path\": str(image_path),\n",
        "            \"file_exists\": False,\n",
        "            \"readable\": False,\n",
        "            \"overall_status\": \"unknown\"\n",
        "        }\n",
        "        \n",
        "        if not image_path.exists():\n",
        "            logger.warning(f\"Image file not found: {image_path}\")\n",
        "            results[\"overall_status\"] = \"file_not_found\"\n",
        "            return results\n",
        "        \n",
        "        results[\"file_exists\"] = True\n",
        "        file_size_info = self.check_file_size(image_path)\n",
        "        results.update(file_size_info)\n",
        "        \n",
        "        try:\n",
        "            # Senior-level: Use cv2.IMREAD_UNCHANGED to preserve image properties\n",
        "            image = cv2.imread(str(image_path), cv2.IMREAD_UNCHANGED)\n",
        "            if image is None:\n",
        "                logger.warning(f\"Cannot read image (may be corrupted): {image_path}\")\n",
        "                results[\"overall_status\"] = \"cannot_read\"\n",
        "                return results\n",
        "            \n",
        "            results[\"readable\"] = True\n",
        "            \n",
        "            # Perform all quality checks\n",
        "            resolution_info = self.check_resolution(image)\n",
        "            results.update(resolution_info)\n",
        "            \n",
        "            blur_info = self.check_blur(image)\n",
        "            results[\"blur_score\"] = blur_info.get(\"score\", 0.0)\n",
        "            results[\"blur_status\"] = blur_info.get(\"status\", \"unknown\")\n",
        "            \n",
        "            brightness_info = self.check_brightness(image)\n",
        "            results[\"brightness_score\"] = brightness_info.get(\"score\", 0.0)\n",
        "            results[\"brightness_status\"] = brightness_info.get(\"status\", \"unknown\")\n",
        "            \n",
        "            # Aggregate issues (Senior-level: Structured issue tracking)\n",
        "            issues = []\n",
        "            if resolution_info.get(\"status\") != \"good\":\n",
        "                issues.append(\"resolution\")\n",
        "            if blur_info.get(\"status\") not in [\"good\", \"unknown\"]:\n",
        "                issues.append(\"blur\")\n",
        "            if brightness_info.get(\"status\") not in [\"good\", \"unknown\"]:\n",
        "                issues.append(\"brightness\")\n",
        "            if file_size_info.get(\"status\") != \"good\":\n",
        "                issues.append(\"file_size\")\n",
        "            \n",
        "            if not issues:\n",
        "                results[\"overall_status\"] = \"good\"\n",
        "            else:\n",
        "                results[\"overall_status\"] = f\"issues: {', '.join(issues)}\"\n",
        "                results[\"issues\"] = issues\n",
        "            \n",
        "            logger.debug(f\"Quality assessment complete for {image_path.name}: {results['overall_status']}\")\n",
        "            \n",
        "        except cv2.error as e:\n",
        "            logger.error(f\"OpenCV error processing {image_path}: {e}\", exc_info=True)\n",
        "            results[\"overall_status\"] = f\"opencv_error: {str(e)}\"\n",
        "            results[\"error\"] = str(e)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Unexpected error assessing {image_path}: {e}\", exc_info=True)\n",
        "            results[\"overall_status\"] = f\"error: {str(e)}\"\n",
        "            results[\"error\"] = str(e)\n",
        "        \n",
        "        return results\n",
        "\n",
        "logger.info(\"‚úì ImageQualityAssessor class loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class LabelingWorkflow:\n",
        "    \"\"\"Main workflow for creating labeled dataset\"\"\"\n",
        "    \n",
        "    def __init__(self, \n",
        "                 data_dir: Path,\n",
        "                 output_dir: Path,\n",
        "                 class_labels: Dict[str, str] = None,\n",
        "                 quality_thresholds: Dict = None):\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.output_dir = Path(output_dir)\n",
        "        self.class_labels = class_labels or CLASS_LABELS\n",
        "        quality_thresholds = quality_thresholds or QUALITY_THRESHOLDS\n",
        "        self.quality_assessor = ImageQualityAssessor(**quality_thresholds)\n",
        "        \n",
        "        # Create output structure\n",
        "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
        "        (self.output_dir / \"images\").mkdir(exist_ok=True)\n",
        "        (self.output_dir / \"labels\").mkdir(exist_ok=True)\n",
        "        (self.output_dir / \"metadata\").mkdir(exist_ok=True)\n",
        "    \n",
        "    def extract_class_from_folder(self, folder_path: Path) -> Optional[str]:\n",
        "        \"\"\"Extract class label from folder name\"\"\"\n",
        "        folder_name = folder_path.name\n",
        "        for key, label in self.class_labels.items():\n",
        "            if key.lower() in folder_name.lower():\n",
        "                return label\n",
        "        return None\n",
        "    \n",
        "    def extract_metadata(self, image_path: Path, class_label: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Extract comprehensive metadata for a single image.\n",
        "        \n",
        "        Senior-level: Comprehensive metadata extraction with validation.\n",
        "        \n",
        "        Args:\n",
        "            image_path: Path to image file\n",
        "            class_label: Class label for the image\n",
        "            \n",
        "        Returns:\n",
        "            Dict with comprehensive metadata\n",
        "        \"\"\"\n",
        "        if not isinstance(image_path, Path):\n",
        "            image_path = Path(image_path)\n",
        "        \n",
        "        metadata = {\n",
        "            \"image_path\": str(image_path),\n",
        "            \"class_label\": class_label,\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"source_folder\": str(image_path.parent.name),\n",
        "            \"filename\": image_path.name\n",
        "        }\n",
        "        \n",
        "        # Quality assessment\n",
        "        quality_result = self.quality_assessor.assess_image(image_path)\n",
        "        \n",
        "        metadata.update({\n",
        "            \"width\": quality_result.get(\"width\", 0),\n",
        "            \"height\": quality_result.get(\"height\", 0),\n",
        "            \"file_size_kb\": round(quality_result.get(\"size_kb\", 0), 2),\n",
        "            \"file_size_mb\": round(quality_result.get(\"size_mb\", 0), 3),\n",
        "            \"blur_score\": round(quality_result.get(\"blur_score\", 0), 2),\n",
        "            \"brightness_score\": round(quality_result.get(\"brightness_score\", 0), 2),\n",
        "            \"quality_status\": quality_result.get(\"overall_status\", \"unknown\"),\n",
        "            \"blur_status\": quality_result.get(\"blur_status\", \"unknown\"),\n",
        "            \"brightness_status\": quality_result.get(\"brightness_status\", \"unknown\")\n",
        "        })\n",
        "        \n",
        "        # Senior-level: Add issues list if present\n",
        "        if \"issues\" in quality_result:\n",
        "            metadata[\"quality_issues\"] = quality_result[\"issues\"]\n",
        "        \n",
        "        return metadata\n",
        "    \n",
        "    def process_stage_folder(self, stage_folder: Path) -> List[Dict[str, any]]:\n",
        "        \"\"\"Process all images in a stage folder\"\"\"\n",
        "        class_label = self.extract_class_from_folder(stage_folder)\n",
        "        if not class_label:\n",
        "            print(f\"‚ö† Warning: Could not determine class for {stage_folder}\")\n",
        "            return []\n",
        "        \n",
        "        print(f\"\\nüìÅ Processing {stage_folder.name} -> Class: {class_label}\")\n",
        "        \n",
        "        image_files = []\n",
        "        for ext in [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.JPG\", \"*.JPEG\", \"*.PNG\"]:\n",
        "            image_files.extend(stage_folder.glob(ext))\n",
        "        \n",
        "        if not image_files:\n",
        "            print(f\"  ‚ö† No images found in {stage_folder}\")\n",
        "            return []\n",
        "        \n",
        "        metadata_list = []\n",
        "        \n",
        "        # Senior-level: Progress bar for batch processing\n",
        "        for img_path in tqdm(image_files, desc=f\"Processing {class_label}\", leave=False):\n",
        "            try:\n",
        "                metadata = self.extract_metadata(img_path, class_label)\n",
        "                output_image_name = f\"{class_label}_{img_path.name}\"\n",
        "                output_image_path = self.output_dir / \"images\" / output_image_name\n",
        "                \n",
        "                # Senior-level: Check if file already exists to avoid overwriting\n",
        "                if output_image_path.exists():\n",
        "                    logger.warning(f\"Output file already exists, skipping: {output_image_path}\")\n",
        "                    metadata[\"output_image_path\"] = str(output_image_path)\n",
        "                    metadata[\"output_image_name\"] = output_image_name\n",
        "                    metadata[\"status\"] = \"skipped_duplicate\"\n",
        "                    metadata_list.append(metadata)\n",
        "                    continue\n",
        "                \n",
        "                shutil.copy2(img_path, output_image_path)\n",
        "                metadata[\"output_image_path\"] = str(output_image_path)\n",
        "                metadata[\"output_image_name\"] = output_image_name\n",
        "                metadata[\"status\"] = \"success\"\n",
        "                metadata_list.append(metadata)\n",
        "                logger.debug(f\"Processed: {img_path.name} -> {output_image_name}\")\n",
        "                \n",
        "            except shutil.Error as e:\n",
        "                logger.error(f\"File operation error for {img_path.name}: {e}\")\n",
        "                metadata[\"error\"] = str(e)\n",
        "                metadata[\"status\"] = \"error\"\n",
        "                metadata_list.append(metadata)\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Unexpected error processing {img_path.name}: {e}\", exc_info=True)\n",
        "                metadata = {\"image_path\": str(img_path), \"class_label\": class_label, \"error\": str(e), \"status\": \"error\"}\n",
        "                metadata_list.append(metadata)\n",
        "        \n",
        "        return metadata_list\n",
        "    \n",
        "    def generate_csv_labels(self, metadata_list: List[Dict[str, any]]) -> Path:\n",
        "        \"\"\"Generate CSV file with labels\"\"\"\n",
        "        csv_path = self.output_dir / \"labels\" / \"dataset_labels.csv\"\n",
        "        \n",
        "        fieldnames = METADATA_FIELDS.copy()\n",
        "        fieldnames.extend([\"output_image_path\", \"output_image_name\"])\n",
        "        \n",
        "        with open(csv_path, 'w', newline='', encoding='utf-8') as f:\n",
        "            writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction='ignore')\n",
        "            writer.writeheader()\n",
        "            \n",
        "            for metadata in metadata_list:\n",
        "                row = {k: v for k, v in metadata.items() if k in fieldnames}\n",
        "                writer.writerow(row)\n",
        "        \n",
        "        print(f\"\\n‚úì CSV labels saved: {csv_path}\")\n",
        "        return csv_path\n",
        "    \n",
        "    def generate_json_labels(self, metadata_list: List[Dict[str, any]]) -> Path:\n",
        "        \"\"\"Generate JSON file with labels\"\"\"\n",
        "        json_path = self.output_dir / \"labels\" / \"dataset_labels.json\"\n",
        "        \n",
        "        output_data = {\n",
        "            \"dataset_info\": {\n",
        "                \"total_images\": len(metadata_list),\n",
        "                \"classes\": list(set(m[\"class_label\"] for m in metadata_list)),\n",
        "                \"generated_at\": datetime.now().isoformat()\n",
        "            },\n",
        "            \"images\": metadata_list\n",
        "        }\n",
        "        \n",
        "        with open(json_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
        "        \n",
        "        print(f\"‚úì JSON labels saved: {json_path}\")\n",
        "        return json_path\n",
        "    \n",
        "    def run_workflow(self, \n",
        "                    generate_csv: bool = True,\n",
        "                    generate_json: bool = True) -> Dict[str, any]:\n",
        "        \"\"\"Run complete labeling workflow\"\"\"\n",
        "        print(\"=\"*60)\n",
        "        print(\"BANANA LEAF DATA LABELING WORKFLOW\")\n",
        "        print(\"=\"*60)\n",
        "        \n",
        "        stage_folders = [d for d in self.data_dir.iterdir() \n",
        "                        if d.is_dir() and any(key.lower() in d.name.lower() \n",
        "                                            for key in self.class_labels.keys())]\n",
        "        \n",
        "        if not stage_folders:\n",
        "            print(f\"‚ö† No stage folders found in {self.data_dir}\")\n",
        "            return {\"error\": \"No stage folders found\"}\n",
        "        \n",
        "        print(f\"\\nüìÇ Found {len(stage_folders)} stage folders\")\n",
        "        \n",
        "        all_metadata = []\n",
        "        for stage_folder in sorted(stage_folders):\n",
        "            metadata_list = self.process_stage_folder(stage_folder)\n",
        "            all_metadata.extend(metadata_list)\n",
        "        \n",
        "        if not all_metadata:\n",
        "            print(\"\\n‚ö† No images processed\")\n",
        "            return {\"error\": \"No images processed\"}\n",
        "        \n",
        "        output_files = {}\n",
        "        \n",
        "        if generate_csv:\n",
        "            csv_path = self.generate_csv_labels(all_metadata)\n",
        "            output_files[\"csv\"] = str(csv_path)\n",
        "        \n",
        "        if generate_json:\n",
        "            json_path = self.generate_json_labels(all_metadata)\n",
        "            output_files[\"json\"] = str(json_path)\n",
        "        \n",
        "        class_counts = {}\n",
        "        for metadata in all_metadata:\n",
        "            class_label = metadata.get(\"class_label\", \"unknown\")\n",
        "            class_counts[class_label] = class_counts.get(class_label, 0) + 1\n",
        "        \n",
        "        summary = {\n",
        "            \"total_images\": len(all_metadata),\n",
        "            \"class_distribution\": class_counts,\n",
        "            \"output_files\": output_files,\n",
        "            \"output_directory\": str(self.output_dir)\n",
        "        }\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"WORKFLOW SUMMARY\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"Total images processed: {summary['total_images']}\")\n",
        "        print(f\"\\nClass distribution:\")\n",
        "        for class_label, count in sorted(class_counts.items()):\n",
        "            print(f\"  {class_label}: {count}\")\n",
        "        print(f\"\\nOutput directory: {self.output_dir}\")\n",
        "        print(\"=\"*60)\n",
        "        \n",
        "        return summary\n",
        "\n",
        "print(\"‚úì LabelingWorkflow class loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "valid_params = {\n",
        "    'min_resolution': QUALITY_THRESHOLDS.get('min_resolution', (256, 256)),\n",
        "    'max_blur_threshold': QUALITY_THRESHOLDS.get('max_blur_threshold', 100.0),\n",
        "    'min_brightness': QUALITY_THRESHOLDS.get('min_brightness', 20),\n",
        "    'max_brightness': QUALITY_THRESHOLDS.get('max_brightness', 240)\n",
        "}\n",
        "assessor = ImageQualityAssessor(**valid_params) \n",
        "\n",
        "# Example: Assess Stage1 images\n",
        "stage1_dir = DATA_DIR / \"Stage1\"\n",
        "if stage1_dir.exists():\n",
        "    image_files = list(stage1_dir.glob(\"*.jpg\"))[:5]  # First 5 images\n",
        "    \n",
        "    print(\"Quality Assessment Results:\")\n",
        "    for img_path in image_files:\n",
        "        result = assessor.assess_image(img_path)\n",
        "        print(f\"\\n{img_path.name}:\")\n",
        "        print(f\"  Blur Score: {result.get('blur_score', 0):.2f} ({result.get('blur_status')})\")\n",
        "        print(f\"  Brightness: {result.get('brightness_score', 0):.2f} ({result.get('brightness_status')})\")\n",
        "        print(f\"  Resolution: {result.get('width')}x{result.get('height')}\")\n",
        "        print(f\"  Status: {result.get('overall_status')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    # Filter QUALITY_THRESHOLDS to only include parameters expected by ImageQualityAssessor\n",
        "    quality_params_for_assessor = {\n",
        "        'min_resolution': QUALITY_THRESHOLDS.get('min_resolution'),\n",
        "        'max_blur_threshold': QUALITY_THRESHOLDS.get('max_blur_threshold'),\n",
        "        'min_brightness': QUALITY_THRESHOLDS.get('min_brightness'),\n",
        "        'max_brightness': QUALITY_THRESHOLDS.get('max_brightness')\n",
        "    }\n",
        "    quality_params_for_assessor = {k: v for k, v in quality_params_for_assessor.items() if v is not None}\n",
        "\n",
        "    workflow = LabelingWorkflow(\n",
        "        data_dir=DATA_DIR,\n",
        "        output_dir=OUTPUT_DIR,\n",
        "        quality_thresholds=quality_params_for_assessor # Pass the filtered parameters\n",
        "    )\n",
        "\n",
        "    logger.info(\"Starting labeling workflow...\")\n",
        "    results = workflow.run_workflow(\n",
        "        generate_csv=True,\n",
        "        generate_json=True\n",
        "    )\n",
        "\n",
        "    if \"error\" in results:\n",
        "        logger.error(f\"Workflow failed: {results['error']}\")\n",
        "    else:\n",
        "        logger.info(f\"Workflow completed successfully. Processed {results.get('total_images', 0)} images.\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(f\"Fatal error in workflow: {e}\", exc_info=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if \"error\" not in results:\n",
        "    # Load CSV to display\n",
        "    csv_path = Path(results[\"output_files\"][\"csv\"])\n",
        "    if csv_path.exists():\n",
        "        df = pd.read_csv(csv_path)\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"DATASET SUMMARY \")\n",
        "        print(\"=\"*60)\n",
        "        \n",
        "        print(f\"\\nüìä Total Images: {len(df)}\")\n",
        "        \n",
        "        print(f\"\\nüìà Class Distribution:\")\n",
        "        class_dist = df[\"class_label\"].value_counts()\n",
        "        print(class_dist)\n",
        "        print(f\"   Balance ratio: {class_dist.max() / class_dist.min():.2f}x\")\n",
        "        \n",
        "        print(f\"\\n‚úÖ Quality Status Distribution:\")\n",
        "        quality_dist = df[\"quality_status\"].value_counts()\n",
        "        print(quality_dist)\n",
        "        \n",
        "        # Senior-level: Statistical summary\n",
        "        print(f\"\\nüìê Image Statistics:\")\n",
        "        print(f\"   Average Resolution: {df['width'].mean():.0f}x{df['height'].mean():.0f}\")\n",
        "        print(f\"   Average Blur Score: {df['blur_score'].mean():.2f}\")\n",
        "        print(f\"   Average Brightness: {df['brightness_score'].mean():.2f}\")\n",
        "        print(f\"   Average File Size: {df['file_size_kb'].mean():.2f} KB\")\n",
        "        \n",
        "        # Senior-level: Quality issues analysis\n",
        "        if \"quality_issues\" in df.columns:\n",
        "            print(f\"\\n‚ö†Ô∏è  Quality Issues:\")\n",
        "            issue_counts = df[\"quality_issues\"].value_counts()\n",
        "            print(issue_counts)\n",
        "        \n",
        "        print(f\"\\nüìã Sample Data (first 5 rows):\")\n",
        "        display_cols = [\"output_image_name\", \"class_label\", \"width\", \"height\", \n",
        "                       \"blur_score\", \"brightness_score\", \"quality_status\"]\n",
        "        available_cols = [col for col in display_cols if col in df.columns]\n",
        "        print(df[available_cols].head().to_string())\n",
        "        \n",
        "        print(f\"\\n‚úì Labels saved to: {csv_path}\")\n",
        "        print(f\"‚úì JSON saved to: {results['output_files']['json']}\")\n",
        "        print(f\"‚úì Log file: data_labeling.log\")\n",
        "        \n",
        "        logger.info(\"Summary displayed successfully\")\n",
        "else:\n",
        "    error_msg = results.get('error', 'Unknown error')\n",
        "    logger.error(f\"Workflow error: {error_msg}\")\n",
        "    print(f\"‚ùå Error: {error_msg}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "FINAL_DATASET_DIR = OUTPUT_DIR / 'final_dataset'\n",
        "IMAGE_SOURCE_DIR = OUTPUT_DIR / 'images'\n",
        "\n",
        "# Create the final_dataset directory if it doesn't exist\n",
        "FINAL_DATASET_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Load the dataset labels into a pandas DataFrame\n",
        "labels_df = pd.read_csv(results['output_files']['csv'])\n",
        "\n",
        "print(f\"‚úì Imported train_test_split.\")\n",
        "print(f\"‚úì Final dataset directory defined: {FINAL_DATASET_DIR}\")\n",
        "print(f\"‚úì Image source directory defined: {IMAGE_SOURCE_DIR}\")\n",
        "print(f\"‚úì Labels DataFrame loaded with {len(labels_df)} entries.\")\n",
        "labels_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "random_state = 42\n",
        "\n",
        "# 1. Split labels_df into training and temporary combined validation/test set\n",
        "train_df, val_test_df = train_test_split(\n",
        "    labels_df,\n",
        "    test_size=0.3,\n",
        "    random_state=random_state,\n",
        "    stratify=labels_df['class_label']\n",
        ")\n",
        "\n",
        "# 2. Further split val_test_df into validation and test sets\n",
        "val_df, test_df = train_test_split(\n",
        "    val_test_df,\n",
        "    test_size=0.5,\n",
        "    random_state=random_state,\n",
        "    stratify=val_test_df['class_label']\n",
        ")\n",
        "\n",
        "# 3. Print the number of samples in each resulting DataFrame\n",
        "print(f\"\\nDataset split summary:\")\n",
        "print(f\"  Training set: {len(train_df)} samples\")\n",
        "print(f\"  Validation set: {len(val_df)} samples\")\n",
        "print(f\"  Test set: {len(test_df)} samples\")\n",
        "\n",
        "print(f\"\\nClass distribution in Training set:\\n{train_df['class_label'].value_counts()}\\n\")\n",
        "print(f\"Class distribution in Validation set:\\n{val_df['class_label'].value_counts()}\\n\")\n",
        "print(f\"Class distribution in Test set:\\n{test_df['class_label'].value_counts()}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "split_types = ['train', 'validation', 'test']\n",
        "\n",
        "for split_type in split_types:\n",
        "    split_dir = FINAL_DATASET_DIR / split_type\n",
        "    split_dir.mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"‚úì Created directory: {split_dir}\")\n",
        "\n",
        "    # Create 'images' and 'labels' subdirectories within each split\n",
        "    images_dir = split_dir / 'images'\n",
        "    images_dir.mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"  ‚úì Created directory: {images_dir}\")\n",
        "\n",
        "    labels_dir = split_dir / 'labels'\n",
        "    labels_dir.mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"  ‚úì Created directory: {labels_dir}\")\n",
        "\n",
        "print(\"\\n‚úì Adjusted dataset directory structure created successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "splits_to_process = [\n",
        "    (train_df, 'train'),\n",
        "    (val_df, 'validation'),\n",
        "    (test_df, 'test')\n",
        "]\n",
        "\n",
        "for df_split, split_type in splits_to_process:\n",
        "    print(f\"\\nCopying images for {split_type} split...\")\n",
        "    for index, row in tqdm(df_split.iterrows(), total=len(df_split), desc=f\"Copying {split_type} images\"):\n",
        "        source_image_name = row['output_image_name']\n",
        "        # The class_label is implicitly part of the structure, but not needed for the image copy destination itself in this new structure\n",
        "\n",
        "        source_path = IMAGE_SOURCE_DIR / source_image_name\n",
        "        destination_path = FINAL_DATASET_DIR / split_type / 'images' / source_image_name\n",
        "\n",
        "        try:\n",
        "            shutil.copy2(source_path, destination_path)\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Warning: Source file not found: {source_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error copying {source_image_name} to {destination_path}: {e}\")\n",
        "    print(f\"‚úì Finished copying images for {split_type} split.\")\n",
        "\n",
        "print(\"\\n‚úì All images copied to their respective split directories.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "split_dataframes = [\n",
        "    (train_df, 'train'),\n",
        "    (val_df, 'validation'),\n",
        "    (test_df, 'test')\n",
        "]\n",
        "\n",
        "for df, split_type in split_dataframes:\n",
        "    output_csv_path = FINAL_DATASET_DIR / split_type / 'labels' / f'{split_type}_labels.csv'\n",
        "    df.to_csv(output_csv_path, index=False)\n",
        "    print(f\"‚úì {split_type.capitalize()} labels saved to: {output_csv_path}\")\n",
        "\n",
        "print(\"\\n‚úì All split label CSVs generated.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "YOLO_CLASSIFICATION_DIR = OUTPUT_DIR / 'yolo_classification_dataset'\n",
        "# Create YOLO classification structure\n",
        "yolo_train_dir = YOLO_CLASSIFICATION_DIR / 'train'\n",
        "yolo_val_dir = YOLO_CLASSIFICATION_DIR / 'validation'\n",
        "yolo_test_dir = YOLO_CLASSIFICATION_DIR / 'test'\n",
        "\n",
        "for split_dir in [yolo_train_dir, yolo_val_dir, yolo_test_dir]:\n",
        "    split_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"\\n YOLO classification dataset directory: {YOLO_CLASSIFICATION_DIR}\")  \n",
        "train_csv = FINAL_DATASET_DIR / 'train' / 'labels' / 'train_labels.csv'\n",
        "val_csv = FINAL_DATASET_DIR / 'validation' / 'labels' / 'validation_labels.csv'\n",
        "test_csv = FINAL_DATASET_DIR / 'test' / 'labels' / 'test_labels.csv'\n",
        "\n",
        "print(f\"\\n Loading split CSVs...\")\n",
        "print(f\"   Train: {train_csv.exists()}\")\n",
        "print(f\"   Validation: {val_csv.exists()}\")\n",
        "print(f\"   Test: {test_csv.exists()}\")\n",
        "\n",
        "\n",
        "if not all([train_csv.exists(), val_csv.exists(), test_csv.exists()]):\n",
        "    print(\"\\n ERROR: Split CSVs not found!\")\n",
        "    print(\"   Please run Cells 7-11 (data splitting) first\")\n",
        "else:\n",
        "    print(\"All split CSVs found!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Define splits mapping\n",
        "splits = {\n",
        "    'train': (train_csv, yolo_train_dir),\n",
        "    'validation': (val_csv, yolo_val_dir),\n",
        "    'test': (test_csv, yolo_test_dir)\n",
        "}\n",
        "\n",
        "# Get all unique classes from all splits\n",
        "all_classes = set()\n",
        "for split_name, (csv_path, _) in splits.items():\n",
        "    if csv_path.exists():\n",
        "        df = pd.read_csv(csv_path)\n",
        "        all_classes.update(df['class_label'].unique())\n",
        "\n",
        "# Sort classes for consistent mapping\n",
        "# Order: Healthy, Stage1, Stage2, Stage3, Stage4, Stage5, Stage6\n",
        "sorted_classes = sorted(list(all_classes))\n",
        "class_to_id = {cls: idx for idx, cls in enumerate(sorted_classes)}\n",
        "id_to_class = {idx: cls for cls, idx in class_to_id.items()}\n",
        "\n",
        "print(f\"\\nClasses found: {sorted_classes}\")\n",
        "print(f\"\\nClass to ID mapping:\")\n",
        "for cls, idx in class_to_id.items():\n",
        "    print(f\"   {idx}: {cls}\")\n",
        "\n",
        "# Process each split\n",
        "for split_name, (csv_path, yolo_dir) in splits.items():\n",
        "    if not csv_path.exists():\n",
        "        continue\n",
        "\n",
        "    print(f\" Processing {split_name.upper()} split...\")\n",
        "    \n",
        "    \n",
        "    df = pd.read_csv(csv_path)\n",
        "    \n",
        "    # Copy images and organize by class\n",
        "    copied_count = 0\n",
        "    skipped_count = 0\n",
        "    \n",
        "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=f\"Processing {split_name}\"):\n",
        "        try:\n",
        "            # Get image path and class from CSV\n",
        "            source_image_path = Path(row['output_image_path'])\n",
        "            class_label = row['class_label']  # Healthy, Stage1, Stage2, etc.\n",
        "            \n",
        "            # Create class directory in YOLO format\n",
        "            # YOLO classification: images organized by class name\n",
        "            class_dir = yolo_dir / class_label\n",
        "            class_dir.mkdir(parents=True, exist_ok=True)\n",
        "            \n",
        "            # Copy image to class directory\n",
        "            if source_image_path.exists():\n",
        "                dest_image_path = class_dir / source_image_path.name\n",
        "                shutil.copy2(source_image_path, dest_image_path)\n",
        "                copied_count += 1\n",
        "            else:\n",
        "                skipped_count += 1\n",
        "                logger.warning(f\"Image not found: {source_image_path}\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing {row.get('output_image_name', 'unknown')}: {e}\")\n",
        "            skipped_count += 1\n",
        "    \n",
        "    print(f\"\\n {split_name.capitalize()} split:\")\n",
        "    print(f\"   Copied: {copied_count} images\")\n",
        "    print(f\"   Skipped: {skipped_count} images\")\n",
        "    \n",
        "    # Show class distribution\n",
        "    class_counts = df['class_label'].value_counts()\n",
        "    print(f\"\\n   Class distribution:\")\n",
        "    for cls, count in class_counts.items():\n",
        "        print(f\"      {cls}: {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create YOLO dataset.yaml file\n",
        "yolo_yaml_path = YOLO_CLASSIFICATION_DIR / 'dataset.yaml'\n",
        "\n",
        "# YOLO classification configuration\n",
        "yolo_config = {\n",
        "    'path': str(YOLO_CLASSIFICATION_DIR.absolute()),\n",
        "    'train': 'train',\n",
        "    'val': 'validation',\n",
        "    'test': 'test',\n",
        "    'names': id_to_class,  # {0: 'Healthy', 1: 'Stage1', 2: 'Stage2', ...}\n",
        "    'nc': len(sorted_classes)  # Number of classes\n",
        "}\n",
        "\n",
        "with open(yolo_yaml_path, 'w') as f:\n",
        "    yaml.dump(yolo_config, f, default_flow_style=False, sort_keys=False)\n",
        "\n",
        "print(f\"\\nYAML file created: {yolo_yaml_path}\")\n",
        "print(f\"\\nConfiguration:\")\n",
        "print(f\"   Path: {yolo_config['path']}\")\n",
        "print(f\"   Train: {yolo_config['train']}\")\n",
        "print(f\"   Validation: {yolo_config['val']}\")\n",
        "print(f\"   Test: {yolo_config['test']}\")\n",
        "print(f\"   Number of classes: {yolo_config['nc']}\")\n",
        "print(f\"   Classes: {list(yolo_config['names'].values())}\")\n",
        "\n",
        "with open(yolo_yaml_path, 'r') as f:\n",
        "    print(f.read())\n",
        "\n",
        "for split_name in ['train', 'validation', 'test']:\n",
        "    split_dir = YOLO_CLASSIFICATION_DIR / split_name\n",
        "    if split_dir.exists():\n",
        "        print(f\"\\nüìÇ {split_name.upper()}:\")\n",
        "        class_dirs = [d for d in split_dir.iterdir() if d.is_dir()]\n",
        "        total_images = 0\n",
        "        \n",
        "        for class_dir in sorted(class_dirs):\n",
        "            images = list(class_dir.glob('*.jpg')) + list(class_dir.glob('*.JPG')) + \\\n",
        "                     list(class_dir.glob('*.png')) + list(class_dir.glob('*.PNG'))\n",
        "            total_images += len(images)\n",
        "            print(f\"   {class_dir.name}: {len(images)} images\")\n",
        "        \n",
        "        print(f\"   Total: {total_images} images\")\n",
        "\n",
        "print(f\"\\nüìÅ Dataset location: {YOLO_CLASSIFICATION_DIR}\")\n",
        "print(f\"   Config file: {yolo_yaml_path}\")\n",
        "print(f\"\\n Next step: Train YOLO classification model\")\n",
        "print(f\"   Use: model = YOLO('yolov8n-cls.pt')  # or yolov8s-cls.pt, yolov8m-cls.pt\")\n",
        "print(f\"   model.train(data='{yolo_yaml_path}', epochs=100)\")\n",
        "print(f\"   Dataset config: {yolo_yaml_path}\")\n",
        "print(f\"   Dataset location: {YOLO_CLASSIFICATION_DIR}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
