# Copy to .env and fill in values.

# --- Airflow (docker-airflow.yml). Required on Windows so container files have correct owner. ---
# Linux: use AIRFLOW_UID=$(id -u). Windows: use 50000.
AIRFLOW_UID=50000

# --- PostgreSQL (feedback enhancement + scripts/setup_database.py) ---
POSTGRES_HOST=localhost
POSTGRES_PORT=5433
POSTGRES_DB=ai_banana_early_stage
POSTGRES_USER=postgres
POSTGRES_PASSWORD=012345

# --- Phase 2: MinIO (object storage). Set STORAGE_ENDPOINT to enable. ---
# Use the S3 API port (usually 9000), NOT the Console port (9001). "S3 API Requests must be made to API port" = wrong port.
# With Docker: host.docker.internal:9000 from container, or localhost:9000 from host.
STORAGE_ENDPOINT=localhost:9000
STORAGE_ACCESS_KEY=minioadmin
STORAGE_SECRET_KEY=minioadmin123
STORAGE_SECURE=false
# Bucket name (default: ai-banana-early-stage)
# STORAGE_BUCKET=ai-banana-early-stage

# --- MLflow (Enhancement 2). Optional: for training script / Python client. ---
# MLflow server runs in Docker (docker-mlflow.yml). From host use:
# MLFLOW_TRACKING_URI=http://localhost:5000
# If you need to connect to MLflow's Postgres from host (e.g. pgAdmin), port is 55432.
# Run stack: docker compose -f docker/docker-mlops-pipeline.yml up -d
# POSTGRES_MLFLOW_HOST=localhost
# POSTGRES_MLFLOW_PORT=55432

# --- Optional: override server (default 0.0.0.0:8000) ---
# HOST=0.0.0.0
# PORT=8000
