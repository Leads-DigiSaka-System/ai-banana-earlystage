# Enhancement 4: Improve Training Depth & Model Performance

## ğŸ“‹ Overview

**Enhancement Type:** Model Optimization / Performance  
**Priority:** ğŸŸ¡ MEDIUM  
**Estimated Timeline:** 2-3 weeks  
**Complexity:** Medium  
**Dependencies:** Enhancement 1 (Feedback Collection), Enhancement 2 (MLOps Pipeline)

---

## ğŸ¯ Why This Enhancement is Needed

### Current Problem:
Your current model has room for improvement:
- âŒ Using YOLO12n (nano) - smallest/fastest model
- âŒ No hyperparameter optimization
- âŒ Basic augmentation strategy
- âŒ Limited training epochs (10-50)
- âŒ No ensemble methods
- âŒ No fine-tuning capability
- âŒ May not achieve optimal accuracy

### Current Model Performance:
```
Model: YOLO12n (nano)
Parameters: ~3 million
Training: 10-50 epochs
mAP50: ~0.70-0.75 (estimated)
mAP50-95: ~0.50-0.55
Inference: ~15ms on CPU, ~5ms on GPU
```

### Target Performance After Enhancement:
```
Model: YOLO12s/m (small/medium) or ensemble
Parameters: ~11-25 million
Training: 100+ epochs with optimization
mAP50: > 0.85 (target)
mAP50-95: > 0.65
Inference: ~30-50ms on CPU, ~10-15ms on GPU
```

### Benefits After Implementation:
1. âœ… **Better accuracy** - Detect disease stages more reliably
2. âœ… **Fewer false positives** - Reduce incorrect classifications
3. âœ… **Better generalization** - Work on diverse images
4. âœ… **Automated optimization** - Find best hyperparameters automatically
5. âœ… **Fine-tuning capability** - Update model without full retraining
6. âœ… **Ensemble option** - Combine models for best results

---

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Training Strategy                          â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚  Baseline  â”‚  â”‚  Optimized    â”‚  â”‚  Ensemble    â”‚       â”‚
â”‚  â”‚  (Current) â”‚  â”‚  Single Model â”‚  â”‚  (Advanced)  â”‚       â”‚
â”‚  â”‚            â”‚  â”‚               â”‚  â”‚              â”‚       â”‚
â”‚  â”‚  YOLO12n   â”‚  â”‚  YOLO12s/m    â”‚  â”‚  Multiple    â”‚       â”‚
â”‚  â”‚  Default   â”‚  â”‚  + Optuna     â”‚  â”‚  models      â”‚       â”‚
â”‚  â”‚  params    â”‚  â”‚  + Advanced   â”‚  â”‚  + Voting    â”‚       â”‚
â”‚  â”‚            â”‚  â”‚    augment    â”‚  â”‚              â”‚       â”‚
â”‚  â”‚  mAP: 0.70 â”‚  â”‚  mAP: 0.85    â”‚  â”‚  mAP: 0.88   â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ Implementation Guide

### Phase 1: Larger Model Training (Days 1-3)

#### Step 1.1: Train YOLO12s (Small)

Create `training/train_larger_models.py`:

```python
# training/train_larger_models.py

from ultralytics import YOLO
from pathlib import Path
import yaml

def train_yolo12s(
    data_yaml: str = "combined_yolo_dataset/data.yaml",
    epochs: int = 100,
    batch_size: int = 16,  # Smaller batch for larger model
    img_size: int = 736,
    device: str = "0"  # GPU
):
    """
    Train YOLO12s (small) model
    
    YOLO12s has:
    - ~11M parameters (vs 3M for nano)
    - Better feature extraction
    - Higher accuracy
    - Slower inference (but still fast)
    """
    print("Training YOLO12s (Small) model...")
    
    # Initialize model
    model = YOLO('yolo12s.pt')
    
    # Train with optimized parameters
    results = model.train(
        data=data_yaml,
        epochs=epochs,
        imgsz=img_size,
        batch=batch_size,
        
        # Optimizer settings
        optimizer='AdamW',
        lr0=0.001,          # Initial learning rate
        lrf=0.0001,         # Final learning rate
        momentum=0.937,
        weight_decay=0.0005,
        warmup_epochs=5,
        warmup_momentum=0.8,
        warmup_bias_lr=0.1,
        
        # Loss weights
        box=7.5,
        cls=0.5,
        dfl=1.5,
        
        # Augmentation
        hsv_h=0.015,        # Hue augmentation
        hsv_s=0.7,          # Saturation
        hsv_v=0.4,          # Value
        degrees=10,         # Rotation
        translate=0.1,      # Translation
        scale=0.9,          # Scale
        shear=0.0,          # Shear
        perspective=0.0,    # Perspective
        flipud=0.0,         # Vertical flip (usually 0 for objects)
        fliplr=0.5,         # Horizontal flip
        mosaic=1.0,         # Mosaic augmentation
        mixup=0.15,         # Mixup augmentation
        copy_paste=0.0,     # Copy-paste augmentation
        
        # Early stopping
        patience=20,
        
        # Save settings
        save=True,
        save_period=10,
        plots=True,
        
        # Other
        device=device,
        workers=8,
        project='runs/train_yolo12s',
        name='experiment_001',
        exist_ok=True,
        verbose=True
    )
    
    return results

def train_yolo12m(
    data_yaml: str = "combined_yolo_dataset/data.yaml",
    epochs: int = 100,
    batch_size: int = 8,  # Even smaller batch
    img_size: int = 736,
    device: str = "0"
):
    """
    Train YOLO12m (medium) model
    
    YOLO12m has:
    - ~25M parameters
    - Best balance of accuracy and speed
    - Recommended for production
    """
    print("Training YOLO12m (Medium) model...")
    
    model = YOLO('yolo12m.pt')
    
    results = model.train(
        data=data_yaml,
        epochs=epochs,
        imgsz=img_size,
        batch=batch_size,
        
        # Similar settings to YOLO12s
        optimizer='AdamW',
        lr0=0.001,
        lrf=0.0001,
        
        patience=25,  # More patience for larger model
        
        device=device,
        project='runs/train_yolo12m',
        name='experiment_001'
    )
    
    return results

def compare_models():
    """Compare trained models"""
    import pandas as pd
    
    models = {
        'YOLO12n': 'runs/train_yolo12n/experiment_001/weights/best.pt',
        'YOLO12s': 'runs/train_yolo12s/experiment_001/weights/best.pt',
        'YOLO12m': 'runs/train_yolo12m/experiment_001/weights/best.pt'
    }
    
    results = []
    
    for model_name, model_path in models.items():
        if Path(model_path).exists():
            model = YOLO(model_path)
            
            # Validate on test set
            metrics = model.val(
                data='combined_yolo_dataset/data.yaml',
                split='test'
            )
            
            results.append({
                'Model': model_name,
                'mAP50': metrics.box.map50,
                'mAP50-95': metrics.box.map,
                'Precision': metrics.box.p,
                'Recall': metrics.box.r,
                'Parameters': model.model.parameters(),
                'FPS (CPU)': 1 / metrics.speed['inference']  # Approximate
            })
    
    df = pd.DataFrame(results)
    print("\nModel Comparison:")
    print(df.to_string(index=False))
    
    return df

if __name__ == "__main__":
    # Train YOLO12s
    results_s = train_yolo12s()
    
    # Train YOLO12m (if you have GPU with enough VRAM)
    # results_m = train_yolo12m()
    
    # Compare
    compare_models()
```

Run training:
```bash
# Train YOLO12s
python training/train_larger_models.py
```

---

### Phase 2: Hyperparameter Optimization (Days 4-7)

#### Step 2.1: Setup Optuna

```bash
pip install optuna optuna-dashboard
```

#### Step 2.2: Create Hyperparameter Tuning Script

Create `training/hyperparameter_tuning.py`:

```python
# training/hyperparameter_tuning.py

import optuna
from ultralytics import YOLO
from pathlib import Path
import yaml

def objective(trial):
    """
    Optuna objective function for hyperparameter optimization
    
    Args:
        trial: Optuna trial object
        
    Returns:
        Validation mAP50 (to maximize)
    """
    # Suggest hyperparameters
    lr0 = trial.suggest_float('lr0', 1e-5, 1e-2, log=True)
    lrf = trial.suggest_float('lrf', 1e-6, 1e-3, log=True)
    momentum = trial.suggest_float('momentum', 0.8, 0.98)
    weight_decay = trial.suggest_float('weight_decay', 1e-5, 1e-3, log=True)
    warmup_epochs = trial.suggest_int('warmup_epochs', 1, 10)
    
    # Batch size
    batch = trial.suggest_categorical('batch', [8, 16, 32])
    
    # Image size
    imgsz = trial.suggest_categorical('imgsz', [640, 736, 832])
    
    # Loss weights
    box = trial.suggest_float('box', 5.0, 10.0)
    cls = trial.suggest_float('cls', 0.3, 1.0)
    dfl = trial.suggest_float('dfl', 1.0, 2.0)
    
    # Augmentation
    hsv_h = trial.suggest_float('hsv_h', 0.0, 0.05)
    hsv_s = trial.suggest_float('hsv_s', 0.3, 0.9)
    hsv_v = trial.suggest_float('hsv_v', 0.2, 0.6)
    degrees = trial.suggest_float('degrees', 0.0, 20.0)
    translate = trial.suggest_float('translate', 0.0, 0.2)
    scale = trial.suggest_float('scale', 0.5, 0.95)
    fliplr = trial.suggest_float('fliplr', 0.0, 0.7)
    mosaic = trial.suggest_float('mosaic', 0.5, 1.0)
    mixup = trial.suggest_float('mixup', 0.0, 0.3)
    
    # Initialize model
    model = YOLO('yolo12s.pt')
    
    # Train with suggested parameters
    try:
        results = model.train(
            data='combined_yolo_dataset/data.yaml',
            epochs=50,  # Fewer epochs for tuning
            imgsz=imgsz,
            batch=batch,
            
            # Optimizer
            optimizer='AdamW',
            lr0=lr0,
            lrf=lrf,
            momentum=momentum,
            weight_decay=weight_decay,
            warmup_epochs=warmup_epochs,
            
            # Loss weights
            box=box,
            cls=cls,
            dfl=dfl,
            
            # Augmentation
            hsv_h=hsv_h,
            hsv_s=hsv_s,
            hsv_v=hsv_v,
            degrees=degrees,
            translate=translate,
            scale=scale,
            fliplr=fliplr,
            mosaic=mosaic,
            mixup=mixup,
            
            # Other
            patience=10,
            save=False,  # Don't save intermediate models
            plots=False,
            device='0',
            workers=8,
            project=f'runs/optuna',
            name=f'trial_{trial.number}',
            exist_ok=True,
            verbose=False  # Less verbose output
        )
        
        # Return validation mAP50 (metric to maximize)
        val_map50 = results.results_dict['metrics/mAP50(B)']
        
        # Also report other metrics
        trial.set_user_attr('val_map50_95', results.results_dict['metrics/mAP50-95(B)'])
        trial.set_user_attr('precision', results.results_dict['metrics/precision(B)'])
        trial.set_user_attr('recall', results.results_dict['metrics/recall(B)'])
        
        return val_map50
        
    except Exception as e:
        print(f"Trial {trial.number} failed: {e}")
        return 0.0  # Return poor score on failure

def run_optimization(n_trials: int = 50):
    """
    Run hyperparameter optimization
    
    Args:
        n_trials: Number of trials to run
    """
    # Create study
    study = optuna.create_study(
        study_name='yolo12s_optimization',
        direction='maximize',  # Maximize mAP50
        storage='sqlite:///optuna_study.db',  # Persist study to database
        load_if_exists=True,
        sampler=optuna.samplers.TPESampler(seed=42)
    )
    
    # Run optimization
    study.optimize(
        objective,
        n_trials=n_trials,
        n_jobs=1,  # Parallel trials (if you have multiple GPUs)
        show_progress_bar=True
    )
    
    # Print results
    print("\n" + "="*80)
    print("Optimization Complete!")
    print("="*80)
    print(f"\nBest trial: {study.best_trial.number}")
    print(f"Best mAP50: {study.best_value:.4f}")
    print("\nBest hyperparameters:")
    for key, value in study.best_params.items():
        print(f"  {key}: {value}")
    
    # Print top 5 trials
    print("\nTop 5 trials:")
    top_trials = sorted(study.trials, key=lambda t: t.value if t.value else 0, reverse=True)[:5]
    for i, trial in enumerate(top_trials, 1):
        print(f"{i}. Trial {trial.number}: mAP50 = {trial.value:.4f}")
    
    # Save best parameters
    best_params_path = Path('training/best_hyperparameters.yaml')
    with open(best_params_path, 'w') as f:
        yaml.dump(study.best_params, f, default_flow_style=False)
    
    print(f"\nBest parameters saved to: {best_params_path}")
    
    return study

def visualize_optimization(study_name: str = 'yolo12s_optimization'):
    """Visualize optimization results"""
    import optuna
    from optuna.visualization import (
        plot_optimization_history,
        plot_param_importances,
        plot_parallel_coordinate,
        plot_slice
    )
    
    # Load study
    study = optuna.load_study(
        study_name=study_name,
        storage='sqlite:///optuna_study.db'
    )
    
    # Create visualizations
    fig1 = plot_optimization_history(study)
    fig1.write_image('runs/optuna/optimization_history.png')
    
    fig2 = plot_param_importances(study)
    fig2.write_image('runs/optuna/param_importances.png')
    
    fig3 = plot_parallel_coordinate(study)
    fig3.write_image('runs/optuna/parallel_coordinate.png')
    
    fig4 = plot_slice(study)
    fig4.write_image('runs/optuna/param_slices.png')
    
    print("Visualizations saved to runs/optuna/")

if __name__ == "__main__":
    # Run optimization
    study = run_optimization(n_trials=50)
    
    # Visualize results
    visualize_optimization()
```

Run optimization:
```bash
# This will take several hours/days depending on n_trials
python training/hyperparameter_tuning.py
```

#### Step 2.3: Train with Optimized Hyperparameters

```python
# training/train_optimized.py

from ultralytics import YOLO
import yaml

def train_with_best_params():
    """Train model with best hyperparameters from Optuna"""
    
    # Load best parameters
    with open('training/best_hyperparameters.yaml', 'r') as f:
        best_params = yaml.safe_load(f)
    
    print("Training with optimized hyperparameters:")
    for key, value in best_params.items():
        print(f"  {key}: {value}")
    
    # Initialize model
    model = YOLO('yolo12s.pt')
    
    # Train with best parameters
    results = model.train(
        data='combined_yolo_dataset/data.yaml',
        epochs=100,  # Full training now
        device='0',
        **best_params  # Unpack best parameters
    )
    
    return results

if __name__ == "__main__":
    results = train_with_best_params()
```

---

### Phase 3: Fine-Tuning Capability (Days 8-10)

#### Step 3.1: Implement Fine-Tuning

Create `training/fine_tuning.py`:

```python
# training/fine_tuning.py

from ultralytics import YOLO
from pathlib import Path
import torch

def fine_tune_model(
    base_model_path: str,
    new_data_yaml: str,
    epochs: int = 20,
    freeze_layers: int = 10,
    learning_rate: float = 0.0001  # Lower LR for fine-tuning
):
    """
    Fine-tune existing model with new data
    
    Strategy:
    - Freeze early layers (feature extractors)
    - Only train later layers + head
    - Use lower learning rate
    - Use fewer epochs
    
    Args:
        base_model_path: Path to existing model
        new_data_yaml: Path to new training data
        epochs: Number of fine-tuning epochs
        freeze_layers: Number of layers to freeze
        learning_rate: Fine-tuning learning rate
    """
    print(f"Fine-tuning model from: {base_model_path}")
    print(f"Freezing first {freeze_layers} layers")
    
    # Load existing model
    model = YOLO(base_model_path)
    
    # Freeze early layers
    for i, (name, param) in enumerate(model.model.named_parameters()):
        if i < freeze_layers:
            param.requires_grad = False
            print(f"  Frozen: {name}")
        else:
            print(f"  Trainable: {name}")
    
    # Fine-tune with lower learning rate
    results = model.train(
        data=new_data_yaml,
        epochs=epochs,
        imgsz=736,
        batch=16,
        lr0=learning_rate,
        lrf=learning_rate / 10,
        optimizer='AdamW',
        patience=10,
        freeze=freeze_layers,  # YOLO built-in freeze
        resume=False,  # Don't resume, start fresh
        plots=True,
        device='0',
        project='runs/fine_tune',
        name='fine_tune_001'
    )
    
    print(f"\nFine-tuning complete!")
    print(f"Final mAP50: {results.results_dict['metrics/mAP50(B)']:.4f}")
    
    return results

def incremental_learning(
    base_model_path: str,
    new_samples_dir: Path,
    threshold: int = 500
):
    """
    Incremental learning: Add new data and fine-tune
    
    Args:
        base_model_path: Current model
        new_samples_dir: Directory with new samples
        threshold: Min samples needed to trigger fine-tuning
    """
    # Count new samples
    new_sample_count = len(list(new_samples_dir.rglob('*.jpg')))
    
    if new_sample_count < threshold:
        print(f"Only {new_sample_count} new samples (< {threshold}). Skipping fine-tuning.")
        return None
    
    print(f"Found {new_sample_count} new samples. Starting fine-tuning...")
    
    # Prepare new data YAML
    # (This would merge new samples with existing data)
    new_data_yaml = prepare_incremental_dataset(new_samples_dir)
    
    # Fine-tune
    results = fine_tune_model(
        base_model_path=base_model_path,
        new_data_yaml=new_data_yaml,
        epochs=20,
        freeze_layers=8
    )
    
    return results

if __name__ == "__main__":
    # Example: Fine-tune best model with new feedback data
    fine_tune_model(
        base_model_path='models/weights/best.pt',
        new_data_yaml='new_feedback_data/data.yaml',
        epochs=20,
        freeze_layers=10
    )
```

---

### Phase 4: Ensemble Methods (Days 11-14)

#### Step 4.1: Create Ensemble Predictor

Create `services/ensemble_predictor.py`:

```python
# services/ensemble_predictor.py

from ultralytics import YOLO
from typing import List, Dict, Any
import numpy as np
from collections import defaultdict

class EnsemblePredictor:
    def __init__(self, model_paths: List[str], weights: List[float] = None):
        """
        Ensemble predictor combining multiple YOLO models
        
        Args:
            model_paths: List of model file paths
            weights: Optional weights for each model (default: equal weights)
        """
        self.models = [YOLO(path) for path in model_paths]
        
        if weights is None:
            self.weights = [1.0 / len(self.models)] * len(self.models)
        else:
            # Normalize weights
            total = sum(weights)
            self.weights = [w / total for w in weights]
        
        print(f"Loaded {len(self.models)} models for ensemble")
        print(f"Weights: {self.weights}")
    
    def predict(
        self,
        image,
        conf_threshold: float = 0.25,
        iou_threshold: float = 0.45,
        method: str = "weighted_voting"
    ) -> List[Dict[str, Any]]:
        """
        Make ensemble prediction
        
        Args:
            image: Input image
            conf_threshold: Confidence threshold
            iou_threshold: IOU threshold for NMS
            method: Ensemble method ('weighted_voting', 'max_confidence', 'unanimous')
            
        Returns:
            List of aggregated predictions
        """
        # Get predictions from all models
        all_predictions = []
        
        for i, model in enumerate(self.models):
            results = model.predict(
                image,
                conf=conf_threshold,
                iou=iou_threshold,
                verbose=False
            )
            
            # Extract predictions
            for result in results:
                boxes = result.boxes
                for j in range(len(boxes)):
                    pred = {
                        'class_id': int(boxes.cls[j]),
                        'class_name': result.names[int(boxes.cls[j])],
                        'confidence': float(boxes.conf[j]),
                        'bbox': boxes.xyxy[j].tolist(),
                        'model_idx': i,
                        'model_weight': self.weights[i]
                    }
                    all_predictions.append(pred)
        
        # Aggregate predictions
        if method == "weighted_voting":
            aggregated = self._weighted_voting(all_predictions, iou_threshold)
        elif method == "max_confidence":
            aggregated = self._max_confidence(all_predictions, iou_threshold)
        elif method == "unanimous":
            aggregated = self._unanimous_voting(all_predictions, iou_threshold)
        else:
            raise ValueError(f"Unknown method: {method}")
        
        return aggregated
    
    def _weighted_voting(
        self,
        predictions: List[Dict],
        iou_threshold: float
    ) -> List[Dict]:
        """
        Weighted voting: Combine predictions with weights
        """
        if not predictions:
            return []
        
        # Group overlapping predictions
        groups = self._group_overlapping_predictions(predictions, iou_threshold)
        
        aggregated = []
        
        for group in groups:
            # Calculate weighted class votes
            class_votes = defaultdict(float)
            total_weight = 0
            bbox_sum = np.zeros(4)
            
            for pred in group:
                class_votes[pred['class_id']] += pred['confidence'] * pred['model_weight']
                total_weight += pred['model_weight']
                bbox_sum += np.array(pred['bbox']) * pred['confidence'] * pred['model_weight']
            
            # Get winning class
            winning_class = max(class_votes.items(), key=lambda x: x[1])
            
            # Average bbox weighted by confidence
            avg_bbox = bbox_sum / sum(p['confidence'] * p['model_weight'] for p in group)
            
            aggregated.append({
                'class_id': winning_class[0],
                'class_name': group[0]['class_name'],  # Assume same mapping
                'confidence': winning_class[1] / total_weight,
                'bbox': avg_bbox.tolist(),
                'num_models_agree': len(group)
            })
        
        return aggregated
    
    def _max_confidence(
        self,
        predictions: List[Dict],
        iou_threshold: float
    ) -> List[Dict]:
        """
        Max confidence: Take prediction with highest confidence
        """
        groups = self._group_overlapping_predictions(predictions, iou_threshold)
        
        aggregated = []
        
        for group in groups:
            # Take prediction with max confidence
            best = max(group, key=lambda p: p['confidence'])
            aggregated.append(best)
        
        return aggregated
    
    def _unanimous_voting(
        self,
        predictions: List[Dict],
        iou_threshold: float
    ) -> List[Dict]:
        """
        Unanimous voting: Only keep predictions all models agree on
        """
        groups = self._group_overlapping_predictions(predictions, iou_threshold)
        
        aggregated = []
        
        for group in groups:
            # Only keep if all models agree
            if len(group) == len(self.models):
                # All models detected this object
                class_votes = defaultdict(int)
                for pred in group:
                    class_votes[pred['class_id']] += 1
                
                # Check if all agree on class
                if max(class_votes.values()) == len(self.models):
                    # All models agree
                    avg_confidence = np.mean([p['confidence'] for p in group])
                    avg_bbox = np.mean([p['bbox'] for p in group], axis=0)
                    
                    aggregated.append({
                        'class_id': group[0]['class_id'],
                        'class_name': group[0]['class_name'],
                        'confidence': avg_confidence,
                        'bbox': avg_bbox.tolist(),
                        'num_models_agree': len(group)
                    })
        
        return aggregated
    
    def _group_overlapping_predictions(
        self,
        predictions: List[Dict],
        iou_threshold: float
    ) -> List[List[Dict]]:
        """Group predictions that overlap (based on IOU)"""
        if not predictions:
            return []
        
        groups = []
        used = set()
        
        for i, pred1 in enumerate(predictions):
            if i in used:
                continue
            
            group = [pred1]
            used.add(i)
            
            for j, pred2 in enumerate(predictions[i+1:], start=i+1):
                if j in used:
                    continue
                
                # Calculate IOU
                iou = self._calculate_iou(pred1['bbox'], pred2['bbox'])
                
                if iou > iou_threshold:
                    group.append(pred2)
                    used.add(j)
            
            groups.append(group)
        
        return groups
    
    @staticmethod
    def _calculate_iou(box1: List[float], box2: List[float]) -> float:
        """Calculate Intersection over Union"""
        x1_min, y1_min, x1_max, y1_max = box1
        x2_min, y2_min, x2_max, y2_max = box2
        
        # Intersection
        x_min = max(x1_min, x2_min)
        y_min = max(y1_min, y2_min)
        x_max = min(x1_max, x2_max)
        y_max = min(y1_max, y2_max)
        
        if x_max < x_min or y_max < y_min:
            return 0.0
        
        intersection = (x_max - x_min) * (y_max - y_min)
        
        # Union
        area1 = (x1_max - x1_min) * (y1_max - y1_min)
        area2 = (x2_max - x2_min) * (y2_max - y2_min)
        union = area1 + area2 - intersection
        
        return intersection / union if union > 0 else 0.0

# Usage example
if __name__ == "__main__":
    # Create ensemble
    ensemble = EnsemblePredictor(
        model_paths=[
            'models/yolo12s_v1.pt',
            'models/yolo12m_v1.pt',
            'models/yolo12s_v2.pt'
        ],
        weights=[0.3, 0.5, 0.2]  # Give more weight to medium model
    )
    
    # Make prediction
    predictions = ensemble.predict(
        'test_image.jpg',
        method='weighted_voting'
    )
    
    print(f"Ensemble predictions: {len(predictions)}")
    for pred in predictions:
        print(f"  {pred['class_name']}: {pred['confidence']:.2f}")
```

---

## ğŸ“Š Phase-by-Phase Implementation Schedule

### Phase 1: Larger Models (Week 1 - Days 1-3)
- âœ… Train YOLO12s
- âœ… Train YOLO12m (optional)
- âœ… Compare with baseline
- âœ… Select best model size

### Phase 2: Hyperparameter Optimization (Week 1-2 - Days 4-7)
- âœ… Setup Optuna
- âœ… Define search space
- âœ… Run optimization (50+ trials)
- âœ… Train with best parameters

### Phase 3: Fine-Tuning (Week 2 - Days 8-10)
- âœ… Implement fine-tuning logic
- âœ… Test incremental learning
- âœ… Integrate with MLOps pipeline
- âœ… Document process

### Phase 4: Ensemble (Week 2-3 - Days 11-14)
- âœ… Implement ensemble predictor
- âœ… Test different methods
- âœ… Benchmark performance
- âœ… Decide if worth the complexity

---

## âœ… Verification Checklist

### Model Training
- [ ] YOLO12s trains successfully
- [ ] YOLO12m trains successfully (optional)
- [ ] Models achieve better mAP50 than baseline
- [ ] Training time is acceptable

### Hyperparameter Optimization
- [ ] Optuna study runs successfully
- [ ] Top trials show improvement
- [ ] Best parameters are saved
- [ ] Model trained with best params performs well

### Fine-Tuning
- [ ] Fine-tuning preserves base model knowledge
- [ ] Fine-tuning improves on new data
- [ ] Faster than full retraining
- [ ] Integrated with MLOps pipeline

### Ensemble
- [ ] Ensemble gives better results than single model
- [ ] Inference time is acceptable
- [ ] Different methods tested
- [ ] Production-ready implementation

---

## ğŸ“ˆ Success Metrics

- **mAP50**: Improve from ~0.70 to > 0.85
- **mAP50-95**: Improve from ~0.50 to > 0.65
- **Precision**: > 0.80
- **Recall**: > 0.80
- **Inference Time**: < 50ms on CPU (acceptable for production)
- **Model Size**: < 50MB (deployable)

---

## ğŸ“ Next Steps

After implementing this enhancement:

1. **Deploy best model**: Replace production model
2. **Monitor performance**: Track real-world accuracy
3. **Continue optimization**: Regular hyperparameter tuning
4. **Experiment with other architectures**: Try different models
5. **Implement Enhancement 5**: Add monitoring

---

**Better models = happier users = better business results!**
